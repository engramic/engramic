{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engramic API Reference","text":"<p>Engramic is an inference engine. It's special because it natively supports long-term, contextual memory. This specialty makes it very adept at building applications that need to perform reliable question and answer over a corpus of data over time. Additionally, long-term, contextual memory provides the means for basic learning capabilities, resulting in deeper, more thoughtful responses.</p> <p>Engramic is pre-alpha. It's a great time to start working with these core systems for some developers, but we have yet to complete many important features and even the core systems may not be fully tested. In other words, use of this code base will require some development experience and the ability to work in maturing environments. The flip side, is that a new community is forming and as a pioneer, you have an opportunity to get in early so that you can someday tell your friends, I used Engramic before it was cool.</p> <p>There is currently no support for the following:</p> <ul> <li>There is no support for individual users.</li> <li>There is no HTTP(s) interface at this time. </li> <li>This is no support for documents such as PDFs.</li> <li>Windows and MacOS is not being tested as part of our release process.</li> </ul> <p>These features, along with others, will be available in the near future.</p> <p>For an evergreen overview of Engramic, visit our online Knowledge Base.</p>"},{"location":"#introduction-to-engramic","title":"Introduction to Engramic","text":"<pre><code>flowchart LR\n    sense --&gt; consolidate\n    prompt --&gt; retrieve\n    consolidate --&gt; retrieve\n    retrieve --&gt; respond\n    respond --&gt; stream \n    codify --&gt; consolidate\n    respond --&gt; codify \n</code></pre> <p>Engramic's core services. Engramic uses a learning loop, building memories from responses and responses from memories.</p>"},{"location":"#why-engramic","title":"Why Engramic?","text":"<p>Engramic is designed to learn from your unstructured, proprietary data using any large language model (LLM). When we study, we often begin by reading a document from start to finish. But true understanding comes from synthesizing the information, asking questions, identifying what\u2019s meaningful, and connecting it to prior knowledge and related context. Learning is an iterative process\u2014not a linear one. That\u2019s why we believe a large context window alone doesn\u2019t solve the challenge of truly understanding a dataset. This belief, shaped by two years of research, is what inspired Engramic\u2019s design.</p>"},{"location":"#engramic-architecture-philosophy","title":"Engramic Architecture Philosophy","text":"<ul> <li> <p>Modular   The plugin system allows easy switching between LLMs, databases, vector databases, and embedding tools.</p> </li> <li> <p>Scalable   Built as a set of independent services, Engramic can run on a single machine or be distributed across multiple systems.</p> </li> <li> <p>Fast   Optimized for usage patterns involving many blocking API calls, ensuring responsive performance.</p> </li> <li> <p>Extensible   Easily create custom services or plugins to extend functionality.</p> </li> </ul>"},{"location":"#engramic-core-concepts","title":"Engramic Core Concepts","text":"<ul> <li> <p>Memory   Supports both short-term and long-term memory mechanisms.</p> </li> <li> <p>Engram   The fundamental unit of information, which includes content and base and customizable contextual metadata</p> </li> <li> <p>Citable Engrams   External documents or media that are directly referenced. Citable Engrams are high-fidelity textual representations of the media.</p> </li> <li> <p>Long-Term Memory Engrams   Constructed from one or more Citable Engram or other Long-Term Memory Engrams.</p> </li> <li> <p>Learning   Built through the combination of memory, citable external sources, and user interaction or input.</p> </li> <li> <p>Unified Memory   All engrams are stored within a unified system, enabling both full and selective access to memory content.</p> </li> </ul>"},{"location":"#engramic-services","title":"Engramic Services","text":""},{"location":"getting_started/","title":"Getting Started with Engramic","text":"<p>Please contact us at info@engramic.org if you have any issues with these instructions.</p> 1. Pre-Requisites - OS &amp; IDE <p>We are currently testing Engramic on WSL on Windows using Visual Studio Code With Ubuntu 24.0.1 LTS. It may run on other configurations\u2014we'll begin cross-platform testing soon. If you'd like to help us, please reach out at info@engramic.org.</p> <p>Engramic is availible via pip, however, working from source is recommended for this release.</p> <pre><code>pip install engramic\n</code></pre> <p>To set up your dev environment, you will need the following: - Python 3.10+ - Visual Studio Code - Git - Pipx - Hatch - MS VS Code WSL Extension - MS Python Debugger - Google Gemini API key (optional)</p> 2. Clone or Fork From GitHub <p>Clone or fork the latest version from the <code>main</code> branch of the Engramic GitHub repository:</p> <p>\ud83d\udcce https://github.com/engramic/engramic</p> 3. Install Hatch <p>We use Hatch as our Python project manager. It handles all dependencies, Python versioning, testing, versioning, scripts, and virtual environments.</p> <p>\ud83d\udd17 Hatch Installation</p> <p>We recommend installing with <code>pipx</code> as described in the Hatch installation instructions. Restart your terminal after running pipx ensurepath.</p> 4. Initialize the Environment <p>Now that Hatch is installed:</p> <ol> <li>Navigate to the root of the Engramic project in your terminal.</li> <li> <p>Run:</p> <pre><code>hatch env create\n</code></pre> <p>Enter into the default shell (\"default\" has no name after \"shell\".) <pre><code>hatch shell\n</code></pre></p> </li> </ol> <p>This will install all dependencies (should be quick\u2014we work hard to minimize dependencies). Watch a video of Hatch in Engramic.</p> <ol> <li> <p>Open Visual Studio Code and install the WSL extension.</p> </li> <li> <p>Launch VS Code from the WSL terminal:</p> <pre><code>code .\n</code></pre> </li> <li> <p>In Windows, in VS Code, install the Python Debugger extension from Microsoft.</p> </li> </ol> 5. Configure the Python Interpreter <p>In Visual Studio Code:</p> <ol> <li>Press <code>Ctrl + Shift + P</code></li> <li>Search for and select \"Python: Select Interpreter\"</li> <li>Choose the environment that looks like: <code>Python X.XX.X ('engramic')</code></li> </ol> <p>Note: If you aren't sure of the path, you can type the following while in the hatch shell:</p> <pre><code>python -c \"import sys;print(sys.executable)\"\n</code></pre> <p>If you are stuck, make sure your top, middle search bar in VS Code reads: engramic [WSL: Ubuntu-24.04] (or your distro). If not, your issue is probably related to the WSL extension.</p> 6. Run the Mock Example 7. Run the Standard Example 8. Create your First Memory."},{"location":"getting_started/#running-the-code","title":"Running The Code","text":"<p>The code is available at: <pre><code>    engramic/examples/mock_profile/mock_profile.py\n</code></pre></p> <ol> <li>Open the Run and Debug sidebar in VS Code.</li> <li>Choose \"Example - Mock\" and run it.</li> </ol> <p>Congrats! \ud83c\udf89 You've just run the mock version of the system using mock plugins. You should see an output message in terminal window.</p>"},{"location":"getting_started/#looking-at-the-code","title":"Looking At The Code","text":"<p>The mock version doesn't actually use AI calls, just emulated API calls that return static responses via the Mock plugins. In this example, you can see how to create a Host, MessageService, RetrieveService, and a ResponseService. Also, we have created a service called TestService whose only job is to recieve the Response call from the subscirbed callback on_main_prompt_complete.</p>"},{"location":"getting_started/#running-the-code_1","title":"Running The Code","text":"<p>Now, let's run an example with actual AI. This example uses Google Gemini.</p> <pre><code>    engramic/examples/standard_profile/standard_profile.py\n</code></pre> <ol> <li> <p>For this example, you'll need a Gemini API key:</p> <ul> <li>Create a Google Cloud account if you don't already have one.</li> <li>Follow Google's documentation to create a Generative Language API key.</li> </ul> </li> <li> <p>Add a <code>.env</code> file to the root of the project with the following content (multiple plugin paths coming soon.):</p> <pre><code>GEMINI_API_KEY=PUT_YOUR_KEY_HERE_WITH_NO_QUOTES_OR_ANYTHING_ELSE\nLOCAL_STORAGE_ROOT_PATH=./local_storage\n</code></pre> <p>Locate this line and change it if you would like to.</p> <pre><code>retrieve_service.submit(Prompt('Briefly tell me about Chamath Palihapitiya.'))\n</code></pre> </li> <li> <p>In Run and Debug, select \"Example - Standard\".</p> <p>Note: this takes some time on first run. Be patient.</p> </li> </ol>"},{"location":"getting_started/#looking-at-the-code_1","title":"Looking At The Code","text":"<p>Run the program. The plugins will automatically download all dependencies on the first run and check for updates on subsequent runs. Configuration for plugins are defined by profiles, in this case, the profile is named \"standard\". Each plugin contains it's dependencies in a plugin.toml file.</p> <p>Hit Run and you'll see the result in the terminal window. This example adds Storage, Codify, and Consolidate service, but doesn't actually use them.</p> <p>Let's try those services in the next demo.</p>"},{"location":"getting_started/#running-the-code_2","title":"Running The Code","text":"<p>Let's generate our first memory.</p> <pre><code>    engramic/examples/create_memory/create_memory.py\n</code></pre> <ol> <li>Run and Debug the profile named \"Example - Create Memory\".</li> </ol> <p>You should see three outputs, Response, Meta Summary, and Engrams.</p>"},{"location":"getting_started/#looking-at-the-code_2","title":"Looking at The Code","text":"<p>This time, we've added another call to our TestService. Services support sync and async threads. Some features that services perform run on the main thread, such as subscribing, while others such as sending messages or running tasks must run on the async thread. The Codify service is listening for the SET_TRAINING_MODE call sent by TestService.</p> <pre><code>    class TestService(Service):\n        def start(self):\n            self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n            self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n\n            async def send_message() -&gt; None:\n                self.send_message_async(Service.Topic.SET_TRAINING_MODE, {'training_mode': True})\n\n            self.run_task(send_message())\n\n            super().start()\n</code></pre> <p>Let's look at the Observation, the output of the Codify service. Two types of data structures are output on the screen, the first is a set of Engrams, these are the memories extracted from the response of the training. The next is the Meta Summary. Meta data are summary information about all Engrams that were generated. This data structure is created to help the retrieval stage with awareness of it's memory set.</p> <p>To delete all memories, enter into the hatch shell named \"dev\".</p> <pre><code>cd /engramic\nhatch shell dev\nhatch run delete_dbs\n</code></pre>"},{"location":"host_and_services/","title":"Host &amp; Services","text":"<p>The Engramic Inference Engine consists of several services, each designed to facilitate distinct aspects of knowledge processing.</p>"},{"location":"host_and_services/#host","title":"Host","text":"<p>The Host contains all of the executing services on the instance/machine. You may add all services or just one. The host manages initialization and the set of resources and capabilities used by all other systems. The Host is like a very lightweight abstraction layer for all services.</p> <pre><code>flowchart LR\n    sense --&gt; consolidate\n    prompt --&gt; retrieve\n    consolidate --&gt; retrieve\n    retrieve --&gt; respond\n    respond --&gt; stream \n    codify --&gt; consolidate\n    respond --&gt; codify \n</code></pre>"},{"location":"host_and_services/#services","title":"Services","text":"<ul> <li>Retrieve: Analyzes the prompt, manages short-term memory, and performs retrieval of all engrams.</li> <li>Respond: Constructs the response to the user.</li> <li>Codify: While in training mode, assesses the validity of responses, integrating them as long-term memories.</li> <li>Consolidate: Transforms data into observations, contained knowledge units rich in context.</li> <li>Sense: Converts raw data into observations.</li> </ul>"},{"location":"host_and_services/#centralized-services","title":"Centralized Services","text":"<ul> <li>Store: Centralized storage for long-term, context-aware memory.</li> <li>Message: Centeralized message passing between all services.</li> <li>Process: Centralized progress tracking from input (e.g. prompt or document) to inserting into Retrieve.</li> </ul>"},{"location":"host_and_services/#services-in-development","title":"Services In Development","text":"<ul> <li>Teach: Encourages the emergence of new engrams by stimulating connections between existing ones.</li> </ul>"},{"location":"how_to/","title":"More How To's Coming Soon...","text":"<p>Build A Web App.</p> <p>Build Memories on a File.</p> <p>Use The Teach Service to Automate Training.</p> <p>Develop Your Own Plugin.</p> <p>Develop Your Own Service.</p>"},{"location":"profiles/","title":"Engramic Plugin and Profile System","text":"<p>Engramic is designed with a plugin layer that allows seamless integration with various components such as LLMs, databases, vector databases, and embedding services. The glue that binds these systems together are called profiles.</p>"},{"location":"profiles/#example-default-profile-configuration","title":"Example: Default Profile Configuration","text":"<pre><code>version = 0.1\n\n[mock]\ntype = \"profile\"\nvector_db.db = {name=\"Mock\"}\nllm.retrieve_gen_conversation_direction = {name=\"Mock\"}\nllm.retrieve_gen_index = {name=\"Mock\"}\nllm.retrieve_prompt_analysis = {name=\"Mock\"}\ndb.document = {name=\"Mock\"}\nllm.response_main = {name=\"Mock\"}\nllm.validate = {name=\"Mock\"}\nllm.summary = {name=\"Mock\"}\nllm.gen_indices = {name=\"Mock\"}\nembedding.gen_embed = {name=\"Mock\"}\n\n[standard]\ntype = \"pointer\"\nptr = \"standard-2025-04-01\"\n\n[standard-2025-04-01]\ntype = \"profile\"\nvector_db.db = {name=\"ChromaDB\"}\nllm.retrieve_gen_conversation_direction = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.retrieve_gen_index = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.retrieve_prompt_analysis = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\ndb.document = {name=\"Sqlite\"}\nllm.response_main = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.validate = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.summary = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.gen_indices = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nembedding.gen_embed = {name=\"Gemini\", model=\"text-embedding-004\"}\n</code></pre>"},{"location":"profiles/#loading-a-profile-with-the-host","title":"Loading a Profile with the Host","text":"<p>When creating a <code>Host</code>, you should load the initial profile like so:</p> <pre><code>host = Host(\n    'standard',\n    [\n        MessageService,\n        RetrieveService,\n        ResponseService,\n        StorageService,\n        CodifyService,\n        ConsolidateService\n    ]\n)\n</code></pre> <p>In this example, the developer is using the <code>standard</code> profile. This is a pointer profile that redirects to a dated version (e.g., <code>standard-2025-04-01</code>). As Engramic evolves, the <code>standard</code> pointer will be updated to reference the currently recommended profile. By using <code>standard</code>, you're always aligned with the recommended configuration.</p> <p>The <code>mock</code> profile is another key option. When using <code>mock</code>, all plugins are mock implementations \u2014 meaning no API calls are made. This mode is intended exclusively for testing and development.</p>"},{"location":"howto/how_to_parse_documents/","title":"PDF Parse","text":"<pre><code>flowchart LR\n    sense --&gt; consolidate\n    prompt --&gt; retrieve\n    consolidate --&gt; retrieve\n    retrieve --&gt; respond\n    respond --&gt; stream \n    codify --&gt; consolidate\n    respond --&gt; codify \n\n    classDef green fill:#b2f2bb;\n    class sense green\n</code></pre> <p>PDF parsing is part of the sense service. When a document is parsed, it is sent to the consolidate service where it is processed and passed to retrieval for storing in a vector database and to response if it is matched semantically.</p>"},{"location":"howto/how_to_parse_documents/#example-code","title":"Example Code","text":"<p>The full code is avaiable in the source code at <code>/engramic/examples/document/document.py</code></p> Imports and logging config. <pre><code>import logging\nfrom typing import Any\n\nfrom engramic.application.codify.codify_service import CodifyService\nfrom engramic.application.consolidate.consolidate_service import ConsolidateService\nfrom engramic.application.message.message_service import MessageService\nfrom engramic.application.progress.progress_service import ProgressService\nfrom engramic.application.response.response_service import ResponseService\nfrom engramic.application.retrieve.retrieve_service import RetrieveService\nfrom engramic.application.sense.sense_service import SenseService\nfrom engramic.application.storage.storage_service import StorageService\nfrom engramic.core.document import Document\nfrom engramic.core.host import Host\nfrom engramic.core.prompt import Prompt\nfrom engramic.core.response import Response\nfrom engramic.infrastructure.system import Service\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n</code></pre> The Test Service <pre><code># This service is built only to subscribe to the main prompt completion message.\nclass TestService(Service):\n    def start(self):\n        super().start()\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n        self.subscribe(Service.Topic.INPUT_COMPLETED, self.on_input_complete)\n\n        sense_service = self.host.get_service(SenseService)\n        document = Document(Document.Root.RESOURCE, 'engramic.resources.job_descriptions', 'GH SC Official Job Descriptions.pdf')\n        self.document_id = document.id\n        sense_service.submit_document(document)\n\n\n    def on_main_prompt_complete(self, message_in: dict[str, Any]) -&gt; None:\n        response = Response(**message_in)\n        logging.info('\\n\\n================[Response]==============\\n%s\\n\\n', response.response)\n\n    def on_input_complete(self, message_in: dict[str, Any]) -&gt; None:\n        input_id = message_in['input_id']\n        if self.document_id == input_id:\n            retrieve_service = self.host.get_service(RetrieveService)\n            prompt = Prompt('List the roles at GH Star Collector.')\n            retrieve_service.submit(prompt)\n</code></pre> <p>The TestService is small service whose pupose is to run the example. It subscribes to MAIN_PROMPT_COMPLETE which is sent by the Response Service when the response is complete. INPUT_COMPLETED tells us when our document is complete, a process which includes the following steps:</p>"},{"location":"howto/how_to_parse_documents/#sense-service","title":"Sense Service","text":"<ul> <li>Convert PDF page to PNGs</li> <li>Extract meta data from first few pages</li> <li>Convert from image into annotated text</li> <li>Summarize annotated text for Meta object</li> <li>Parsed from annotated text into Engrams</li> <li>Packaged into an observation (Meta + Engrams)</li> </ul>"},{"location":"howto/how_to_parse_documents/#consolidate-service","title":"Consolidate Service","text":"<ul> <li>Unpackage observation</li> <li>Generate indices for meta and engrams</li> <li>Generate embeddings</li> <li>Save meta and engrams</li> <li>Send meta and engrams index to retrieve to store in meta and engrams vector databases.</li> </ul> Main <pre><code>def main() -&gt; None:\n    host = Host(\n        'standard',\n        [\n            MessageService,\n            SenseService,\n            RetrieveService,\n            ResponseService,\n            StorageService,\n            ConsolidateService,\n            CodifyService,\n            ProgressService,\n            TestService,\n        ],\n    )\n\n    # The host continues to run and waits for a shutdown message to exit.\n    host.wait_for_shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>The code in the section simple adds all services into the Host. (note: Codify isn't actually used in the example.)</p>"},{"location":"howto/how_to_parse_documents/#loading-from-data-dir","title":"Loading From Data Dir","text":"<p>The document object is passed to the SenseService and defines the file or resource path and the file name. In the exmample above, the code is referencing a file saved in the resources directory, which is packaged with the distribution. If you would like to load a file that isn't part of the resources, you can choose Document.Root.DATA which will set a base directory of ~.local/share/engramic</p> <p>Note: Windows and MacOS is not tested at this time.</p>"},{"location":"reference/ask/","title":"Ask","text":"<p>               Bases: <code>Retrieval</code></p> <p>Ask is a specifc type of retrieval focused on traditional Q&amp;A. It is a single instanc of an ask.</p> <p>This class handles the end-to-end workflow of transforming a raw prompt into contextual embeddings, querying the vector database, and returning retrieved engram ids. It also supports various conversation analysis and prompt index generation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for this retrieval session.</p> <code>prompt</code> <code>Prompt</code> <p>The original prompt provided by the user.</p> <code>plugin_manager</code> <code>PluginManager</code> <p>Plugin manager used to access LLM, vector DB, and embedding components.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks operational metrics for observability.</p> <code>db_plugin</code> <code>dict</code> <p>Plugin used to interact with the document database.</p> <code>service</code> <code>RetrieveService</code> <p>Reference to the parent service coordinating this request.</p> <code>library</code> <code>str | None</code> <p>Optional name of the target library to search within.</p> <code>conversation_direction</code> <code>dict[str, str]</code> <p>Stores current user intent and working memory.</p> <code>prompt_analysis</code> <code>PromptAnalysis | None</code> <p>Stores structured analysis of the prompt after processing.</p> <p>Methods:</p> Name Description <code>get_sources</code> <p>Initiates the async pipeline for directional memory retrieval.</p> <code>_fetch_history</code> <p>Asynchronously retrieves prior user history from the document DB.</p> <code>_retrieve_gen_conversation_direction</code> <p>Uses LLM plugin to extract user intent and conversational memory.</p> <code>_embed_gen_direction</code> <p>Converts extracted intent into embeddings. General direction determines intent and manages short term memory.</p> <code>_vector_fetch_direction_meta</code> <p>Queries the metadata collection in the vector DB using intent embeddings.</p> <code>_fetch_direction_meta</code> <p>Loads Meta objects from the metadata store based on query results.</p> <code>_analyze_prompt</code> <p>Uses LLM plugin to analyze the user prompt in the context of metadata.</p> <code>_generate_indices</code> <p>Generates semantic indices from the prompt and metadata for retrieval.</p> <code>_generate_indicies_embeddings</code> <p>Converts generated index phrases into embeddings.</p> <code>_query_index_db</code> <p>Searches the main vector DB with embeddings to identify related engrams.</p> <code>on_fetch_history_complete</code> <p>Callback when prompt history fetch is complete; begins direction generation.</p> <code>on_direction_ret_complete</code> <p>Callback when conversation direction is generated; begins embedding.</p> <code>on_embed_direction_complete</code> <p>Callback when direction embedding is ready; begins vector DB search for metadata.</p> <code>on_vector_fetch_direction_meta_complete</code> <p>Callback when metadata vector search is complete; begins metadata fetch.</p> <code>on_fetch_direction_meta_complete</code> <p>Callback when metadata objects are fetched; begins analysis and index generation.</p> <code>on_analyze_complete</code> <p>Callback when prompt analysis and index generation are complete; begins embedding generation.</p> <code>on_indices_embeddings_generated</code> <p>Callback when index embeddings are ready; triggers main index DB query.</p> <code>on_query_index_db</code> <p>Final callback when engram retrieval is complete; assembles and emits the result.</p> Source code in <code>src/engramic/application/retrieve/ask.py</code> <pre><code>class Ask(Retrieval):\n    \"\"\"\n    Ask is a specifc type of retrieval focused on traditional Q&amp;A. It is a single instanc of an ask.\n\n    This class handles the end-to-end workflow of transforming a raw prompt into\n    contextual embeddings, querying the vector database, and returning retrieved engram ids.\n    It also supports various conversation analysis and prompt index generation.\n\n    Attributes:\n        id (str): A unique identifier for this retrieval session.\n        prompt (Prompt): The original prompt provided by the user.\n        plugin_manager (PluginManager): Plugin manager used to access LLM, vector DB, and embedding components.\n        metrics_tracker (MetricsTracker): Tracks operational metrics for observability.\n        db_plugin (dict): Plugin used to interact with the document database.\n        service (RetrieveService): Reference to the parent service coordinating this request.\n        library (str | None): Optional name of the target library to search within.\n        conversation_direction (dict[str, str]): Stores current user intent and working memory.\n        prompt_analysis (PromptAnalysis | None): Stores structured analysis of the prompt after processing.\n\n    Methods:\n        get_sources():\n            Initiates the async pipeline for directional memory retrieval.\n\n        _fetch_history():\n            Asynchronously retrieves prior user history from the document DB.\n\n        _retrieve_gen_conversation_direction():\n            Uses LLM plugin to extract user intent and conversational memory.\n\n        _embed_gen_direction():\n            Converts extracted intent into embeddings. General direction determines intent and manages short term memory.\n\n        _vector_fetch_direction_meta():\n            Queries the metadata collection in the vector DB using intent embeddings.\n\n        _fetch_direction_meta():\n            Loads Meta objects from the metadata store based on query results.\n\n        _analyze_prompt():\n            Uses LLM plugin to analyze the user prompt in the context of metadata.\n\n        _generate_indices():\n            Generates semantic indices from the prompt and metadata for retrieval.\n\n        _generate_indicies_embeddings():\n            Converts generated index phrases into embeddings.\n\n        _query_index_db():\n            Searches the main vector DB with embeddings to identify related engrams.\n\n        on_fetch_history_complete(fut):\n            Callback when prompt history fetch is complete; begins direction generation.\n\n        on_direction_ret_complete(fut):\n            Callback when conversation direction is generated; begins embedding.\n\n        on_embed_direction_complete(fut):\n            Callback when direction embedding is ready; begins vector DB search for metadata.\n\n        on_vector_fetch_direction_meta_complete(fut):\n            Callback when metadata vector search is complete; begins metadata fetch.\n\n        on_fetch_direction_meta_complete(fut):\n            Callback when metadata objects are fetched; begins analysis and index generation.\n\n        on_analyze_complete(fut):\n            Callback when prompt analysis and index generation are complete; begins embedding generation.\n\n        on_indices_embeddings_generated(fut):\n            Callback when index embeddings are ready; triggers main index DB query.\n\n        on_query_index_db(fut):\n            Final callback when engram retrieval is complete; assembles and emits the result.\n    \"\"\"\n\n    def __init__(\n        self,\n        ask_id: str,\n        prompt: Prompt,\n        plugin_manager: PluginManager,\n        metrics_tracker: MetricsTracker[engramic.application.retrieve.retrieve_service.RetrieveMetric],\n        db_plugin: dict[str, Any],\n        service: RetrieveService,\n        library: str | None = None,\n    ) -&gt; None:\n        self.id = ask_id\n        self.service = service\n        self.metrics_tracker: MetricsTracker[engramic.application.retrieve.retrieve_service.RetrieveMetric] = (\n            metrics_tracker\n        )\n        self.library = library\n        self.prompt = prompt\n        self.conversation_direction: dict[str, str]\n        self.prompt_analysis: PromptAnalysis | None = None\n        self.retrieve_gen_conversation_direction_plugin = plugin_manager.get_plugin(\n            'llm', 'retrieve_gen_conversation_direction'\n        )\n        self.prompt_analysis_plugin = plugin_manager.get_plugin('llm', 'retrieve_prompt_analysis')\n        self.prompt_retrieve_indices_plugin = plugin_manager.get_plugin('llm', 'retrieve_gen_index')\n        self.prompt_vector_db_plugin = plugin_manager.get_plugin('vector_db', 'db')\n        self.prompt_db_document_plugin = db_plugin\n        self.embeddings_gen_embed = plugin_manager.get_plugin('embedding', 'gen_embed')\n\n    def get_sources(self) -&gt; None:\n        direction_step = self.service.run_task(self._fetch_history())\n        direction_step.add_done_callback(self.on_fetch_history_complete)\n\n    \"\"\"\n    ### CONVERSATION DIRECTION\n\n    Fetches related domain knowledge based on the prompt intent.\n    \"\"\"\n\n    async def _fetch_history(self) -&gt; list[dict[str, Any]]:\n        plugin = self.prompt_db_document_plugin\n        args = plugin['args']\n        args['history'] = 10\n\n        ret_val = await asyncio.to_thread(plugin['func'].fetch, table=DB.DBTables.HISTORY, ids=[], args=args)\n        history_dict: list[dict[str, Any]] = ret_val[0]\n        return history_dict\n\n    def on_fetch_history_complete(self, fut: Future[Any]) -&gt; None:\n        response_array: list[dict[str, Any]] = fut.result()\n        retrieve_gen_conversation_direction_step = self.service.run_task(\n            self._retrieve_gen_conversation_direction(response_array)\n        )\n        retrieve_gen_conversation_direction_step.add_done_callback(self.on_direction_ret_complete)\n\n    async def _retrieve_gen_conversation_direction(self, response_array: list[dict[str, Any]]) -&gt; dict[str, str]:\n        if __debug__:\n            self.service.send_message_async(self.service.Topic.DEBUG_ASK_CREATED, {'ask_id': self.id})\n\n        input_data = {'history_array': response_array}\n        plugin = self.retrieve_gen_conversation_direction_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt_gen = PromptGenConversation(prompt_str=self.prompt.prompt_str, input_data=input_data)\n\n        structured_schema = {\n            'current_user_intent': str,\n            'working_memory_step_1': str,\n            'working_memory_step_2': str,\n            'working_memory_step_3': str,\n            'working_memory_step_4': str,\n        }\n\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt_gen,\n            structured_schema=structured_schema,\n            args=self.service.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        json_parsed: dict[str, str] = json.loads(ret[0]['llm_response'])\n\n        self.conversation_direction = {}\n        self.conversation_direction['current_user_intent'] = json_parsed['current_user_intent']\n\n        self.conversation_direction['working_memory'] = json_parsed['working_memory_step_4']\n\n        if __debug__:\n            self.service.send_message_async(\n                self.service.Topic.DEBUG_CONVERSATION_DIRECTION,\n                {'ask_id': self.id, 'prompt': prompt_gen.render_prompt(), 'working_memory': ret[0]['llm_response']},\n            )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.CONVERSATION_DIRECTION_CALCULATED\n        )\n\n        return json_parsed\n\n    def on_direction_ret_complete(self, fut: Future[Any]) -&gt; None:\n        direction_ret = fut.result()\n\n        logging.debug('current_user_intent: %s', direction_ret)\n        intent_and_direction = direction_ret['current_user_intent']\n\n        embed_step = self.service.run_task(self._embed_gen_direction(intent_and_direction))\n        embed_step.add_done_callback(self.on_embed_direction_complete)\n\n    async def _embed_gen_direction(self, main_prompt: str) -&gt; list[float]:\n        plugin = self.embeddings_gen_embed\n\n        ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=[main_prompt], args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        float_array: list[float] = ret[0]['embeddings_list'][0]\n        return float_array\n\n    def on_embed_direction_complete(self, fut: Future[Any]) -&gt; None:\n        embedding = fut.result()\n        fetch_direction_step = self.service.run_task(self._vector_fetch_direction_meta(embedding))\n        fetch_direction_step.add_done_callback(self.on_vector_fetch_direction_meta_complete)\n\n    async def _vector_fetch_direction_meta(self, embedding: list[float]) -&gt; list[str]:\n        plugin = self.prompt_vector_db_plugin\n        plugin['args'].update({'threshold': 0.6, 'n_results': 5})\n\n        ret = await asyncio.to_thread(\n            plugin['func'].query,\n            collection_name='meta',\n            embeddings=embedding,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        list_str: list[str] = ret[0]['query_set']\n        # logging.warning(list_str)\n        return list_str\n\n    def on_vector_fetch_direction_meta_complete(self, fut: Future[Any]) -&gt; None:\n        meta_ids = fut.result()\n        meta_fetch_step = self.service.run_task(self._fetch_direction_meta(meta_ids))\n        meta_fetch_step.add_done_callback(self.on_fetch_direction_meta_complete)\n\n    async def _fetch_direction_meta(self, meta_id: list[str]) -&gt; list[Meta]:\n        meta_list = self.service.meta_repository.load_batch(meta_id)\n\n        if __debug__:\n            dict_meta = [meta.summary_full.text if meta.summary_full is not None else '' for meta in meta_list]\n\n            self.service.send_message_async(\n                self.service.Topic.DEBUG_ASK_META, {'ask_id': self.id, 'ask_meta': dict_meta}\n            )\n\n        return meta_list\n\n    def on_fetch_direction_meta_complete(self, fut: Future[Any]) -&gt; None:\n        meta_list = fut.result()\n        analyze_step = self.service.run_tasks([self._analyze_prompt(meta_list), self._generate_indices(meta_list)])\n        analyze_step.add_done_callback(self.on_analyze_complete)\n\n    \"\"\"\n    ### Prompt Analysis\n\n    Analyzies the prompt and generates lookups that will aid in vector searching of related content\n    \"\"\"\n\n    async def _analyze_prompt(self, meta_list: list[Meta]) -&gt; dict[str, str]:\n        plugin = self.prompt_analysis_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt = PromptAnalyzePrompt(\n            prompt_str=self.prompt.prompt_str,\n            input_data={'meta_list': meta_list, 'working_memory': self.conversation_direction['working_memory']},\n        )\n        structured_response = {'response_length': str, 'user_prompt_type': str, 'thinking_steps': str}\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=structured_response,\n            args=self.service.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        self.metrics_tracker.increment(engramic.application.retrieve.retrieve_service.RetrieveMetric.PROMPTS_ANALYZED)\n\n        if not isinstance(ret[0], dict):\n            error = f'Expected dict[str, str], got {type(ret[0])}'\n            raise TypeError(error)\n\n        return ret[0]\n\n    def on_analyze_complete(self, fut: Future[Any]) -&gt; None:\n        analysis = fut.result()  # This will raise an exception if the coroutine fails\n\n        self.prompt_analysis = PromptAnalysis(\n            json.loads(analysis['_analyze_prompt'][0]['llm_response']),\n            json.loads(analysis['_generate_indices'][0]['llm_response']),\n        )\n\n        genrate_indices_future = self.service.run_task(\n            self._generate_indicies_embeddings(self.prompt_analysis.indices['indices'])\n        )\n        genrate_indices_future.add_done_callback(self.on_indices_embeddings_generated)\n\n    async def _generate_indices(self, meta_list: list[Meta]) -&gt; dict[str, str]:\n        plugin = self.prompt_retrieve_indices_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt = PromptGenIndices(prompt_str=self.prompt.prompt_str, input_data={'meta_list': meta_list})\n        structured_output = {'indices': list[str]}\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=structured_output,\n            args=self.service.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        if __debug__:\n            prompt_render = prompt.render_prompt()\n            self.service.send_message_async(\n                Service.Topic.DEBUG_ASK_INDICES,\n                {'ask_id': self.id, 'prompt': prompt_render, 'indices': ret[0]['llm_response']},\n            )\n\n        self.service.host.update_mock_data(plugin, ret)\n        response = ret[0]['llm_response']\n        response_json = json.loads(response)\n        count = len(response_json['indices'])\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.DYNAMIC_INDICES_GENERATED, count\n        )\n\n        if not isinstance(ret[0], dict):\n            error = f'Expected dict[str, str], got {type(ret[0])}'\n            raise TypeError(error)\n\n        return ret[0]\n\n    def on_indices_embeddings_generated(self, fut: Future[Any]) -&gt; None:\n        embeddings = fut.result()\n\n        query_index_db_future = self.service.run_task(self._query_index_db(embeddings))\n        query_index_db_future.add_done_callback(self.on_query_index_db)\n\n    async def _generate_indicies_embeddings(self, indices: list[str]) -&gt; list[list[float]]:\n        plugin = self.embeddings_gen_embed\n\n        ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=indices, args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        embeddings_list: list[list[float]] = ret[0]['embeddings_list']\n        return embeddings_list\n\n    \"\"\"\n    ### Fetch Engram IDs\n\n    Use the indices to fetch related Engram IDs\n    \"\"\"\n\n    async def _query_index_db(self, embeddings: list[list[float]]) -&gt; set[str]:\n        plugin = self.prompt_vector_db_plugin\n\n        ids = set()\n\n        ret = await asyncio.to_thread(\n            plugin['func'].query,\n            collection_name='main',\n            embeddings=embeddings,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        ids.update(ret[0]['query_set'])\n\n        num_queries = len(ids)\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.VECTOR_DB_QUERIES, num_queries\n        )\n\n        return ids\n\n    def on_query_index_db(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        logging.debug('Query Result: %s', ret)\n\n        if self.prompt_analysis is None:\n            error = 'on_query_index_db failed: prompt_analysis is None and likely failed during an earlier process.'\n            raise RuntimeError\n\n        retrieve_result = RetrieveResult(\n            self.id,\n            self.prompt.prompt_id,\n            engram_id_array=list(ret),\n            conversation_direction=self.conversation_direction,\n            analysis=asdict(self.prompt_analysis)['prompt_analysis'],\n        )\n\n        if self.prompt_analysis is None:\n            error = 'Prompt analysis None in on_query_index_db'\n            raise RuntimeError(error)\n\n        retrieve_response = {\n            'analysis': asdict(self.prompt_analysis),\n            'prompt_str': self.prompt.prompt_str,\n            'retrieve_response': asdict(retrieve_result),\n        }\n\n        if __debug__:\n            self.service.host.update_mock_data_output(self.service, retrieve_response)\n\n        self.service.send_message_async(Service.Topic.RETRIEVE_COMPLETE, retrieve_response)\n</code></pre>"},{"location":"reference/codify_service/","title":"Codify Service","text":"<p>               Bases: <code>Service</code></p> <p>CodifyService is a system-level service responsible for validating and extracting engrams (memories) from AI model responses using a TOML-based validation pipeline.</p> <p>This service listens for prompts that have completed processing, and if the system is in training mode, it fetches related engrams and metadata, applies an LLM-based validation process, and stores structured observations. It tracks metrics related to its activity and supports training workflows.</p> <p>Key Responsibilities: - Subscribes to relevant service events like <code>MAIN_PROMPT_COMPLETE</code> and <code>ACKNOWLEDGE</code>. - Fetches engrams and their associated metadata based on a completed model response. - Uses a validation plugin to process model responses and extract structured observation data. - Validates and loads TOML-encoded responses into structured Observation objects. - Merges observations when applicable and sends results asynchronously to downstream systems. - Tracks system-level metrics for observability and debugging.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manages access to system plugins such as the LLM and document DB.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Repository for accessing and managing engram data.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Repository for associated metadata retrieval.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles validation and normalization of observation data.</p> <code>prompt</code> <code>Prompt</code> <p>Default prompt object used during validation.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks custom CodifyMetric metrics.</p> <code>training_mode</code> <code>bool</code> <p>Flag indicating whether the system is in training mode.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes the service to key topics.</p> <code>stop</code> <p>Stops the service.</p> <code>init_async</code> <p>Initializes async components, including DB connections.</p> <code>on_set_training_mode</code> <p>Sets training mode flag based on incoming message.</p> <code>on_main_prompt_complete</code> <p>Main entry point triggered after a model completes a prompt.</p> <code>fetch_engrams</code> <p>Asynchronously fetches engrams associated with a response.</p> <code>on_fetch_engram_complete</code> <p>Callback that processes fetched engrams and triggers metadata retrieval.</p> <code>fetch_meta</code> <p>Asynchronously fetches metadata for given engrams.</p> <code>on_fetch_meta_complete</code> <p>Callback that begins the validation process after fetching metadata.</p> <code>validate</code> <p>Runs the validation plugin on the response and returns an observation.</p> <code>on_validate_complete</code> <p>Final step that emits the completed observation to other systems.</p> <code>on_acknowledge</code> <p>Responds to ACK messages by reporting and resetting metrics.</p> Source code in <code>src/engramic/application/codify/codify_service.py</code> <pre><code>class CodifyService(Service):\n    \"\"\"\n    CodifyService is a system-level service responsible for validating and extracting engrams (memories) from AI model responses using a TOML-based validation pipeline.\n\n    This service listens for prompts that have completed processing, and if the system is in training mode, it fetches related engrams and metadata, applies an LLM-based validation process, and stores structured observations. It tracks metrics related to its activity and supports training workflows.\n\n    Key Responsibilities:\n    - Subscribes to relevant service events like `MAIN_PROMPT_COMPLETE` and `ACKNOWLEDGE`.\n    - Fetches engrams and their associated metadata based on a completed model response.\n    - Uses a validation plugin to process model responses and extract structured observation data.\n    - Validates and loads TOML-encoded responses into structured Observation objects.\n    - Merges observations when applicable and sends results asynchronously to downstream systems.\n    - Tracks system-level metrics for observability and debugging.\n\n    Attributes:\n        plugin_manager (PluginManager): Manages access to system plugins such as the LLM and document DB.\n        engram_repository (EngramRepository): Repository for accessing and managing engram data.\n        meta_repository (MetaRepository): Repository for associated metadata retrieval.\n        observation_repository (ObservationRepository): Handles validation and normalization of observation data.\n        prompt (Prompt): Default prompt object used during validation.\n        metrics_tracker (MetricsTracker): Tracks custom CodifyMetric metrics.\n        training_mode (bool): Flag indicating whether the system is in training mode.\n\n    Methods:\n        start(): Subscribes the service to key topics.\n        stop(): Stops the service.\n        init_async(): Initializes async components, including DB connections.\n        on_set_training_mode(message_in): Sets training mode flag based on incoming message.\n        on_main_prompt_complete(response_dict): Main entry point triggered after a model completes a prompt.\n        fetch_engrams(response): Asynchronously fetches engrams associated with a response.\n        on_fetch_engram_complete(fut): Callback that processes fetched engrams and triggers metadata retrieval.\n        fetch_meta(engram_array, meta_id_array, response): Asynchronously fetches metadata for given engrams.\n        on_fetch_meta_complete(fut): Callback that begins the validation process after fetching metadata.\n        validate(engram_array, meta_array, response): Runs the validation plugin on the response and returns an observation.\n        on_validate_complete(fut): Final step that emits the completed observation to other systems.\n        on_acknowledge(message_in): Responds to ACK messages by reporting and resetting metrics.\n    \"\"\"\n\n    ACCURACY_CONSTANT = 3\n    RELEVANCY_CONSTANT = 3\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.llm_validate = self.plugin_manager.get_plugin('llm', 'validate')\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.meta_repository: MetaRepository = MetaRepository(self.db_document_plugin)\n        self.observation_repository: ObservationRepository = ObservationRepository(self.db_document_plugin)\n\n        self.prompt = Prompt('Validate the llm.')\n        self.metrics_tracker: MetricsTracker[CodifyMetric] = MetricsTracker[CodifyMetric]()\n        self.training_mode = False\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n        self.subscribe(Service.Topic.SET_TRAINING_MODE, self.on_set_training_mode)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_set_training_mode(self, message_in: dict[str, Any]) -&gt; None:\n        self.training_mode = message_in['training_mode']\n\n    def on_main_prompt_complete(self, response_dict: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, response_dict)\n\n        if not self.training_mode:\n            return\n\n        prompt_str = response_dict['prompt_str']\n        model = response_dict['model']\n        analysis = PromptAnalysis(**response_dict['analysis'])\n        retrieve_result = RetrieveResult(**response_dict['retrieve_result'])\n        response = Response(\n            response_dict['id'],\n            response_dict['input_id'],\n            response_dict['response'],\n            retrieve_result,\n            prompt_str,\n            analysis,\n            model,\n        )\n        self.metrics_tracker.increment(CodifyMetric.RESPONSE_RECIEVED)\n        fetch_engram_step = self.run_task(self._fetch_engrams(response))\n        fetch_engram_step.add_done_callback(self.on_fetch_engram_complete)\n\n    \"\"\"\n    ### Fetch Engrams &amp; Meta\n\n    Fetch engrams based on retrieved results.\n    \"\"\"\n\n    async def _fetch_engrams(self, response: Response) -&gt; dict[str, Any]:\n        engram_array: list[Engram] = await asyncio.to_thread(\n            self.engram_repository.load_batch_retrieve_result, response.retrieve_result\n        )\n\n        self.metrics_tracker.increment(CodifyMetric.ENGRAM_FETCHED, len(engram_array))\n\n        meta_array: set[str] = set()\n        for engram in engram_array:\n            if engram.meta_ids is not None:\n                meta_array.update(engram.meta_ids)\n\n        return {'engram_array': engram_array, 'meta_array': list(meta_array), 'response': response}\n\n    def on_fetch_engram_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        fetch_meta_step = self.run_task(self._fetch_meta(ret['engram_array'], ret['meta_array'], ret['response']))\n        fetch_meta_step.add_done_callback(self.on_fetch_meta_complete)\n\n    async def _fetch_meta(\n        self, engram_array: list[Engram], meta_id_array: list[str], response: Response\n    ) -&gt; dict[str, Any]:\n        meta_array: list[Meta] = await asyncio.to_thread(self.meta_repository.load_batch, meta_id_array)\n        # assembled main_prompt, render engrams.\n\n        return {'engram_array': engram_array, 'meta_array': meta_array, 'response': response}\n\n    def on_fetch_meta_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        fetch_meta_step = self.run_task(self._validate(ret['engram_array'], ret['meta_array'], ret['response']))\n        fetch_meta_step.add_done_callback(self.on_validate_complete)\n\n    \"\"\"\n    ### Validate\n\n    Validates and extracts engrams (i.e. memories) from responses.\n    \"\"\"\n\n    async def _validate(self, engram_array: list[Engram], meta_array: list[Meta], response: Response) -&gt; dict[str, Any]:\n        # insert prompt engineering\n\n        del meta_array\n\n        input_data = {\n            'engram_list': engram_array,\n            'response': response.response,\n        }\n\n        prompt = PromptValidatePrompt(response.prompt_str, input_data=input_data)\n\n        plugin = self.llm_validate\n        validate_response = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=None,\n            args=self.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        self.host.update_mock_data(self.llm_validate, validate_response)\n\n        toml_data = None\n\n        try:\n            if __debug__:\n                prompt_render = prompt.render_prompt()\n                self.send_message_async(\n                    Service.Topic.DEBUG_OBSERVATION_TOML_COMPLETE,\n                    {'prompt': prompt_render, 'toml': validate_response[0]['llm_response'], 'response_id': response.id},\n                )\n\n            toml_data = tomli.loads(validate_response[0]['llm_response'])\n\n        except tomli.TOMLDecodeError as e:\n            logging.exception('TOML decode error: %s', validate_response[0]['llm_response'])\n            error = 'Malformed TOML file in codify:validate.'\n            raise TypeError(error) from e\n\n        if 'not_memorable' in toml_data:\n            return {'return_observation': None}\n\n        if not self.observation_repository.validate_toml_dict(toml_data):\n            error = 'Codify TOML did not pass validation.'\n            raise TypeError(error)\n\n        return_observation = self.observation_repository.load_toml_dict(\n            self.observation_repository.normalize_toml_dict(toml_data, response)\n        )\n\n        # if this observation is from multiple sources, it must be merged the sources into it's meta.\n        if len(engram_array) &gt; 0:\n            return_observation_merged: Observation = return_observation.merge_observation(\n                return_observation,\n                CodifyService.ACCURACY_CONSTANT,\n                CodifyService.RELEVANCY_CONSTANT,\n                self.engram_repository,\n            )\n\n            return {'return_observation': return_observation_merged}\n\n        self.metrics_tracker.increment(CodifyMetric.ENGRAM_VALIDATED)\n\n        return {'return_observation': return_observation}\n\n    def on_validate_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n\n        if ret['return_observation'] is not None:\n            self.send_message_async(Service.Topic.OBSERVATION_COMPLETE, asdict(ret['return_observation']))\n\n        if __debug__:\n            self.host.update_mock_data_output(self, asdict(ret['return_observation']))\n\n    \"\"\"\n    ### Ack\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/consolidate_service/","title":"Consolidate Service","text":"<p>               Bases: <code>Service</code></p> <p>The ConsolidateService orchestrates the post-processing pipeline for completed observations, coordinating summarization, engram generation, index generation, and embedding creation.</p> <p>This service is triggered when an observation is marked complete and is responsible for the following:</p> <ol> <li>Summarization - Generates a natural language summary from the observation using an LLM plugin.</li> <li>Embedding Summaries - Uses an embedding plugin to create vector embeddings of the summary text.</li> <li>Engram Generation - Extracts or constructs engrams from the observation's content.</li> <li>Index Generation - Applies an LLM to generate meaningful textual indices for each engram.</li> <li>Embedding Indices - Uses an embedding plugin to convert each index into a vector representation.</li> <li>Publishing Results - Emits messages like <code>ENGRAM_COMPLETE</code>, <code>META_COMPLETE</code>, and <code>INDEX_COMPLETE</code> at various stages to notify downstream systems.</li> </ol> <p>Metrics are tracked throughout the pipeline using a <code>MetricsTracker</code> and returned on demand via the <code>on_acknowledge</code> method.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manages access to all system plugins.</p> <code>llm_summary</code> <code>dict</code> <p>Plugin used for generating summaries.</p> <code>llm_gen_indices</code> <code>dict</code> <p>Plugin used for generating indices from engrams.</p> <code>embedding_gen_embed</code> <code>dict</code> <p>Plugin used for generating embeddings for summaries and indices.</p> <code>db_document</code> <code>dict</code> <p>Plugin for document-level database access.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles deserialization of incoming observations.</p> <code>engram_builder</code> <code>dict[str, Engram]</code> <p>In-memory store of engrams awaiting completion.</p> <code>index_builder</code> <code>dict[str, Index]</code> <p>In-memory store of indices being constructed.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks metrics across each processing stage.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes the service to message topics.</p> <code>stop</code> <p>Stops the service and clears subscriptions.</p> <code>on_observation_complete</code> <p>Handles post-processing when an observation completes.</p> <code>_ generate_summary</code> <p>Creates a summary of the observation content. (not implemented yet)</p> <code>on_summary</code> <p>Callback after summary generation completes.</p> <code>generate_summary_embeddings</code> <p>Generates and attaches embeddings for a summary.</p> <code>generate_engrams</code> <p>Constructs engrams from observation data.</p> <code>on_engrams</code> <p>Callback after engram generation; handles index and embedding creation.</p> <code>gen_indices</code> <p>Uses an LLM to create indices from an engram.</p> <code>gen_embeddings</code> <p>Creates embeddings for generated indices.</p> <code>on_acknowledge</code> <p>Sends a metrics snapshot for observability/debugging.</p> Source code in <code>src/engramic/application/consolidate/consolidate_service.py</code> <pre><code>class ConsolidateService(Service):\n    \"\"\"\n    The ConsolidateService orchestrates the post-processing pipeline for completed observations,\n    coordinating summarization, engram generation, index generation, and embedding creation.\n\n    This service is triggered when an observation is marked complete and is responsible for the following:\n\n    1. **Summarization** - Generates a natural language summary from the observation using an LLM plugin.\n    2. **Embedding Summaries** - Uses an embedding plugin to create vector embeddings of the summary text.\n    3. **Engram Generation** - Extracts or constructs engrams from the observation's content.\n    4. **Index Generation** - Applies an LLM to generate meaningful textual indices for each engram.\n    5. **Embedding Indices** - Uses an embedding plugin to convert each index into a vector representation.\n    6. **Publishing Results** - Emits messages like `ENGRAM_COMPLETE`, `META_COMPLETE`, and `INDEX_COMPLETE` at various stages to notify downstream systems.\n\n    Metrics are tracked throughout the pipeline using a `MetricsTracker` and returned on demand via the\n    `on_acknowledge` method.\n\n    Attributes:\n        plugin_manager (PluginManager): Manages access to all system plugins.\n        llm_summary (dict): Plugin used for generating summaries.\n        llm_gen_indices (dict): Plugin used for generating indices from engrams.\n        embedding_gen_embed (dict): Plugin used for generating embeddings for summaries and indices.\n        db_document (dict): Plugin for document-level database access.\n        observation_repository (ObservationRepository): Handles deserialization of incoming observations.\n        engram_builder (dict[str, Engram]): In-memory store of engrams awaiting completion.\n        index_builder (dict[str, Index]): In-memory store of indices being constructed.\n        metrics_tracker (MetricsTracker): Tracks metrics across each processing stage.\n\n    Methods:\n        start(): Subscribes the service to message topics.\n        stop(): Stops the service and clears subscriptions.\n        on_observation_complete(observation_dict): Handles post-processing when an observation completes.\n        _ generate_summary(observation): Creates a summary of the observation content. (not implemented yet)\n        on_summary(summary_fut): Callback after summary generation completes.\n        generate_summary_embeddings(meta): Generates and attaches embeddings for a summary.\n        generate_engrams(observation): Constructs engrams from observation data.\n        on_engrams(engram_list_fut): Callback after engram generation; handles index and embedding creation.\n        gen_indices(index, id_in, engram): Uses an LLM to create indices from an engram.\n        gen_embeddings(id_and_index_dict, process_index): Creates embeddings for generated indices.\n        on_acknowledge(message_in): Sends a metrics snapshot for observability/debugging.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.llm_summary: dict[str, Any] = self.plugin_manager.get_plugin('llm', 'summary')\n        self.llm_gen_indices: dict[str, Any] = self.plugin_manager.get_plugin('llm', 'gen_indices')\n        self.embedding_gen_embed: dict[str, Any] = self.plugin_manager.get_plugin('embedding', 'gen_embed')\n        self.db_document: dict[str, Any] = self.plugin_manager.get_plugin('db', 'document')\n        self.observation_repository = ObservationRepository(self.db_document)\n        self.engram_builder: dict[str, Engram] = {}\n        self.metrics_tracker: MetricsTracker[ConsolidateMetric] = MetricsTracker[ConsolidateMetric]()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def on_observation_complete(self, observation_dict: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, observation_dict, observation_dict['input_id'])\n\n        # should run a task for this.\n        observation = self.observation_repository.load_dict(observation_dict)\n        self.metrics_tracker.increment(ConsolidateMetric.OBSERVATIONS_RECIEVED)\n\n        self.run_task(self._generate_summary_embeddings(observation))\n\n        generate_engrams = self.run_task(self._generate_engrams(observation))\n        generate_engrams.add_done_callback(self.on_engrams)\n\n    \"\"\"\n    ### Generate meta embeddings\n    \"\"\"\n\n    async def _generate_summary_embeddings(self, observation: Observation) -&gt; None:\n        if observation.meta.summary_full is None:\n            error = 'Summary full is none.'\n            raise ValueError(error)\n\n        plugin = self.embedding_gen_embed\n        embedding_list_ret = await asyncio.to_thread(\n            plugin['func'].gen_embed,\n            strings=[observation.meta.summary_full.text],\n            args=self.host.mock_update_args(plugin, 0, observation.input_id),\n        )\n\n        self.host.update_mock_data(plugin, embedding_list_ret, 0, observation.input_id)\n\n        embedding_list = embedding_list_ret[0]['embeddings_list']\n        observation.meta.summary_full.embedding = embedding_list[0]\n\n        self.send_message_async(Service.Topic.META_COMPLETE, asdict(observation.meta))\n\n    \"\"\"\n    ### Generate Engrams\n\n    Create engrams from the observation.\n    \"\"\"\n\n    async def _generate_engrams(self, observation: Observation) -&gt; Observation:\n        self.metrics_tracker.increment(ConsolidateMetric.ENGRAMS_GENERATED, len(observation.engram_list))\n\n        return observation\n\n    def on_engrams(self, observation_fut: Future[Any]) -&gt; None:\n        observation = observation_fut.result()\n        engram_list = observation.engram_list\n        input_id = observation.input_id\n\n        self.send_message_async(Service.Topic.ENGRAM_CREATED, {'input_id': input_id, 'count': len(engram_list)})\n\n        # Keep references so we can fill them in later\n        for engram in engram_list:\n            if self.engram_builder.get(engram.id) is None:\n                self.engram_builder[engram.id] = engram\n            else:\n                error = 'Engram ID Collision. During conslidation, two Engrams with the same IDs were detected.'\n                raise RuntimeError(error)\n\n        # 1) Generate indices for each engram\n        index_tasks = [self._gen_indices(i, engram.id, input_id, engram) for i, engram in enumerate(engram_list)]\n\n        indices_future = self.run_tasks(index_tasks)\n\n        indices_future.add_done_callback(self.on_indices_done)\n\n    async def _gen_indices(self, index: int, id_in: str, input_id: str, engram: Engram) -&gt; dict[str, Any]:\n        data_input = {'engram': engram}\n\n        prompt = PromptGenIndices(prompt_str='', input_data=data_input)\n        plugin = self.llm_gen_indices\n\n        response_schema = {'index_text_array': list[str]}\n\n        indices = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=response_schema,\n            args=self.host.mock_update_args(plugin, index, input_id),\n            images=None,\n        )\n\n        load_json = json.loads(indices[0]['llm_response'])\n        response_json: dict[str, Any] = {'index_text_array': []}\n\n        # generate context\n        context_string = 'Context: '\n\n        if engram.context is None:\n            error = 'None context found in engram.'\n            raise RuntimeError(error)\n\n        for item, key in engram.context.items():\n            if key != 'null':\n                context_string += f'{item}: {key}\\n'\n\n        # add in the context to each index.\n        for index_item in load_json['index_text_array']:\n            response_json['index_text_array'].append(context_string + ' Content: ' + index_item)\n\n        self.host.update_mock_data(plugin, indices, index, input_id)\n\n        number_indicies = len(load_json['index_text_array'])\n        self.send_message_async(Service.Topic.INDEX_CREATED, {'input_id': input_id, 'count': number_indicies})\n\n        self.metrics_tracker.increment(ConsolidateMetric.INDICES_GENERATED, len(indices))\n\n        if len(response_json['index_text_array']) == 0:\n            error = 'An empty index was created.'\n            raise RuntimeError(error)\n\n        return {'id': id_in, 'input_id': input_id, 'indices': response_json['index_text_array']}\n\n    # Once all indices are generated, generate embeddings\n    def on_indices_done(self, indices_list_fut: Future[Any]) -&gt; None:\n        # This is the accumulated result of each gen_indices(...) call\n        indices_list: dict[str, Any] = indices_list_fut.result()\n        # indices_list should have a key like 'gen_indices' -&gt; list[dict[str, Any]]\n        index_sets: list[dict[str, Any]] = indices_list['_gen_indices']\n\n        # 2) Generate embeddings for each index set\n        embed_tasks = [self._gen_embeddings(index_set, i) for i, index_set in enumerate(index_sets)]\n\n        embed_future = self.run_tasks(embed_tasks)\n\n        embed_future.add_done_callback(self.on_embeddings_done)\n\n    async def _gen_embeddings(self, id_and_index_dict: dict[str, Any], process_index: int) -&gt; dict[str, Any]:\n        indices = id_and_index_dict['indices']\n        engram_id: str = id_and_index_dict['id']\n        input_id: str = id_and_index_dict['input_id']\n\n        plugin = self.embedding_gen_embed\n\n        embedding_list_ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=indices, args=self.host.mock_update_args(plugin, process_index, input_id)\n        )\n\n        self.host.update_mock_data(plugin, embedding_list_ret, process_index, input_id)\n\n        embedding_list = embedding_list_ret[0]['embeddings_list']\n\n        self.metrics_tracker.increment(ConsolidateMetric.EMBEDDINGS_GENERATED, len(embedding_list))\n\n        # Convert raw embeddings to Index objects and attach them\n        try:\n            index_array: list[Index] = []\n            for i, vec in enumerate(embedding_list):\n                index = Index(indices[i], vec)\n                index_array.append(index)\n        except Exception:\n            logging.exception('Exception caught.')\n\n        self.engram_builder[engram_id].indices = index_array\n        serialized_index_array = [asdict(index) for index in index_array]\n\n        # We can optionally notify about newly attached indices\n        self.send_message_async(\n            Service.Topic.INDEX_COMPLETE,\n            {'index': serialized_index_array, 'engram_id': engram_id, 'input_id': input_id},\n        )\n\n        # Return the ID so we know which engram was updated\n        return {'engram_id': engram_id, 'input_id': input_id}\n\n    # Once embeddings are generated, then we're truly done\n    def on_embeddings_done(self, embed_fut: Future[Any]) -&gt; None:\n        ret = embed_fut.result()  # ret should have 'gen_embeddings' -&gt; list of engram IDs\n\n        ret_dict = ret['_gen_embeddings']  # which IDs got their embeddings updated\n        input_id = ret_dict[0]['input_id']\n\n        # Now that embeddings exist, we can send \"ENGRAM_COMPLETE\" for each\n        engram_dict: list[dict[str, Any]] = []\n        engram_dict = [asdict(self.engram_builder[eid['engram_id']]) for eid in ret_dict]\n\n        self.send_message_async(Service.Topic.ENGRAM_COMPLETE, {'engram_array': engram_dict, 'input_id': input_id})\n\n        if __debug__:\n            self.host.update_mock_data_output(self, {'engram_array': engram_dict}, 0, input_id)\n\n        for eid in ret_dict:\n            del self.engram_builder[eid['engram_id']]\n\n    \"\"\"\n    ### Acknowledge\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/emgram/","title":"Engram Class","text":"<p>Represents a unit of memory that encapsulates a text fragment with rich metadata for semantic indexing and contextual relevance.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the engram.</p> <code>locations</code> <code>list[str]</code> <p>One or more file paths, URLs, or other locations associated with the engram.</p> <code>source_ids</code> <code>list[str]</code> <p>Identifiers of the original source documents from which the engram was derived.</p> <code>content</code> <code>str</code> <p>The main textual content of the engram.</p> <code>is_native_source</code> <code>bool</code> <p>Indicates if the content was directly extracted (True) or generated (False).</p> <code>context</code> <code>dict[str, str] | None</code> <p>Optional contextual metadata in key-value format to enhance retrieval or classification.</p> <code>indices</code> <code>list[Index] | None</code> <p>Optional semantic indices, typically for vector-based retrieval.</p> <code>meta_ids</code> <code>list[str] | None</code> <p>Optional list of metadata tags or identifiers relevant to the engram.</p> <code>library_ids</code> <code>list[str] | None</code> <p>Optional identifiers linking this engram to document groups or libraries.</p> <code>accuracy</code> <code>int | None</code> <p>Optional accuracy score assigned during validation (e.g., via Codify Service).</p> <code>relevancy</code> <code>int | None</code> <p>Optional relevancy score assigned during validation (e.g., via Codify Service).</p> <code>created_date</code> <code>int | None</code> <p>Optional Unix timestamp representing the creation time of the engram.</p> <p>Methods:</p> Name Description <code>generate_toml</code> <p>Serializes the engram into a TOML-formatted string, including non-null fields. Nested indices are flattened, and context is rendered as an inline TOML table.</p> Source code in <code>src/engramic/core/engram.py</code> <pre><code>@dataclass()\nclass Engram:\n    \"\"\"\n    Represents a unit of memory that encapsulates a text fragment with rich metadata\n    for semantic indexing and contextual relevance.\n\n    Attributes:\n        id (str): Unique identifier for the engram.\n        locations (list[str]): One or more file paths, URLs, or other locations associated with the engram.\n        source_ids (list[str]): Identifiers of the original source documents from which the engram was derived.\n        content (str): The main textual content of the engram.\n        is_native_source (bool): Indicates if the content was directly extracted (True) or generated (False).\n        context (dict[str, str] | None): Optional contextual metadata in key-value format to enhance retrieval or classification.\n        indices (list[Index] | None): Optional semantic indices, typically for vector-based retrieval.\n        meta_ids (list[str] | None): Optional list of metadata tags or identifiers relevant to the engram.\n        library_ids (list[str] | None): Optional identifiers linking this engram to document groups or libraries.\n        accuracy (int | None): Optional accuracy score assigned during validation (e.g., via Codify Service).\n        relevancy (int | None): Optional relevancy score assigned during validation (e.g., via Codify Service).\n        created_date (int | None): Optional Unix timestamp representing the creation time of the engram.\n\n    Methods:\n        generate_toml() -&gt; str:\n            Serializes the engram into a TOML-formatted string, including non-null fields.\n            Nested indices are flattened, and context is rendered as an inline TOML table.\n    \"\"\"\n\n    id: str\n    locations: list[str]\n    source_ids: list[str]\n    content: str\n    is_native_source: bool\n    context: dict[str, str] | None = None\n    indices: list[Index] | None = None\n    meta_ids: list[str] | None = None\n    library_ids: list[str] | None = None\n    accuracy: int | None = 0\n    relevancy: int | None = 0\n    created_date: int | None = None\n\n    def generate_toml(self) -&gt; str:\n        def toml_escape(value: str) -&gt; str:\n            return f'\"{value}\"'\n\n        def toml_list(values: list[str]) -&gt; str:\n            return '[' + ', '.join(toml_escape(v) for v in values) + ']'\n\n        lines = [\n            f'id = {toml_escape(self.id)}',\n            f'content = {toml_escape(self.content)}',\n            f'is_native_source = {str(self.is_native_source).lower()}',\n            f'locations = {toml_list(self.locations)}',\n            f'source_ids = {toml_list(self.source_ids)}',\n        ]\n\n        if self.meta_ids:\n            lines.append(f'meta_ids = {toml_list(self.meta_ids)}')\n\n        if self.library_ids:\n            lines.append(f'library_ids = {toml_list(self.library_ids)}')\n\n        if self.context:\n            # Assuming context has a render_toml() method or can be represented as a dict\n            inline = ', '.join(f'{k} = {toml_escape(v)}' for k, v in self.context.items())\n            lines.append(f'context = {{ {inline} }}')\n\n        if self.indices:\n            # Flatten the index section\n            for index in self.indices:\n                # Assuming index has `text` and `embedding` attributes\n                if index.text is None:\n                    error = 'Null text in generate_toml.'\n                    raise ValueError(error)\n\n                lines.extend([\n                    '[[indices]]',\n                    f'text = {toml_escape(index.text)}',\n                    f'embedding = {toml_escape(str(index.embedding))}',\n                ])\n\n        return '\\n'.join(lines)\n</code></pre>"},{"location":"reference/message_service/","title":"Message Service","text":"<p>               Bases: <code>BaseMessageService</code></p> <p>A system-level message handling service that provides runtime CPU profiling and metrics reporting.</p> <p>MessageService extends BaseMessageService to handle system-level control messages, enabling dynamic profiling using Python's built-in <code>cProfile</code> module and responding to acknowledgment messages for metrics tracking. It operates by subscribing to control topics and exposing runtime observability features.</p> <p>Responsibilities: - Handles <code>START_PROFILER</code> and <code>END_PROFILER</code> messages to control a CPU profiler. - Responds to <code>ACKNOWLEDGE</code> messages by emitting a status update with performance metrics. - Uses the <code>metrics_tracker</code> inherited from BaseMessageService to report accumulated metrics.</p> <p>Attributes:</p> Name Type Description <code>profiler</code> <code>Profile | None</code> <p>An optional CPU profiler used to capture runtime performance data. The profiler is started and stopped in response to specific control messages and saves output to a file named 'profile_output.prof'.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Resets the profiler during asynchronous initialization.</p> <code>start</code> <p>Subscribes to relevant system topics for profiler control and metric acknowledgment.</p> <code>stop</code> <p>Gracefully shuts down the message service.</p> <code>start_profiler</code> <p>Initializes and starts the CPU profiler.</p> <code>end_profiler</code> <p>Stops the profiler and dumps results to a profile file.</p> <code>on_acknowledge</code> <p>Sends a metric snapshot and service status in response to ACKNOWLEDGE messages.</p> Source code in <code>src/engramic/application/message/message_service.py</code> <pre><code>class MessageService(BaseMessageService):\n    \"\"\"\n    A system-level message handling service that provides runtime CPU profiling and metrics reporting.\n\n    MessageService extends BaseMessageService to handle system-level control messages, enabling\n    dynamic profiling using Python's built-in `cProfile` module and responding to acknowledgment\n    messages for metrics tracking. It operates by subscribing to control topics and exposing\n    runtime observability features.\n\n    Responsibilities:\n    - Handles `START_PROFILER` and `END_PROFILER` messages to control a CPU profiler.\n    - Responds to `ACKNOWLEDGE` messages by emitting a status update with performance metrics.\n    - Uses the `metrics_tracker` inherited from BaseMessageService to report accumulated metrics.\n\n    Attributes:\n        profiler (cProfile.Profile | None): An optional CPU profiler used to capture runtime performance data.\n            The profiler is started and stopped in response to specific control messages and saves output\n            to a file named 'profile_output.prof'.\n\n    Methods:\n        init_async(): Resets the profiler during asynchronous initialization.\n        start(): Subscribes to relevant system topics for profiler control and metric acknowledgment.\n        stop(): Gracefully shuts down the message service.\n        start_profiler(data): Initializes and starts the CPU profiler.\n        end_profiler(data): Stops the profiler and dumps results to a profile file.\n        on_acknowledge(message_in): Sends a metric snapshot and service status in response to ACKNOWLEDGE messages.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.profiler: cProfile.Profile | None = None\n\n    def init_async(self) -&gt; None:\n        super().init_async()\n        self.profiler = None\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.START_PROFILER, self.start_profiler)\n        self.subscribe(Service.Topic.END_PROFILER, self.end_profiler)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def start_profiler(self, data: dict[Any, Any]) -&gt; None:\n        if data is not None:\n            del data\n        logging.info('Start Profiler')\n        self.profiler = cProfile.Profile()\n        if self.profiler:\n            self.profiler.enable()\n\n    def end_profiler(self, data: dict[Any, Any]) -&gt; None:\n        if data is not None:\n            del data\n        logging.info('Stop Profiler')\n        if self.profiler:\n            self.profiler.disable()\n            self.profiler.dump_stats('profile_output.prof')\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/progress_service/","title":"Progress Service","text":"<p>               Bases: <code>Service</code></p> <p>Tracks and manages the processing progress of inputs (currently prompts and documents) within the Engramic system.</p> <p>This service listens for events related to engram and index creation, maintaining counters that reflect the processing state of each input. Once all expected engrams and indices are inserted, it emits a completion event for that input.</p> <p>Attributes:</p> Name Type Description <code>inputs</code> <code>dict[str, InputProgress]</code> <p>A mapping of input IDs to their corresponding progress state.</p> Inner Classes <p>InputProgress (dataclass): Tracks per-input counters for engrams and indices:     - engram_ctr (int | None): Total engrams created for the input.     - index_created_ctr (int | None): Number of index creation operations triggered.     - index_ctr (int | None): Total indices expected for the input.     - index_insert_ctr (int | None): Number of successfully inserted indices.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Asynchronously initializes the service.</p> <code>start</code> <p>Subscribes to system events relevant to input tracking and begins the service.</p> <code>on_input_create</code> <p>dict): Initializes progress tracking for a new input.</p> <code>on_engram_created</code> <p>dict): Increments the engram counter for the associated input.</p> <code>on_index_created</code> <p>dict): Updates index creation and expected index count for the input.</p> <code>_on_index_inserted</code> <p>dict): Increments the inserted index count and checks for input completion.</p> Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>class ProgressService(Service):\n    \"\"\"\n    Tracks and manages the processing progress of inputs (currently prompts and documents) within the Engramic system.\n\n    This service listens for events related to engram and index creation, maintaining counters\n    that reflect the processing state of each input. Once all expected engrams and indices are\n    inserted, it emits a completion event for that input.\n\n    Attributes:\n        inputs (dict[str, InputProgress]): A mapping of input IDs to their corresponding progress state.\n\n    Inner Classes:\n        InputProgress (dataclass): Tracks per-input counters for engrams and indices:\n            - engram_ctr (int | None): Total engrams created for the input.\n            - index_created_ctr (int | None): Number of index creation operations triggered.\n            - index_ctr (int | None): Total indices expected for the input.\n            - index_insert_ctr (int | None): Number of successfully inserted indices.\n\n    Methods:\n        init_async(): Asynchronously initializes the service.\n        start(): Subscribes to system events relevant to input tracking and begins the service.\n        on_input_create(msg: dict): Initializes progress tracking for a new input.\n        on_engram_created(msg: dict): Increments the engram counter for the associated input.\n        on_index_created(msg: dict): Updates index creation and expected index count for the input.\n        _on_index_inserted(msg: dict): Increments the inserted index count and checks for input completion.\n    \"\"\"\n\n    @dataclass\n    class InputProgress:\n        engram_ctr: int | None = 0\n        index_created_ctr: int | None = 0\n        index_ctr: int | None = 0\n        index_insert_ctr: int | None = 0\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.inputs: dict[str, Any] = {}\n\n    def init_async(self) -&gt; None:\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.INPUT_CREATED, self.on_input_create)\n        self.subscribe(Service.Topic.ENGRAM_CREATED, self.on_engram_created)\n        self.subscribe(Service.Topic.INDEX_CREATED, self.on_index_created)\n        self.subscribe(Service.Topic.INDEX_INSERTED, self._on_index_inserted)\n        super().start()\n\n    def on_input_create(self, msg: dict[Any, Any]) -&gt; None:\n        input_id = msg['input_id']\n        self.inputs[input_id] = ProgressService.InputProgress()\n\n    def on_engram_created(self, msg: dict[Any, Any]) -&gt; None:\n        input_id = msg['input_id']\n        counter = msg['count']\n\n        if input_id in self.inputs:\n            input_process = self.inputs[input_id]\n            input_process.engram_ctr += counter\n\n    def on_index_created(self, msg: dict[Any, Any]) -&gt; None:\n        input_id = msg['input_id']\n        counter = msg['count']\n\n        if input_id in self.inputs:\n            document_process = self.inputs[input_id]\n            document_process.index_ctr += counter\n            document_process.index_created_ctr += 1\n\n    def _on_index_inserted(self, msg: dict[Any, Any]) -&gt; None:\n        input_id = msg['input_id']\n        counter = msg['count']\n\n        if input_id in self.inputs:\n            input_process = self.inputs[input_id]\n            input_process.index_insert_ctr += counter\n\n            if (\n                input_process.engram_ctr == input_process.index_created_ctr\n                and input_process.index_ctr == input_process.index_insert_ctr\n            ):\n                self.send_message_async(Service.Topic.INPUT_COMPLETED, {'input_id': input_id})\n</code></pre>"},{"location":"reference/response_service/","title":"Response Service","text":"<p>               Bases: <code>Service</code></p> <p>ResponseService orchestrates AI response generation by integrating retrieval results, historical context, and prompt engineering. It coordinates plugin-managed large language models (LLMs), websockets for streaming, and metrics tracking.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Provides access to LLM and DB plugins.</p> <code>web_socket_manager</code> <code>WebsocketManager</code> <p>Manages live streaming over websocket.</p> <code>db_document_plugin</code> <code>dict</code> <p>Document store plugin interface.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Access point for loading engrams.</p> <code>llm_main</code> <code>dict</code> <p>Plugin for executing the main LLM-based response generation.</p> <code>instructions</code> <code>Prompt</code> <p>Placeholder prompt object for main prompt design.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks internal response metrics.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes to service topics and initializes websocket manager.</p> <code>stop</code> <p>Shuts down the websocket manager and stops the service.</p> <code>init_async</code> <p>Initializes the DB plugin connection asynchronously.</p> <code>on_retrieve_complete</code> <p>Triggered when retrieval results are received. Initiates engram and history fetch processes.</p> <code>_fetch_history</code> <p>Asynchronously fetches historical conversation context.</p> <code>_fetch_retrieval</code> <p>Loads engrams using retrieve result.</p> <code>on_fetch_data_complete</code> <p>Callback fired after both history and engrams are loaded; launches the main prompt generation task.</p> <code>main_prompt</code> <p>Constructs and submits the main prompt to the LLM plugin; formats and returns the response.</p> <code>on_main_prompt_complete</code> <p>Callback fired after main prompt response is generated; sends result and updates metrics.</p> <code>on_acknowledge</code> <p>Sends current metrics snapshot to monitoring topics.</p> Notes <ul> <li>Asynchronous database and model operations are handled via <code>asyncio.to_thread</code>.</li> <li>Debug mode transmits intermediate prompt input and final output via websocket topics.</li> <li>Metrics are captured throughout the response generation pipeline to monitor performance.</li> </ul> Source code in <code>src/engramic/application/response/response_service.py</code> <pre><code>class ResponseService(Service):\n    \"\"\"\n    ResponseService orchestrates AI response generation by integrating retrieval\n    results, historical context, and prompt engineering. It coordinates plugin-managed\n    large language models (LLMs), websockets for streaming, and metrics tracking.\n\n    Attributes:\n        plugin_manager (PluginManager): Provides access to LLM and DB plugins.\n        web_socket_manager (WebsocketManager): Manages live streaming over websocket.\n        db_document_plugin (dict): Document store plugin interface.\n        engram_repository (EngramRepository): Access point for loading engrams.\n        llm_main (dict): Plugin for executing the main LLM-based response generation.\n        instructions (Prompt): Placeholder prompt object for main prompt design.\n        metrics_tracker (MetricsTracker): Tracks internal response metrics.\n\n    Methods:\n        start(): Subscribes to service topics and initializes websocket manager.\n        stop(): Shuts down the websocket manager and stops the service.\n        init_async(): Initializes the DB plugin connection asynchronously.\n        on_retrieve_complete(retrieve_result_in): Triggered when retrieval results are received.\n            Initiates engram and history fetch processes.\n        _fetch_history(): Asynchronously fetches historical conversation context.\n        _fetch_retrieval(prompt_str, analysis, retrieve_result): Loads engrams using retrieve result.\n        on_fetch_data_complete(fut): Callback fired after both history and engrams are loaded;\n            launches the main prompt generation task.\n        main_prompt(prompt_str, analysis, engram_array, retrieve_result, history_array):\n            Constructs and submits the main prompt to the LLM plugin; formats and returns the response.\n        on_main_prompt_complete(fut): Callback fired after main prompt response is generated;\n            sends result and updates metrics.\n        on_acknowledge(message_in): Sends current metrics snapshot to monitoring topics.\n\n    Notes:\n        - Asynchronous database and model operations are handled via `asyncio.to_thread`.\n        - Debug mode transmits intermediate prompt input and final output via websocket topics.\n        - Metrics are captured throughout the response generation pipeline to monitor performance.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.web_socket_manager: WebsocketManager = WebsocketManager(host)\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.llm_main = self.plugin_manager.get_plugin('llm', 'response_main')\n        self.metrics_tracker: MetricsTracker[ResponseMetric] = MetricsTracker[ResponseMetric]()\n        ##\n        # Many methods are not ready to be until their async component is running.\n        # Do not call async context methods in the constructor.\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.RETRIEVE_COMPLETE, self.on_retrieve_complete)\n        self.web_socket_manager.init_async()\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await self.web_socket_manager.shutdown()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_retrieve_complete(self, retrieve_result_in: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, retrieve_result_in)\n\n        prompt_str = retrieve_result_in['prompt_str']\n        prompt_analysis = PromptAnalysis(**retrieve_result_in['analysis'])\n        retrieve_result = RetrieveResult(**retrieve_result_in['retrieve_response'])\n        input_id = retrieve_result.input_id\n        self.metrics_tracker.increment(ResponseMetric.RETRIEVES_RECIEVED)\n        fetch_engrams_task = self.run_tasks([\n            self._fetch_retrieval(\n                prompt_str=prompt_str, input_id=input_id, analysis=prompt_analysis, retrieve_result=retrieve_result\n            ),\n            self._fetch_history(),\n        ])\n        fetch_engrams_task.add_done_callback(self.on_fetch_data_complete)\n\n    \"\"\"\n    ### Fetch History &amp; Engram\n\n    Fetch engrams based on the IDs provided by the retrieve service.\n    \"\"\"\n\n    async def _fetch_history(self) -&gt; dict[str, Any]:\n        plugin = self.db_document_plugin\n        args = plugin['args']\n        args['history'] = 1\n\n        ret_val = await asyncio.to_thread(plugin['func'].fetch, table=DB.DBTables.HISTORY, ids=[], args=args)\n        history: dict[str, Any] = ret_val[0]\n        return history\n\n    async def _fetch_retrieval(\n        self, prompt_str: str, input_id: str, analysis: PromptAnalysis, retrieve_result: RetrieveResult\n    ) -&gt; dict[str, Any]:\n        engram_array: list[Engram] = await asyncio.to_thread(\n            self.engram_repository.load_batch_retrieve_result, retrieve_result\n        )\n\n        # assembled main_prompt, render engrams.\n        return {\n            'prompt_str': prompt_str,\n            'input_id': input_id,\n            'analysis': analysis,\n            'retrieve_result': retrieve_result,\n            'engram_array': engram_array,\n        }\n\n    def on_fetch_data_complete(self, fut: Future[Any]) -&gt; None:\n        exc = fut.exception()\n        if exc is not None:\n            raise exc\n        result = fut.result()\n        retrieval = result['_fetch_retrieval'][0]\n        history = result['_fetch_history'][0]\n\n        main_prompt_task = self.run_task(\n            self.main_prompt(\n                retrieval['prompt_str'],\n                retrieval['input_id'],\n                retrieval['analysis'],\n                retrieval['engram_array'],\n                retrieval['retrieve_result'],\n                history,\n            )\n        )\n        main_prompt_task.add_done_callback(self.on_main_prompt_complete)\n\n    \"\"\"\n    ### Main Prompt\n\n    Combine the previous stages to generate the response.\n    \"\"\"\n\n    async def main_prompt(\n        self,\n        prompt_str: str,\n        input_id: str,\n        analysis: PromptAnalysis,\n        engram_array: list[Engram],\n        retrieve_result: RetrieveResult,\n        history_array: dict[str, Any],\n    ) -&gt; Response:\n        self.metrics_tracker.increment(ResponseMetric.ENGRAMS_FETCHED, len(engram_array))\n\n        engram_dict_list = [asdict(engram) for engram in engram_array]\n\n        # build main prompt here\n        prompt = PromptMainPrompt(\n            prompt_str=prompt_str,\n            input_data={\n                'engram_list': engram_dict_list,\n                'history': history_array,\n                'working_memory': retrieve_result.conversation_direction,\n                'analysis': retrieve_result.analysis,\n            },\n        )\n\n        plugin = self.llm_main\n\n        response = await asyncio.to_thread(\n            plugin['func'].submit_streaming,\n            prompt=prompt,\n            websocket_manager=self.web_socket_manager,\n            args=self.host.mock_update_args(plugin),\n        )\n\n        if __debug__:\n            main_prompt = prompt.render_prompt()\n            self.send_message_async(\n                Service.Topic.DEBUG_MAIN_PROMPT_INPUT, {'main_prompt': main_prompt, 'ask_id': retrieve_result.ask_id}\n            )\n\n        self.host.update_mock_data(self.llm_main, response)\n\n        model = ''\n        if plugin['args'].get('model'):\n            model = plugin['args']['model']\n\n        response = response[0]['llm_response'].replace('$', 'USD ').replace('&lt;context&gt;', '').replace('&lt;/context&gt;', '')\n\n        response_inst = Response(\n            str(uuid.uuid4()), input_id, response, retrieve_result, prompt.prompt_str, analysis, model\n        )\n\n        return response_inst\n\n    def on_main_prompt_complete(self, fut: Future[Any]) -&gt; None:\n        result = fut.result()\n        self.metrics_tracker.increment(ResponseMetric.MAIN_PROMPTS_RUN)\n\n        self.send_message_async(Service.Topic.MAIN_PROMPT_COMPLETE, asdict(result))\n\n        if __debug__:\n            self.host.update_mock_data_output(self, asdict(result))\n\n    \"\"\"\n    ### Ack\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/retrieve_service/","title":"Retrieve Service","text":"<p>               Bases: <code>Service</code></p> <p>Manages semantic prompt retrieval and indexing by coordinating between vector/document databases, tracking metrics, and responding to system events.</p> <p>This service is responsible for receiving prompt submissions, retrieving relevant information using vector similarity, and handling the indexing and metadata enrichment process. It interfaces with plugin-managed databases and provides observability through metrics tracking.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Access point for system plugins, including vector and document DBs.</p> <code>vector_db_plugin</code> <code>dict</code> <p>Plugin used for vector database operations (e.g., semantic search).</p> <code>db_plugin</code> <code>dict</code> <p>Plugin for interacting with the document database.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Collects and resets retrieval-related metrics for monitoring.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Handles Meta object persistence and transformation.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Initializes database connections and plugin setup asynchronously.</p> <code>start</code> <p>Subscribes to system topics for prompt processing and indexing lifecycle.</p> <code>stop</code> <p>Cleans up the service and halts processing.</p> <code>submit</code> <p>Prompt): Begins the retrieval process and logs submission metrics.</p> <code>on_submit_prompt</code> <p>str): Converts raw prompt string to a Prompt object and submits for processing.</p> <code>on_index_complete</code> <p>dict): Converts index payload into Index objects and queues for insertion.</p> <code>_insert_engram_vector</code> <p>list[Index], engram_id: str): Asynchronously inserts semantic indices into vector DB.</p> <code>on_meta_complete</code> <p>dict): Loads and inserts metadata summary into the vector DB.</p> <code>insert_meta_vector</code> <p>Meta): Runs metadata vector insertion in a background thread.</p> <code>on_acknowledge</code> <p>str): Emits service metrics to the status channel and resets the tracker.</p> Source code in <code>src/engramic/application/retrieve/retrieve_service.py</code> <pre><code>class RetrieveService(Service):\n    \"\"\"\n    Manages semantic prompt retrieval and indexing by coordinating between vector/document databases,\n    tracking metrics, and responding to system events.\n\n    This service is responsible for receiving prompt submissions, retrieving relevant information using\n    vector similarity, and handling the indexing and metadata enrichment process. It interfaces with\n    plugin-managed databases and provides observability through metrics tracking.\n\n    Attributes:\n        plugin_manager (PluginManager): Access point for system plugins, including vector and document DBs.\n        vector_db_plugin (dict): Plugin used for vector database operations (e.g., semantic search).\n        db_plugin (dict): Plugin for interacting with the document database.\n        metrics_tracker (MetricsTracker): Collects and resets retrieval-related metrics for monitoring.\n        meta_repository (MetaRepository): Handles Meta object persistence and transformation.\n\n    Methods:\n        init_async(): Initializes database connections and plugin setup asynchronously.\n        start(): Subscribes to system topics for prompt processing and indexing lifecycle.\n        stop(): Cleans up the service and halts processing.\n\n        submit(prompt: Prompt): Begins the retrieval process and logs submission metrics.\n        on_submit_prompt(data: str): Converts raw prompt string to a Prompt object and submits for processing.\n\n        on_index_complete(index_message: dict): Converts index payload into Index objects and queues for insertion.\n        _insert_engram_vector(index_list: list[Index], engram_id: str): Asynchronously inserts semantic indices into vector DB.\n\n        on_meta_complete(meta_dict: dict): Loads and inserts metadata summary into the vector DB.\n        insert_meta_vector(meta: Meta): Runs metadata vector insertion in a background thread.\n\n        on_acknowledge(message_in: str): Emits service metrics to the status channel and resets the tracker.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.vector_db_plugin = host.plugin_manager.get_plugin('vector_db', 'db')\n        self.db_plugin = host.plugin_manager.get_plugin('db', 'document')\n        self.metrics_tracker: MetricsTracker[RetrieveMetric] = MetricsTracker[RetrieveMetric]()\n        self.meta_repository: MetaRepository = MetaRepository(self.db_plugin)\n\n    def init_async(self) -&gt; None:\n        self.db_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.SUBMIT_PROMPT, self.on_submit_prompt)\n        self.subscribe(Service.Topic.INDEX_COMPLETE, self.on_index_complete)\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    # when called from monitor service\n    def on_submit_prompt(self, msg: dict[Any, Any]) -&gt; None:\n        prompt_str = msg['prompt_str']\n        self.submit(Prompt(prompt_str))\n\n    # when used from main\n    def submit(self, prompt: Prompt) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, asdict(prompt))\n\n        self.metrics_tracker.increment(RetrieveMetric.PROMPTS_SUBMITTED)\n        retrieval = Ask(str(uuid.uuid4()), prompt, self.plugin_manager, self.metrics_tracker, self.db_plugin, self)\n        retrieval.get_sources()\n\n        async def send_message() -&gt; None:\n            self.send_message_async(Service.Topic.INPUT_CREATED, {'input_id': prompt.prompt_id})\n\n        self.run_task(send_message())\n\n    def on_index_complete(self, index_message: dict[str, Any]) -&gt; None:\n        raw_index: list[dict[str, Any]] = index_message['index']\n        engram_id: str = index_message['engram_id']\n        input_id: str = index_message['input_id']\n        index_list: list[Index] = [Index(**item) for item in raw_index]\n        self.run_task(self._insert_engram_vector(index_list, engram_id, input_id))\n\n    async def _insert_engram_vector(self, index_list: list[Index], engram_id: str, input_id: str) -&gt; None:\n        plugin = self.vector_db_plugin\n        self.vector_db_plugin['func'].insert(\n            collection_name='main', index_list=index_list, obj_id=engram_id, args=plugin['args']\n        )\n\n        self.send_message_async(Service.Topic.INDEX_INSERTED, {'input_id': input_id, 'count': len(index_list)})\n\n        self.metrics_tracker.increment(RetrieveMetric.EMBEDDINGS_ADDED_TO_VECTOR)\n\n    def on_meta_complete(self, meta_dict: dict[str, Any]) -&gt; None:\n        meta = self.meta_repository.load(meta_dict)\n        self.run_task(self.insert_meta_vector(meta))\n        self.metrics_tracker.increment(RetrieveMetric.META_ADDED_TO_VECTOR)\n\n    async def insert_meta_vector(self, meta: Meta) -&gt; None:\n        plugin = self.vector_db_plugin\n        await asyncio.to_thread(\n            self.vector_db_plugin['func'].insert,\n            collection_name='meta',\n            index_list=[meta.summary_full],\n            obj_id=meta.id,\n            args=plugin['args'],\n        )\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/scan/","title":"Scan Service","text":"<p>               Bases: <code>Media</code></p> <p>Coordinates the semantic analysis of a submitted document, converting it into engrams and metadata by orchestrating image extraction, LLM-driven summarization, and structured information parsing.</p> <p>This class performs the following operations: - Loads a PDF document and converts each page into Base64-encoded images. - Generates an initial summary using a few preview pages. - Scans each page with an LLM plugin to extract semantic content. - Parses scan results into Engrams and assembles a full summary. - Constructs a Meta object and emits an observation to the system.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the scan session.</p> <code>service</code> <code>SenseService</code> <p>Reference to the parent service orchestrating the scan.</p> <code>page_images</code> <code>list[str]</code> <p>Base64-encoded representations of PDF pages.</p> <code>sense_initial_summary</code> <code>Plugin</code> <p>Plugin used to generate an initial summary from early pages.</p> Constants <p>DPI (int): Resolution used for image extraction. DPI_DIVISOR (int): Used to calculate zoom level for rendering. TEST_PAGE_LIMIT (int): Max number of pages scanned during test runs. MAX_CHUNK_SIZE (int): Max text length before recursive engram chunking. SHORT_SUMMARY_PAGE_COUNT (int): Number of pages used to generate initial summary. SECTION, H1, H3 (int): Enum-like values to manage tag-based engram splitting depth.</p> <p>Methods:</p> Name Description <code>parse_media_resource</code> <p>Loads and validates a PDF, then initiates conversion.</p> <code>_convert_pages_to_images</code> <p>Converts PDF pages to images asynchronously.</p> <code>_page_to_image</code> <p>Converts and encodes a single PDF page to Base64.</p> <code>_on_pages_converted</code> <p>Handles post-conversion logic, triggering initial summary.</p> <code>_generate_short_summary</code> <p>Sends preview pages to the LLM for initial semantic scan.</p> <code>_on_short_summary</code> <p>Kicks off page-by-page scanning after initial summary.</p> <code>_scan_page</code> <p>Sends a single page to the LLM plugin and extracts structured data.</p> <code>_on_pages_scanned</code> <p>Extracts structured context and initiates full summary.</p> <code>_process_engrams</code> <p>Recursively splits and constructs Engrams from HTML text.</p> <code>_generate_full_summary</code> <p>Uses full document content to generate final semantic summary.</p> <code>_on_generate_full_summary</code> <p>Wraps summary and Engrams into a Meta and emits final Observation.</p> Source code in <code>src/engramic/application/sense/scan.py</code> <pre><code>class Scan(Media):\n    \"\"\"\n    Coordinates the semantic analysis of a submitted document, converting it into engrams and metadata\n    by orchestrating image extraction, LLM-driven summarization, and structured information parsing.\n\n    This class performs the following operations:\n    - Loads a PDF document and converts each page into Base64-encoded images.\n    - Generates an initial summary using a few preview pages.\n    - Scans each page with an LLM plugin to extract semantic content.\n    - Parses scan results into Engrams and assembles a full summary.\n    - Constructs a Meta object and emits an observation to the system.\n\n    Attributes:\n        id (str): Unique identifier for the scan session.\n        service (SenseService): Reference to the parent service orchestrating the scan.\n        page_images (list[str]): Base64-encoded representations of PDF pages.\n        sense_initial_summary (Plugin): Plugin used to generate an initial summary from early pages.\n\n    Constants:\n        DPI (int): Resolution used for image extraction.\n        DPI_DIVISOR (int): Used to calculate zoom level for rendering.\n        TEST_PAGE_LIMIT (int): Max number of pages scanned during test runs.\n        MAX_CHUNK_SIZE (int): Max text length before recursive engram chunking.\n        SHORT_SUMMARY_PAGE_COUNT (int): Number of pages used to generate initial summary.\n        SECTION, H1, H3 (int): Enum-like values to manage tag-based engram splitting depth.\n\n    Methods:\n        parse_media_resource(document): Loads and validates a PDF, then initiates conversion.\n        _convert_pages_to_images(pdf, start_page, end_page): Converts PDF pages to images asynchronously.\n        _page_to_image(pdf, page_number): Converts and encodes a single PDF page to Base64.\n        _on_pages_converted(future): Handles post-conversion logic, triggering initial summary.\n        _generate_short_summary(): Sends preview pages to the LLM for initial semantic scan.\n        _on_short_summary(future): Kicks off page-by-page scanning after initial summary.\n        _scan_page(page_num): Sends a single page to the LLM plugin and extracts structured data.\n        _on_pages_scanned(future): Extracts structured context and initiates full summary.\n        _process_engrams(text_in, context, depth): Recursively splits and constructs Engrams from HTML text.\n        _generate_full_summary(summary): Uses full document content to generate final semantic summary.\n        _on_generate_full_summary(future): Wraps summary and Engrams into a Meta and emits final Observation.\n    \"\"\"\n\n    DPI = 72\n    DPI_DIVISOR = 72\n    TEST_PAGE_LIMIT = 100\n    MAX_CHUNK_SIZE = 1200\n    SHORT_SUMMARY_PAGE_COUNT = 4\n    MAX_DEPTH = 3\n    SECTION = 0\n    H1 = 1\n    H3 = 2\n\n    def __init__(self, parent_service: SenseService, scan_id: str):\n        self.id = scan_id\n        self.service = parent_service\n        self.page_images: list[str] = []\n        self.sense_initial_summary = self.service.sense_initial_summary\n\n    def parse_media_resource(self, document: Document) -&gt; None:\n        file_path: Traversable | Path\n        if document.root_directory == Document.Root.RESOURCE:\n            file_path = files(document.file_path).joinpath(document.file_name)\n        elif document.root_directory == Document.Root.DATA:\n            file_path = Path(document.file_path) / document.file_name\n\n        self.document = document\n        self.source_id = document.id\n\n        try:\n            with file_path.open('rb') as file_ptr:\n                pdf_document: fitz.Document = fitz.open(stream=file_ptr.read(), filetype='pdf')\n\n                total_pages = pdf_document.page_count\n\n                if total_pages == 0:\n                    error = 'PDF loaded with zero page count.'\n                    raise RuntimeError(error)\n\n                self.page_images = [''] * total_pages\n                self.total_pages = total_pages\n                self._convert_pages_to_images(pdf_document, 0, total_pages)\n        except FileNotFoundError as e:\n            error = f'File {document.file_name} failed to open. {e}'\n            logging.exception(error)\n        except RuntimeError as e:\n            error = f'File {document.file_name} failed to open. {e}'\n            logging.exception(error)\n\n    def _convert_pages_to_images(self, pdf: fitz.Document, start_page: int, end_page: int) -&gt; None:\n        coroutines = [self._page_to_image(pdf, i) for i in range(start_page, end_page)]\n        future = self.service.run_tasks(coroutines)\n        future.add_done_callback(self._on_pages_converted)\n\n    async def _page_to_image(self, pdf: fitz.Document, page_number: int) -&gt; bool:\n        page = pdf.load_page(page_number)\n        zoom = Scan.DPI / Scan.DPI_DIVISOR\n        matrix = fitz.Matrix(zoom, zoom)\n        pix = page.get_pixmap(matrix=matrix)\n\n        # Convert the pixmap to PNG bytes in memory\n        img_bytes = pix.tobytes('png')\n\n        # Encode to Base64\n        encoded_img = base64.b64encode(img_bytes).decode('utf-8')\n\n        # Store the Base64 string in image_array\n        self.page_images[page_number] = encoded_img\n\n        return True\n\n    def _on_pages_converted(self, future: Future[Any]) -&gt; None:\n        ret_functions = future.result()\n        del ret_functions\n        summary_future = self.service.run_task(self._generate_short_summary())\n        summary_future.add_done_callback(self._on_short_summary)\n\n    async def _generate_short_summary(self) -&gt; Any:\n        plugin = self.sense_initial_summary\n        summary_images = self.page_images[: Scan.SHORT_SUMMARY_PAGE_COUNT]\n\n        prompt = PromptGenMeta(input_data={'file_path': self.document.file_path, 'file_name': self.document.file_name})\n\n        structured_response = {\n            'file_path': str,\n            'file_name': str,\n            'subject': str,\n            'audience': str,\n            'document_title': str,\n            'document_format': str,\n            'document_type': str,\n            'toc': str,\n            'summary_initial': str,\n            'author': str,\n            'date': str,\n            'version': str,\n        }\n\n        ret = self.sense_initial_summary['func'].submit(\n            prompt=prompt,\n            images=summary_images,\n            structured_schema=structured_response,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        initial_scan = json.loads(ret[0]['llm_response'])\n\n        return initial_scan\n\n    def _on_short_summary(self, future: Future[Any]) -&gt; None:\n        result = future.result()\n        self.inital_scan = result\n\n        self.total_pages = min(Scan.TEST_PAGE_LIMIT, self.total_pages)\n        coroutines = [self._scan_page(i) for i in range(self.total_pages)]\n        future = self.service.run_tasks(coroutines)\n\n        future.add_done_callback(self._on_pages_scanned)\n\n    async def _scan_page(self, page_num: int) -&gt; Any:\n        plugin = self.service.sense_scan_page\n\n        initial_scan_copy = copy.copy(self.inital_scan)\n        initial_scan_copy.update({'page_number': page_num + 1})\n\n        prompt_scan = PromptScanPage(input_data=initial_scan_copy)\n\n        image = self.page_images[page_num]\n\n        ret = await asyncio.to_thread(\n            self.sense_initial_summary['func'].submit,\n            prompt=prompt_scan,\n            images=[image],\n            structured_schema=None,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        return ret[0]['llm_response']\n\n    def _on_pages_scanned(self, future: Future[Any]) -&gt; None:\n        result = future.result()\n\n        self.meta_id = str(uuid.uuid4())\n\n        context: dict[str, str] = {}\n\n        context = copy.copy(self.inital_scan)\n        del context['summary_initial']\n        del context['toc']\n\n        self.engrams: list[Engram] = []\n\n        assembled = ''\n        for page in result['_scan_page']:\n            assembled += page\n\n        # matches1 = re.findall(r'&lt;h1[^&gt;]*&gt;(.*?)&lt;/h1&gt;', assembled, re.DOTALL | re.IGNORECASE)\n        # matches2 = re.findall(r'&lt;h3[^&gt;]*&gt;(.*?)&lt;/h3&gt;', assembled, re.DOTALL | re.IGNORECASE)\n\n        self._process_engrams(assembled, context)\n\n        future = self.service.run_task(self._generate_full_summary(assembled))\n        future.add_done_callback(self._on_generate_full_summary)\n\n    def _process_engrams(self, text_in: str, context: dict[str, str], depth: int = 0) -&gt; None:\n        if len(text_in) &gt; Scan.MAX_CHUNK_SIZE and depth &lt; Scan.MAX_DEPTH:\n            tag = ''\n            if depth == Scan.SECTION:\n                tag = 'section'\n            if depth == Scan.H1:\n                tag = 'h1'\n            if depth == Scan.H3:\n                tag = 'h3'\n\n            depth += 1\n\n            pattern = rf'(?=&lt;{tag}[^&gt;]*&gt;)'\n            parts = re.split(pattern, text_in, flags=re.IGNORECASE)\n            clean_parts = [part.strip() for part in parts if part.strip()]\n\n            for part in clean_parts:\n                match = re.search(rf'&lt;{tag}[^&gt;]*&gt;(.*?)&lt;/{tag}&gt;', text_in, re.DOTALL | re.IGNORECASE)\n                context_copy = context\n                if match:\n                    tag_str = match.group(1).strip()\n                    context.update({tag: tag_str})\n                    context_copy = copy.copy(context)\n\n                self._process_engrams(part, context_copy, depth)\n\n        else:\n            engram = Engram(\n                str(uuid.uuid4()),\n                [self.inital_scan['file_path']],\n                [self.document.get_source_id()],\n                text_in,\n                True,\n                context,\n                None,\n                [self.meta_id],\n                None,  # library\n                None,  # accuracy\n                None,  # relevancy\n                int(datetime.now(timezone.utc).timestamp()),\n            )\n\n            self.engrams.append(engram)\n\n    async def _generate_full_summary(self, summary: str) -&gt; Any:\n        plugin = self.service.sense_full_summary\n\n        initial_scan_copy = copy.copy(self.inital_scan)\n        initial_scan_copy.update({'full_text': summary})\n\n        prompt = PromptGenFullSummary(input_data=initial_scan_copy)\n\n        structure = {'summary_full': str, 'keywords': str}\n\n        ret = self.service.sense_full_summary['func'].submit(\n            prompt=prompt, images=None, structured_schema=structure, args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        llm_response = ret[0]['llm_response']\n\n        return llm_response\n\n    def _on_generate_full_summary(self, future: Future[Any]) -&gt; None:\n        results = json.loads(future.result())\n\n        meta = Meta(\n            self.meta_id,\n            [self.inital_scan['file_path']],\n            [hashlib.md5(self.inital_scan['file_path'].encode('utf-8')).hexdigest()],\n            results['keywords'].split(','),\n            self.inital_scan['summary_initial'],\n            Index(results['summary_full']),\n        )\n\n        observation = Observation(\n            str(uuid.uuid4()), self.document.id, meta, self.engrams, datetime.now(timezone.utc).timestamp()\n        )\n\n        self.service.host.update_mock_data_output(self.service, asdict(observation))\n\n        self.service.send_message_async(Service.Topic.OBSERVATION_COMPLETE, asdict(observation))\n</code></pre>"},{"location":"reference/sense_service/","title":"Sense Service","text":"<p>               Bases: <code>Service</code></p> <p>Sense currently support documents as it's only input system. Other formats that are not document based will be available in the future.</p> <p>This service listens for document submission events, initializes a scan process that parses the media resource, and notifies the system of newly created inputs.</p> <p>Attributes:</p> Name Type Description <code>sense_initial_summary</code> <code>Plugin</code> <p>Plugin for generating an inital summary of the document.</p> <code>sense_scan_page</code> <code>Plugin</code> <p>Plugin for scanning and interpreting document content.</p> <code>sense_full_summary</code> <code>Plugin</code> <p>Plugin for producing full document summaries.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Initializes the service asynchronously and sets up any required connections or state.</p> <code>start</code> <p>Subscribes to the system topic for document submissions.</p> <code>on_document_submit</code> <p>dict): Extracts file information from a message and submits the document.</p> <code>submit_document</code> <p>Document): Triggers scanning of the submitted document and sends async notification.</p> Source code in <code>src/engramic/application/sense/sense_service.py</code> <pre><code>class SenseService(Service):\n    \"\"\"\n    Sense currently support documents as it's only input system. Other formats that are not document based will be available in the future.\n\n    This service listens for document submission events, initializes a scan process that parses the media\n    resource, and notifies the system of newly created inputs.\n\n    Attributes:\n        sense_initial_summary (Plugin): Plugin for generating an inital summary of the document.\n        sense_scan_page (Plugin): Plugin for scanning and interpreting document content.\n        sense_full_summary (Plugin): Plugin for producing full document summaries.\n\n    Methods:\n        init_async(): Initializes the service asynchronously and sets up any required connections or state.\n        start(): Subscribes to the system topic for document submissions.\n        on_document_submit(msg: dict): Extracts file information from a message and submits the document.\n        submit_document(document: Document): Triggers scanning of the submitted document and sends async notification.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.sense_initial_summary = host.plugin_manager.get_plugin('llm', 'sense_initial_summary')\n        self.sense_scan_page = host.plugin_manager.get_plugin('llm', 'sense_scan')\n        self.sense_full_summary = host.plugin_manager.get_plugin('llm', 'sense_full_summary')\n\n    def init_async(self) -&gt; None:\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.SUBMIT_DOCUMENT, self.on_document_submit)\n        super().start()\n\n    def on_document_submit(self, msg: dict[Any, Any]) -&gt; None:\n        file_path = msg['file_path']\n        file_name = msg['file_name']\n        path_type = msg['root_directory']\n        self.submit_document(Document(Document.Root(path_type), file_path, file_name))\n\n    def submit_document(self, document: Document) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(\n                self,\n                {\n                    'file_path': document.file_path,\n                    'file_name': document.file_name,\n                    'root_directory': document.root_directory.value,\n                },\n            )\n\n        scan = Scan(self, str(uuid.uuid4()))\n        scan.parse_media_resource(document)\n\n        async def send_message() -&gt; None:\n            self.send_message_async(Service.Topic.INPUT_CREATED, {'input_id': document.id})\n\n        self.run_task(send_message())\n</code></pre>"},{"location":"reference/storage_service/","title":"Storage Service","text":"<p>               Bases: <code>Service</code></p> <p>A service responsible for persisting runtime data artifacts within the Engramic system.</p> <p>StorageService listens for various system events and saves corresponding data\u2014including observations, engrams, metadata, and prompt histories\u2014via plugin-based repositories. It also tracks metrics for each type of saved entity to facilitate performance monitoring and operational insights.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Provides access to system plugins, including database integrations.</p> <code>db_document_plugin</code> <p>Plugin used by repositories for data persistence.</p> <code>history_repository</code> <code>HistoryRepository</code> <p>Manages saving of prompt/response history data.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles saving of Observation entities.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Handles saving of Engram entities.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Handles saving of Meta configuration entities.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks counts of saved items for metric reporting.</p> <p>Methods:</p> Name Description <code>start</code> <p>Registers the service to relevant message topics and begins operation.</p> <code>init_async</code> <p>Connects to the database plugin asynchronously before full service startup.</p> <code>on_engram_complete</code> <p>Callback for storing completed engram batches.</p> <code>on_observation_complete</code> <p>Callback for storing completed observations.</p> <code>on_prompt_complete</code> <p>Callback for storing completed prompt/response history.</p> <code>on_meta_complete</code> <p>Callback for storing finalized meta configuration.</p> <code>save_observation</code> <p>Coroutine to persist observations and update metrics.</p> <code>save_history</code> <p>Coroutine to persist prompt/response history and update metrics.</p> <code>save_engram</code> <p>Coroutine to persist engram data and update metrics.</p> <code>save_meta</code> <p>Coroutine to persist metadata and update metrics.</p> <code>on_acknowledge</code> <p>Collects current metrics and publishes service status.</p> Source code in <code>src/engramic/application/storage/storage_service.py</code> <pre><code>class StorageService(Service):\n    \"\"\"\n    A service responsible for persisting runtime data artifacts within the Engramic system.\n\n    StorageService listens for various system events and saves corresponding data\u2014including observations,\n    engrams, metadata, and prompt histories\u2014via plugin-based repositories. It also tracks metrics for each\n    type of saved entity to facilitate performance monitoring and operational insights.\n\n    Attributes:\n        plugin_manager (PluginManager): Provides access to system plugins, including database integrations.\n        db_document_plugin: Plugin used by repositories for data persistence.\n        history_repository (HistoryRepository): Manages saving of prompt/response history data.\n        observation_repository (ObservationRepository): Handles saving of Observation entities.\n        engram_repository (EngramRepository): Handles saving of Engram entities.\n        meta_repository (MetaRepository): Handles saving of Meta configuration entities.\n        metrics_tracker (MetricsTracker): Tracks counts of saved items for metric reporting.\n\n    Methods:\n        start(): Registers the service to relevant message topics and begins operation.\n        init_async(): Connects to the database plugin asynchronously before full service startup.\n        on_engram_complete(engram_dict): Callback for storing completed engram batches.\n        on_observation_complete(response): Callback for storing completed observations.\n        on_prompt_complete(response_dict): Callback for storing completed prompt/response history.\n        on_meta_complete(meta_dict): Callback for storing finalized meta configuration.\n        save_observation(response): Coroutine to persist observations and update metrics.\n        save_history(response): Coroutine to persist prompt/response history and update metrics.\n        save_engram(engram): Coroutine to persist engram data and update metrics.\n        save_meta(meta): Coroutine to persist metadata and update metrics.\n        on_acknowledge(message_in): Collects current metrics and publishes service status.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.history_repository: HistoryRepository = HistoryRepository(self.db_document_plugin)\n        self.observation_repository: ObservationRepository = ObservationRepository(self.db_document_plugin)\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.meta_repository: MetaRepository = MetaRepository(self.db_document_plugin)\n        self.metrics_tracker: MetricsTracker[StorageMetric] = MetricsTracker[StorageMetric]()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_prompt_complete)\n        self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n        self.subscribe(Service.Topic.ENGRAM_COMPLETE, self.on_engram_complete)\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        super().start()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_engram_complete(self, engram_dict: dict[str, Any]) -&gt; None:\n        engram_batch = self.engram_repository.load_batch_dict(engram_dict['engram_array'])\n        for engram in engram_batch:\n            self.run_task(self.save_engram(engram))\n\n    def on_observation_complete(self, response: Observation) -&gt; None:\n        self.run_task(self.save_observation(response))\n\n    def on_prompt_complete(self, response_dict: dict[Any, Any]) -&gt; None:\n        response = Response(**response_dict)\n        self.run_task(self.save_history(response))\n\n    def on_meta_complete(self, meta_dict: dict[str, str]) -&gt; None:\n        meta: Meta = self.meta_repository.load(meta_dict)\n        self.run_task(self.save_meta(meta))\n\n    async def save_observation(self, response: Observation) -&gt; None:\n        self.observation_repository.save(response)\n        self.metrics_tracker.increment(StorageMetric.OBSERVATION_SAVED)\n        logging.debug('Storage service saving observation.')\n\n    async def save_history(self, response: Response) -&gt; None:\n        await asyncio.to_thread(self.history_repository.save_history, response)\n        self.metrics_tracker.increment(StorageMetric.HISTORY_SAVED)\n        logging.debug('Storage service saving history.')\n\n    async def save_engram(self, engram: Engram) -&gt; None:\n        await asyncio.to_thread(self.engram_repository.save_engram, engram)\n        self.metrics_tracker.increment(StorageMetric.ENGRAM_SAVED)\n        logging.debug('Storage service saving engram.')\n\n    async def save_meta(self, meta: Meta) -&gt; None:\n        logging.debug('Storage service saving meta.')\n        await asyncio.to_thread(self.meta_repository.save, meta)\n        self.metrics_tracker.increment(StorageMetric.META_SAVED)\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"}]}