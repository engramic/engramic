{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engramic API Reference","text":"<p>Engramic is an inference engine. It's special because it natively supports long-term, contextual memory. This specialty makes it very adept at building applications that need to perform reliable question and answer over a corpus of data over time. Additionally, long-term, contextual memory provides the means for basic learning capabilities, resulting in deeper, more thoughtful responses.</p> <p>Engramic is pre-alpha. It's a great time to start working with these core systems for some developers, but we have yet to complete many important features. In other words, use of this code base will require some development experience and the ability to work in maturing environments. The flip side, is that a new community is forming and as a pioneer, you have an opportunity to get in early so that you can someday tell your friends, I used Engramic before it was cool.</p> <p>There is currently no support for the following:</p> <ul> <li>There is no support for individual users.</li> <li>There is no HTTP(s) interface at this time.</li> <li>There are no fallbacks if API calls fail.</li> <li>Windows and MacOS is not being tested as part of our release process.</li> </ul> <p>These features, along with others, will be available in the near future.</p> <p>What Engramic is ready for:</p> <ul> <li>Proof of concepts focused on folder directories with 10 or so PDFs with less than 100 pages.</li> <li>Research related to long term memory.</li> <li>Developers looking to support Engramic.</li> </ul> <p>For an evergreen overview of Engramic, visit our online Knowledge Base.</p>"},{"location":"#introduction-to-engramic","title":"Introduction to Engramic","text":"<pre><code>flowchart TD\n    %% Define node styles\n    classDef process fill:#f9f9f9,stroke:#333,stroke-width:1px,rounded:true\n    classDef io fill:#e8f4ff,stroke:#4a86e8,stroke-width:1px,rounded:true\n    classDef external fill:#f0fff0,stroke:#2d862d,stroke-width:1px,rounded:true\n\n    %% Input and external processes\n    prompt([User Prompt]):::io\n    stream([User Stream]):::io\n    sense[Sense]:::external\n\n    respond --&gt; stream\n\n    %% Core processes in learning loop\n    subgraph \"Engramic Learning Loop\"\n      direction RL\n      consolidate[Consolidate]:::process\n      retrieve[Retrieve]:::process\n      respond[Respond]:::process\n      codify[Codify]:::process\n\n      consolidate --&gt; retrieve\n      retrieve --&gt; respond\n      respond --&gt; codify\n      codify --&gt; consolidate\n    end\n\n    %% External connections\n    prompt --&gt; retrieve\n    sense --&gt; consolidate\n\n</code></pre> <p>Engramic's core services. Engramic uses a learning loop, building memories from responses and responses from memories.</p>"},{"location":"#why-engramic","title":"Why Engramic?","text":"<p>Engramic is designed to learn from your unstructured, proprietary data using any large language model (LLM). Understanding comes from synthesizing the information, asking questions, identifying what\u2019s meaningful, and connecting it to prior knowledge and related context. Learning is an iterative process\u2014not a linear one. That\u2019s why we believe a large context window alone doesn\u2019t solve the challenge of truly understanding a dataset. This belief, shaped by two years of research, is what inspired Engramic\u2019s design.</p>"},{"location":"#engramic-architecture-philosophy","title":"Engramic Architecture Philosophy","text":"<ul> <li> <p>Modular   The plugin system allows easy switching between LLMs, databases, vector databases, and embedding tools.</p> </li> <li> <p>Scalable   Built as a set of independent services, Engramic can run on a single machine or be distributed across multiple systems.</p> </li> <li> <p>Fast   Optimized for usage patterns involving many blocking API calls, ensuring responsive performance.</p> </li> <li> <p>Extensible   Easily create custom services or plugins to extend functionality.</p> </li> </ul>"},{"location":"#engramic-core-concepts","title":"Engramic Core Concepts","text":"<ul> <li> <p>Memory   Supports both short-term and long-term memory mechanisms.</p> </li> <li> <p>Engram   The fundamental unit of information, which includes content and base and customizable contextual metadata</p> </li> <li> <p>Citable Engrams   External documents or media that are directly referenced. Citable Engrams are high-fidelity textual representations of the media.</p> </li> <li> <p>Long-Term Memory Engrams   Constructed from one or more Citable Engram or other Long-Term Memory Engrams.</p> </li> <li> <p>Learning   Built through the combination of memory, citable external sources, and user interaction or input.</p> </li> <li> <p>Unified Memory   All engrams are stored within a unified system, enabling both full and selective access to memory content.</p> </li> </ul>"},{"location":"getting_started/","title":"Getting Started with Engramic","text":"<p>Please contact us at info@engramic.org if you have any issues with these instructions.</p> <p>Note: At this time, we are only testing on Linux (Ubuntu).</p> 1. Pre-Requisites - OS &amp; IDE <p>We are currently developing Engramic on WSL on Windows using Visual Studio Code With Ubuntu 24.0.1 LTS. It may run on other configurations\u2014we'll begin cross-platform testing soon. If you'd like to help us, please reach out at info@engramic.org.</p> <p>Engramic is availible via pip, however, working from source is recommended for this release.</p> <pre><code>pip install engramic\n</code></pre> <p>To set up your dev environment, you will need the following:</p> <ul> <li>Python 3.10+</li> <li>Visual Studio Code</li> <li>Git</li> <li>Pipx</li> <li>Hatch</li> <li>MS VS Code WSL Extension</li> <li>MS Python Debugger</li> <li>Google Gemini API key (optional)</li> </ul> <p>We will direct you to installation instructions in the steps below.</p> 2. Clone or Fork From GitHub <p>In Linux, clone or fork the latest version from the <code>main</code> branch of the Engramic GitHub repository:</p> <p>\ud83d\udcce https://github.com/engramic/engramic</p> 3. Install Hatch <p>We use Hatch as our Python project manager. It handles all dependencies, Python versioning, testing, versioning, scripts, and virtual environments.</p> <p>Visit this page to install Hatch on your Linux instance:</p> <p>\ud83d\udd17 Hatch Installation</p> <p>We recommend installing with <code>pipx</code> as described in the Hatch installation instructions. Restart your terminal after running pipx ensurepath.</p> 4. Initialize the Environment <p>Now that Hatch is installed:</p> <ol> <li>Navigate to the root of the Engramic project in your terminal.</li> <li> <p>Run:</p> <pre><code>hatch env create\n</code></pre> <p>Enter into the default shell (\"default\" has no name after \"shell\".) <pre><code>hatch shell\n</code></pre></p> </li> </ol> <p>This will install all dependencies (should be quick\u2014we work hard to minimize dependencies). Watch a video of Hatch in Engramic.</p> <ol> <li> <p>Open Visual Studio Code and install the WSL extension.</p> </li> <li> <p>Launch VS Code from the WSL terminal:</p> <pre><code>code .\n</code></pre> </li> <li> <p>In VS Code, install the Python Debugger extension from Microsoft.</p> </li> </ol> 5. Configure the Python Interpreter <p>In Visual Studio Code:</p> <ol> <li>Press <code>Ctrl + Shift + P</code></li> <li>Search for and select \"Python: Select Interpreter\"</li> <li>Choose the environment that looks like: <code>Python X.XX.X ('engramic')</code></li> </ol> <p>Note: If you aren't sure of the path, you can type the following while in the hatch shell:</p> <pre><code>python -c \"import sys;print(sys.executable)\"\n</code></pre> <p>If you are stuck, make sure your top, middle search bar in VS Code reads: engramic [WSL: Ubuntu-24.04] (or your distro). If not, your issue is probably related to the WSL extension.</p> 6. Run the Mock Example 7. Run the Standard Example 8. Create your First Memory."},{"location":"getting_started/#running-the-code","title":"Running The Code","text":"<p>The code is available at: <pre><code>    engramic/examples/mock_profile/mock_profile.py\n</code></pre></p> <ol> <li>Open the Run and Debug sidebar in VS Code.</li> <li>Choose \"Example - Mock\" and run it.</li> </ol> <p>Congrats! \ud83c\udf89 You've just run the mock version of the system using mock plugins. You should see an output message in terminal window.</p>"},{"location":"getting_started/#looking-at-the-code","title":"Looking At The Code","text":"<p>The mock version doesn't actually use AI calls, just emulated API calls that return static responses via the Mock plugins. In this example, you can see how to create a Host, MessageService, RetrieveService, and a ResponseService. Also, we have created a service called TestService whose only job is to recieve the Response call from the subscirbed callback on_main_prompt_complete.</p>"},{"location":"getting_started/#running-the-code_1","title":"Running The Code","text":"<p>Now, let's run an example with actual AI. This example uses Google Gemini.</p> <pre><code>    engramic/examples/standard_profile/standard_profile.py\n</code></pre> <ol> <li> <p>For this example, you'll need a Gemini API key:</p> <ul> <li>Create a Google Cloud account if you don't already have one.</li> <li>Follow Google's documentation to create a Generative Language API key.</li> </ul> </li> <li> <p>Add a <code>.env</code> file to the root of the project with the following content (multiple plugin paths coming soon.):</p> <pre><code>GEMINI_API_KEY=PUT_YOUR_KEY_HERE_WITH_NO_QUOTES_OR_ANYTHING_ELSE\nLOCAL_STORAGE_ROOT_PATH=./local_storage\n</code></pre> <p>Locate this line and change it if you would like to.</p> <pre><code>retrieve_service.submit(Prompt('Briefly tell me about Chamath Palihapitiya.'))\n</code></pre> </li> <li> <p>In Run and Debug, select \"Example - Standard\".</p> <p>Note: this takes some time on first run. Be patient.</p> </li> </ol>"},{"location":"getting_started/#looking-at-the-code_1","title":"Looking At The Code","text":"<p>Run the program. The plugins will automatically download all dependencies on the first run and check for updates on subsequent runs. Configuration for plugins are defined by profiles, in this case, the profile is named \"standard\". Each plugin contains it's dependencies in a plugin.toml file.</p> <p>Hit Run and you'll see the result in the terminal window. This example adds Storage, Codify, and Consolidate service, but doesn't actually use them.</p> <p>Let's try those services in the next demo.</p>"},{"location":"getting_started/#running-the-code_2","title":"Running The Code","text":"<p>Let's generate our first memory.</p> <pre><code>    engramic/examples/create_memory/create_memory.py\n</code></pre> <ol> <li>Run and Debug the profile named \"Example - Create Memory\".</li> </ol> <p>You should see three outputs, Response, Meta Summary, and Engrams.</p>"},{"location":"getting_started/#looking-at-the-code_2","title":"Looking at The Code","text":"<p>This time, we've added another call to our TestService. Services support sync and async threads. Some features that services perform run on the main thread, such as subscribing, while others such as sending messages or running tasks must run on the async thread. The Codify service is listening for MAIN_PROMPT_COMPLETE and OBSERVATION_COMPLETE calls.</p> <pre><code>    class TestService(Service):\n        def start(self):\n            self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n            self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n</code></pre> <p>Let's look at the Observation, the output of the Codify service. Two types of data structures are output on the screen, the first is a set of Engrams, these are the memories extracted from the response of the training. The next is the Meta Summary. Meta data are summary information about all Engrams that were generated. This data structure is created to help the retrieval stage with awareness of it's memory set.</p> <p>To delete all memories, enter into the hatch shell named \"dev\".</p> <pre><code>cd /engramic\nhatch shell dev\nhatch run delete_dbs\n</code></pre> <p>Congratulations. You have finished the getting started page. If you would like to continue learning, we suggestion you follow the How To's section of the documentation.</p>"},{"location":"host_and_services/","title":"Host &amp; Services","text":"<p>The Engramic Inference Engine consists of several services, each designed to facilitate distinct aspects of knowledge processing.</p>"},{"location":"host_and_services/#host","title":"Host","text":"<p>The Host contains all of the executing services on the instance/machine. You may add all services or just one. The host manages initialization and the set of resources and capabilities used by all other systems. The Host is like a very lightweight abstraction layer for all services.</p> <pre><code>flowchart TD\n    %% Define node styles\n    classDef process fill:#f9f9f9,stroke:#333,stroke-width:1px,rounded:true\n    classDef io fill:#e8f4ff,stroke:#4a86e8,stroke-width:1px,rounded:true\n    classDef external fill:#f0fff0,stroke:#2d862d,stroke-width:1px,rounded:true\n\n    %% Input and external processes\n    prompt([User Prompt]):::io\n    stream([User Stream]):::io\n    sense[Sense]:::external\n\n    respond --&gt; stream\n\n    %% Core processes in learning loop\n    subgraph \"Engramic Learning Loop\"\n      direction RL\n      consolidate[Consolidate]:::process\n      retrieve[Retrieve]:::process\n      respond[Respond]:::process\n      codify[Codify]:::process\n\n      consolidate --&gt; retrieve\n      retrieve --&gt; respond\n      respond --&gt; codify\n      codify --&gt; consolidate\n    end\n\n    %% External connections\n    prompt --&gt; retrieve\n    sense --&gt; consolidate\n</code></pre>"},{"location":"host_and_services/#services","title":"Services","text":"<ul> <li>Retrieve: Analyzes the prompt, manages short-term memory, and performs retrieval of all engrams.</li> <li>Respond: Constructs the response to the user.</li> <li>Codify: While in training mode, assesses the validity of responses, integrating them as long-term memories.</li> <li>Consolidate: Transforms data into observations, contained knowledge units rich in context.</li> <li>Sense: Converts raw data into observations.</li> <li>Teach: (Not in diagram.)Encourages the emergence of new engrams by stimulating connections between existing ones.</li> <li>Repo: Loads document repositories and defines repo ids. One or more repo ids can bet set when prompting.</li> </ul>"},{"location":"host_and_services/#centralized-services","title":"Centralized Services","text":"<ul> <li>Store: Centralized storage for long-term, context-aware memory.</li> <li>Message: Centeralized message passing between all services.</li> <li>Process: Centralized progress tracking from input (e.g. prompt or document) to inserting into Retrieve.</li> </ul>"},{"location":"profiles/","title":"Engramic Plugin and Profile System","text":"<p>Engramic is designed with a plugin layer that allows seamless integration with various components such as LLMs, databases, vector databases, and embedding services. The glue that binds these systems together are called profiles.</p>"},{"location":"profiles/#example-default-profile-configuration","title":"Example: Default Profile Configuration","text":"<pre><code>version = 0.1\n\n[mock]\ntype = \"profile\"\nvector_db.db = {name=\"Mock\"}\nllm.retrieve_gen_conversation_direction = {name=\"Mock\"}\nllm.retrieve_gen_index = {name=\"Mock\"}\nllm.retrieve_prompt_analysis = {name=\"Mock\"}\ndb.document = {name=\"Mock\"}\nllm.response_main = {name=\"Mock\"}\nllm.validate = {name=\"Mock\"}\nllm.summary = {name=\"Mock\"}\nllm.gen_indices = {name=\"Mock\"}\nembedding.gen_embed = {name=\"Mock\"}\n\n[standard]\ntype = \"pointer\"\nptr = \"standard-2025-04-01\"\n\n[standard-2025-04-01]\ntype = \"profile\"\nvector_db.db = {name=\"ChromaDB\"}\nllm.retrieve_gen_conversation_direction = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.retrieve_gen_index = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.retrieve_prompt_analysis = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\ndb.document = {name=\"Sqlite\"}\nllm.response_main = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.validate = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.summary = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.gen_indices = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nembedding.gen_embed = {name=\"Gemini\",model=\"text-embedding-004\"}\nllm.sense_initial_summary = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.sense_scan = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.sense_full_summary = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\nllm.teach_generate_questions = {name=\"Gemini\",model=\"gemini-2.5-flash-preview-04-17\"}\n</code></pre>"},{"location":"profiles/#loading-a-profile-with-the-host","title":"Loading a Profile with the Host","text":"<p>When creating a <code>Host</code>, you should load the initial profile like so:</p> <pre><code>host = Host(\n    'standard', #&lt;--this defines the profile.\n    [\n        MessageService,\n        RetrieveService,\n        ResponseService,\n        StorageService,\n        CodifyService,\n        ConsolidateService\n    ]\n)\n</code></pre> <p>In this example, the developer is using the <code>standard</code> profile. This is a pointer profile that redirects to a dated version (e.g., <code>standard-2025-04-01</code>). As Engramic evolves, the <code>standard</code> pointer will be updated to reference the currently recommended profile. By using <code>standard</code>, you're always aligned with the recommended configuration.</p> <p>The <code>mock</code> profile is another key option. When using <code>mock</code>, all plugins are mock implementations \u2014 meaning no API calls are made. This mode is intended exclusively for testing and development.</p>"},{"location":"howto/how_to_load_a_repo/","title":"Load And Query a Repo","text":"<pre><code>flowchart LR\n    sense --&gt; repo\n    repo --&gt; sense\n\n    classDef green fill:#b2f2bb;\n    class sense green\n</code></pre> <p>A repo is collection of files located at a root folder. The RepoService provides the following abilities:</p> <ul> <li>Returns the set of all root folders.</li> <li>Returns all files within each root folder.</li> <li>Enables prompting filtering by repo.</li> </ul>"},{"location":"howto/how_to_load_a_repo/#example-code-walkthrough","title":"Example Code Walkthrough","text":"<p>The full code is available in the source code at <code>/engramic/examples/repo/repo.py</code>.  You can download the files for this exercise at https://www.engramic.org/assets-page</p> <p>Let's walk through how this example works step-by-step:</p>"},{"location":"howto/how_to_load_a_repo/#1-setting-up-the-environment","title":"1. Setting Up the Environment","text":"<p>The example code creates a <code>TestService</code> class that demonstrates how to:</p> <ul> <li>Scan and discover repository folders</li> <li>Submit documents for processing</li> <li>Listen for repository and document events</li> <li>Query the system with repository filtering</li> </ul>"},{"location":"howto/how_to_load_a_repo/#2-initializing-required-services","title":"2. Initializing Required Services","text":"<pre><code>def main() -&gt; None:\n    host = Host(\n        'standard',\n        [\n            MessageService,\n            SenseService,\n            RetrieveService,\n            ResponseService,\n            StorageService,\n            ConsolidateService,\n            CodifyService,\n            RepoService,  #&lt;-- This is the key service for repository functionality\n            ProgressService,\n            TestService,\n        ],\n    )\n</code></pre> <p>This code sets up all necessary services, with <code>RepoService</code> being essential for the repository functionality.</p>"},{"location":"howto/how_to_load_a_repo/#3-repository-discovery-process","title":"3. Repository Discovery Process","text":"<pre><code># In TestService.start():\ndef start(self):\n    super().start()\n    self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n    self.subscribe(Service.Topic.REPO_FOLDERS, self._on_repo_folders)\n    self.subscribe(Service.Topic.REPO_FILES, self._on_repo_files)\n    self.subscribe(Service.Topic.DOCUMENT_INSERTED, self.on_document_inserted)\n    repo_service = self.host.get_service(RepoService)\n    repo_service.scan_folders()\n    self.run_task(self.submit_documents())\n</code></pre> <p>This code:</p> <ol> <li>Subscribes to repository-related events</li> <li>Gets a reference to the RepoService</li> <li>Initiates a scan of repository folders</li> <li>Launches a task to submit documents for processing</li> </ol>"},{"location":"howto/how_to_load_a_repo/#4-repository-event-handling","title":"4. Repository Event Handling","text":"<p>The TestService subscribes to key events related to repositories:</p> <ul> <li><code>REPO_FOLDERS</code>: Triggered when repository folders are discovered</li> <li><code>REPO_FILES</code>: Triggered after folders are discovered, providing file information</li> <li><code>DOCUMENT_INSERTED</code>: Triggered when a document has been processed</li> <li><code>MAIN_PROMPT_COMPLETE</code>: Triggered when a response to a prompt is ready</li> </ul>"},{"location":"howto/how_to_load_a_repo/#5-processing-repository-folders","title":"5. Processing Repository Folders","text":"<pre><code>def _on_repo_folders(self, message_in: dict[str, Any]) -&gt; None:\n    if message_in['repo_folders'] is not None:\n        self.repos = message_in['repo_folders']\n        self.repo_id1 = next((key for key, value in self.repos.items() if value == 'QuantumNetworking'), None)\n        self.repo_id2 = next((key for key, value in self.repos.items() if value == 'ElysianFields'), None)\n    else:\n        logging.info('No repos found. You can add a repo by adding a folder to home/.local/share/engramic')\n</code></pre> <p>This code:</p> <ol> <li>Stores the discovered repository folders</li> <li>Looks up specific repository IDs by their folder names</li> <li>Provides feedback if no repositories are found</li> </ol>"},{"location":"howto/how_to_load_a_repo/#6-document-submission-process","title":"6. Document Submission Process","text":"<pre><code>async def submit_documents(self) -&gt; None:\n    repo_service = self.host.get_service(RepoService)\n    self.document_id1 = '97a1ae1b8461076cdc679d6e0a5f885e'  # 'IntroductiontoQuantumNetworking.pdf'\n    self.document_id2 = '9c9f0237620b77fa69e2ca63e40a9f27'  # 'Elysian_Fields.pdf'\n    repo_service.submit_ids([self.document_id1], overwrite=True)\n    repo_service.submit_ids([self.document_id2])\n</code></pre> <p>This code:</p> <ol> <li>Defines document IDs to be processed</li> <li>Submits the first document with overwrite=True to force reprocessing</li> <li>Submits the second document without overwrite, using cached version if available</li> </ol>"},{"location":"howto/how_to_load_a_repo/#7-querying-with-repository-filters","title":"7. Querying with Repository Filters","text":"<p>When documents are fully processed (<code>DOCUMENT_INSERTED</code> events), the code sends queries with different repository filters:</p> <pre><code>def on_document_inserted(self, message_in: dict[str, Any]) -&gt; None:\n    document_id = message_in['id']\n    if document_id in {self.document_id1, self.document_id2}:\n        self.count += 1\n        if self.count == TestService.DOCUMENT_COUNT:\n            retrieve_service = self.host.get_service(RetrieveService)\n            prompt1 = Prompt(\n                'This is prompt 1. Briefly tell me about IntroductiontoQuantumNetworking.pdf and Elysian_Fields.pdf. Start with prompt number.',\n                repo_ids_filters=[self.repo_id1, self.repo_id2],\n            )\n            retrieve_service.submit(prompt1)\n            prompt2 = Prompt(\n                'This is prompt 2. Briefly tell me about IntroductiontoQuantumNetworking.pdf and Elysian_Fields.pdf. Start with prompt number.',\n                repo_ids_filters=[self.repo_id1],\n            )\n            retrieve_service.submit(prompt2)\n            prompt3 = Prompt(\n                'This is prompt 3. Briefly tell me about IntroductiontoQuantumNetworking.pdf and Elysian_Fields.pdf.  Start with prompt number.',\n                repo_ids_filters=None,\n            )\n            retrieve_service.submit(prompt3)\n</code></pre> <p>This code:</p> <ol> <li>Tracks when all documents are processed</li> <li>Creates three different prompts with varying repository filters:</li> <li>Prompt 1: Filters by both repositories</li> <li>Prompt 2: Filters by only the first repository</li> <li>Prompt 3: Uses no repository filtering (null repo)</li> <li>Submits the prompts to the RetrieveService</li> </ol>"},{"location":"howto/how_to_load_a_repo/#repository-filtering-options","title":"Repository Filtering Options","text":""},{"location":"howto/how_to_load_a_repo/#access-multiple-repositories","title":"Access Multiple Repositories","text":"<p>This prompt will access files from both repo_id1 and repo_id2.</p> <pre><code>prompt1 = Prompt(\n    'This is prompt 1. Tell me about document content.',\n    repo_ids_filters=[self.repo_id1, self.repo_id2],\n)\n</code></pre>"},{"location":"howto/how_to_load_a_repo/#filter-by-a-single-repository","title":"Filter by a Single Repository","text":"<p>This prompt will only access files from repo_id1.</p> <pre><code>prompt2 = Prompt(\n    'This is prompt 2. Tell me about document content.',\n    repo_ids_filters=[self.repo_id1],\n)\n</code></pre>"},{"location":"howto/how_to_load_a_repo/#the-null-repository","title":"The 'Null' Repository","text":"<p>Documents are usable without using the repo system. In this case, the file is associated with a default repo known as the 'null' repo.</p> <pre><code>prompt3 = Prompt(\n    'This is prompt 3. Tell me about document content.',\n    repo_ids_filters=None,\n)\n</code></pre> <p>Note: If you have previously run the document example, you may have IntroductiontoQuantumNetworking.pdf in the 'null' repo. To remove it you can delete the database (you can simply delete the sql lite file or run the following hatch command).</p> <pre><code>hatch shell dev\n</code></pre> <pre><code>hatch run delete_dbs\n</code></pre>"},{"location":"howto/how_to_load_a_repo/#empty-list-invalid","title":"Empty List (invalid)","text":"<p>\u274c  Invalid - Empty list repo filters are not allowed and will throw an error.</p> <pre><code># The following would throw an exception\n# Prompt('This is prompt 4. Tell me about document content.', repo_ids_filters=[])\n</code></pre>"},{"location":"howto/how_to_load_a_repo/#important-considerations","title":"Important Considerations","text":"<ul> <li>In order to run the code, you must add the REPO_ROOT environment variable. For example, you can set repo root to a path like the following: <code>REPO_ROOT = \"~/.local/share/engramic/\"</code>.</li> <li>For security purposes, there is no ability to access all repos without explicitly providing each repo id as a filter.</li> <li>If you load a document under a repo, it is forever in that repo. For example, if you first scan a document under the 'null' repo and then scan that document as part of a repo it will be detected and not scanned. Because of this, when you query the repo, it will not display results as part of the repo because it is part of the 'null' repo.</li> </ul>"},{"location":"howto/how_to_parse_documents/","title":"Documents","text":"<p>note: Engramic currently only supports PDFs</p> <pre><code>flowchart TD\n    %% Define node styles\n    classDef process fill:#f9f9f9,stroke:#333,stroke-width:1px,rounded:true\n    classDef io fill:#e8f4ff,stroke:#4a86e8,stroke-width:1px,rounded:true\n    classDef external fill:#f0fff0,stroke:#2d862d,stroke-width:1px,rounded:true\n\n    %% Input and external processes\n    prompt([User Prompt]):::io\n    stream([User Stream]):::io\n    sense[Sense]:::external\n\n    respond --&gt; stream\n\n    %% Core processes in learning loop\n    subgraph \"Engramic Learning Loop\"\n      direction RL\n      consolidate[Consolidate]:::process\n      retrieve[Retrieve]:::process\n      respond[Respond]:::process\n      codify[Codify]:::process\n\n      consolidate --&gt; retrieve\n      retrieve --&gt; respond\n      respond --&gt; codify\n      codify --&gt; consolidate\n    end\n\n    %% External connections\n    prompt --&gt; retrieve\n    sense --&gt; consolidate\n\n</code></pre> <p>PDF parsing is part of the sense service. When a document is parsed, it is sent to the consolidate service where it is processed and passed to retrieval for storing in a vector database and to response if it is matched semantically.</p>"},{"location":"howto/how_to_parse_documents/#example-code-walkthrough","title":"Example Code Walkthrough","text":"<p>The full code is available in the source code at <code>/engramic/examples/document/document.py</code>.  You can download the files for this exercise at https://www.engramic.org/assets-page</p> <p>Let's walk through how this example works step-by-step:</p>"},{"location":"howto/how_to_parse_documents/#1-setting-up-the-environment","title":"1. Setting Up the Environment","text":"<p>The example code creates a <code>TestService</code> class that demonstrates how to:</p> <ul> <li>Submit a document for processing</li> <li>Listen for document processing completion</li> <li>Query the system about the processed document</li> </ul>"},{"location":"howto/how_to_parse_documents/#2-document-submission-process","title":"2. Document Submission Process","text":"<pre><code># In TestService.start():\nsense_service = self.host.get_service(SenseService)\ndocument = Document(\n    Document.Root.RESOURCE.value, 'engramic.resources.rag_document', 'IntroductiontoQuantumNetworking.pdf'\n)\nself.document_id = document.id\nsense_service.submit_document(document)\n</code></pre> <p>This code:</p> <ol> <li>Gets a reference to the SenseService</li> <li>Creates a Document object using a PDF from the resources directory</li> <li>Saves the document ID for later reference</li> <li>Submits the document to the SenseService for processing</li> </ol>"},{"location":"howto/how_to_parse_documents/#3-document-processing-flow","title":"3. Document Processing Flow","text":"<p>When a document is submitted, the following happens:</p>"},{"location":"howto/how_to_parse_documents/#sense-service","title":"Sense Service","text":"<ul> <li>Convert PDF page to PNGs</li> <li>Extract meta data from first few pages</li> <li>Convert from image into annotated text</li> <li>Summarize annotated text for Meta object</li> <li>Parse from annotated text into Engrams</li> <li>Package into an observation (Meta + Engrams)</li> </ul>"},{"location":"howto/how_to_parse_documents/#event-handling","title":"Event Handling","text":"<p>The TestService subscribes to two key events: <pre><code>self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\nself.subscribe(Service.Topic.DOCUMENT_INSERTED, self.on_document_inserted)\n</code></pre></p> <ul> <li><code>DOCUMENT_INSERTED</code>: Triggered when document processing is complete</li> <li><code>MAIN_PROMPT_COMPLETE</code>: Triggered when a response to a prompt is ready</li> </ul>"},{"location":"howto/how_to_parse_documents/#4-querying-the-document","title":"4. Querying the Document","text":"<p>When the document is fully processed (<code>DOCUMENT_INSERTED</code> event), the code automatically sends a query:</p> <pre><code>def on_document_inserted(self, message_in: dict[str, Any]) -&gt; None:\n    document_id = message_in['id']\n    if self.document_id == document_id:\n        retrieve_service = self.host.get_service(RetrieveService)\n        prompt = Prompt('Do you have any files about quantum networking? What is it about?')\n        retrieve_service.submit(prompt)\n</code></pre> <p>This:</p> <ol> <li>Checks if the completed document is the one we submitted</li> <li>Gets a reference to the RetrieveService</li> <li>Creates a prompt asking about quantum networking</li> <li>Submits the prompt to the RetrieveService</li> </ol>"},{"location":"howto/how_to_parse_documents/#5-handling-the-response","title":"5. Handling the Response","text":"<p>When the response is ready (<code>MAIN_PROMPT_COMPLETE</code> event), the code logs it:</p> <pre><code>def on_main_prompt_complete(self, message_in: dict[str, Any]) -&gt; None:\n    response = Response(**message_in)\n    logging.info('\\n\\n================[Response]==============\\n%s\\n\\n', response.response)\n</code></pre>"},{"location":"howto/how_to_parse_documents/#document-submission-options","title":"Document Submission Options","text":"<p>To submit a document for processing, you can use the submit_document method from the SenseService (as shown in the example) or via the Document.Topic.SUBMIT_DOCUMENT message.</p> <p>When submitting documents that may have been processed before, you can use the overwrite parameter to force reprocessing:</p> <pre><code># Submit multiple documents with overwrite option\nrepo_service.submit_ids([document_id1], overwrite=True)\n\n# Submit without overwrite (uses cached version if available)\nrepo_service.submit_ids([document_id2])\n</code></pre>"},{"location":"howto/how_to_parse_documents/#loading-from-data-directory","title":"Loading From Data Directory","text":"<p>In the example above, the code is referencing a file saved in the resources directory, which is packaged with the distribution (src/engramic/resources). If you would like to load a file that isn't a resource, you can pass Document.Root.DATA.value to the first parameter of Document which will set a base directory to the REPO_ROOT environment variable.</p> <pre><code># Loading from local data directory\ndocument = Document(\n    Document.Root.DATA.value,\n    '/path/to/document/folder',\n    'document.pdf'\n)\n</code></pre> <p>Example of setting REPO_ROOT environment variable. <pre><code>REPO_ROOT = \"~/.local/share/engramic/\"\n</code></pre></p>"},{"location":"howto/how_to_run_a_lesson/","title":"Run A Lesson","text":"<pre><code>flowchart TD\n    %% Define node styles\n    classDef process fill:#f9f9f9,stroke:#333,stroke-width:1px,rounded:true\n    classDef io fill:#e8f4ff,stroke:#4a86e8,stroke-width:1px,rounded:true\n    classDef external fill:#f0fff0,stroke:#2d862d,stroke-width:1px,rounded:true\n\n    %% Input and external processes\n    prompt([User Prompt]):::io\n    stream([User Stream]):::io\n    sense[Sense]:::external\n    teach[Teach]:::external\n\n    respond --&gt; stream\n\n    %% Core processes in learning loop\n    subgraph \"Engramic Learning Loop\"\n      direction RL\n      consolidate[Consolidate]:::process\n      retrieve[Retrieve]:::process\n      respond[Respond]:::process\n      codify[Codify]:::process\n\n      consolidate --&gt; retrieve\n      retrieve --&gt; respond\n      respond --&gt; codify\n      codify --&gt; consolidate\n    end\n\n    %% External connections\n    prompt --&gt; retrieve\n    sense --&gt; consolidate\n    sense --&gt; teach\n    teach --&gt; prompt\n</code></pre>"},{"location":"howto/how_to_run_a_lesson/#example-code-walkthrough","title":"Example Code Walkthrough","text":"<p>The full code is available in the source code at <code>/engramic/examples/teach/teach.py</code>.  You can download the files for this exercise at https://www.engramic.org/assets-page</p> <p>Let's walk through how this example works step-by-step:</p>"},{"location":"howto/how_to_run_a_lesson/#1-setting-up-the-environment","title":"1. Setting Up the Environment","text":"<p>The example code creates a <code>TestService</code> class that demonstrates how to: - Submit a document for processing - Listen for lesson events - Process lesson results - Query the system after the lesson completes</p>"},{"location":"howto/how_to_run_a_lesson/#2-initializing-required-services","title":"2. Initializing Required Services","text":"<pre><code>def main() -&gt; None:\n    host = Host(\n        'standard',\n        [\n            MessageService,\n            SenseService,\n            RetrieveService,\n            ResponseService,\n            StorageService,\n            ConsolidateService,\n            CodifyService,\n            TeachService,  #&lt;-- This is the key service for lessons\n            RepoService,\n            ProgressService,\n            TestService,\n        ],\n    )\n</code></pre> <p>This code sets up all necessary services, with <code>TeachService</code> being essential for the lesson functionality.</p>"},{"location":"howto/how_to_run_a_lesson/#3-document-submission-process","title":"3. Document Submission Process","text":"<pre><code># In TestService.start():\nsense_service = self.host.get_service(SenseService)\ndocument = Document(\n    Document.Root.RESOURCE.value, 'engramic.resources.rag_document', 'IntroductiontoQuantumNetworking.pdf'\n)\nself.document_id = document.id\nsense_service.submit_document(document, overwrite=True)\n</code></pre> <p>This code:</p> <ol> <li>Gets a reference to the SenseService</li> <li>Creates a Document object using a PDF from the resources directory</li> <li>Saves the document ID for later reference</li> <li>Submits the document to the SenseService for processing with overwrite=True to ensure fresh processing</li> </ol>"},{"location":"howto/how_to_run_a_lesson/#4-lesson-event-handling","title":"4. Lesson Event Handling","text":"<p>The TestService subscribes to key events related to lessons:</p> <pre><code>self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\nself.subscribe(Service.Topic.LESSON_CREATED, self.on_lesson_created)\nself.subscribe(Service.Topic.LESSON_INSERTED, self.on_lesson_inserted)\n</code></pre> <ul> <li><code>LESSON_CREATED</code>: Triggered when a new lesson is created</li> <li><code>LESSON_INSERTED</code>: Triggered when a lesson is fully processed and ready for retrieval</li> <li><code>MAIN_PROMPT_COMPLETE</code>: Triggered when a response to a prompt is completed.</li> </ul>"},{"location":"howto/how_to_run_a_lesson/#5-tracking-lesson-progress","title":"5. Tracking Lesson Progress","text":"<p>The service keeps track of lesson IDs and handles lesson-related events:</p> <pre><code>def on_lesson_created(self, message_in: dict[str, Any]) -&gt; None:\n    self.lesson_id = message_in['id']\n\ndef on_main_prompt_complete(self, message_in: dict[str, Any]) -&gt; None:\n    response = Response(**message_in)\n    if not response.prompt['is_lesson']:\n        logging.info('\\n\\n================[Response]==============\\n%s\\n\\n', response.response)\n    else:\n        logging.info('Lesson Response. %s', response.prompt['prompt_str'])\n</code></pre> <p>This code: 1. Stores the lesson ID when created 2. Distinguishes between regular prompt responses and lesson-related responses</p>"},{"location":"howto/how_to_run_a_lesson/#6-querying-after-lesson-completion","title":"6. Querying After Lesson Completion","text":"<p>When the lesson is fully processed (<code>LESSON_INSERTED</code> event), the code automatically sends a query:</p> <pre><code>def on_lesson_inserted(self, message_in: dict[str, Any]) -&gt; None:\n    lesson_id = message_in['id']\n    if self.lesson_id == lesson_id:\n        retrieve_service = self.host.get_service(RetrieveService)\n        retrieve_service.submit(Prompt('Please tell me about the file IntroductiontoQuantumNetworking.pdf'))\n</code></pre> <p>This: 1. Checks if the completed lesson is the one we're tracking 2. Gets a reference to the RetrieveService 3. Creates a prompt asking about the document 4. Submits the prompt to the RetrieveService to test knowledge gained from the lesson</p>"},{"location":"howto/how_to_run_a_lesson/#how-it-works","title":"How It Works","text":"<p>The TeachService provides an automated way to enhance the system's understanding of documents by running a \"lesson\" that explores content more deeply. Here's how it works:</p> <ul> <li>Document Submission: When a document is submitted to the system, the SenseService processes it and inserts it into the vector database.</li> <li>Meta Creation: During processing, the system creates a meta object that describes the structure of the document.</li> <li>Lesson Initiation: Once the document is inserted, TeachService automatically creates and runs a lesson based on the document's meta information.</li> <li>Question Generation: The lesson uses an LLM to generate relevant questions about the document content.</li> <li>Background Learning: These questions are submitted as prompts that run in the background with training_mode=True and is_lesson=True flags.</li> <li>Knowledge Consolidation: The responses to these prompts are converted into long-term memories, creating a deeper understanding of the document.</li> <li>Enhanced Q&amp;A: Once the lesson completes, the system provides more robust answers to user questions about the document's content.</li> </ul>"},{"location":"howto/how_to_run_a_lesson/#adding-lesson-functionality-to-your-application","title":"Adding Lesson Functionality to Your Application","text":"<p>To add lesson functionality to your Engramic application:</p> <ol> <li> <p>Include the TeachService in your host initialization:</p> <pre><code>host = Host(\n    'standard',\n    [\n        # Other services...\n        TeachService,\n        # Other services...\n    ],\n)\n</code></pre> </li> <li> <p>Submit a document to trigger the lesson process:</p> <pre><code>sense_service = host.get_service(SenseService)\ndocument = Document(\n    Document.Root.RESOURCE.value, \n    'engramic.resources.your_document_path', \n    'YourDocument.pdf'\n)\n\nsense_service.submit_document(document, True)\n</code></pre> </li> <li> <p>Listen for lesson events if needed:</p> <pre><code># Subscribe to lesson events\nself.subscribe(Service.Topic.LESSON_CREATED, self.on_lesson_created)\nself.subscribe(Service.Topic.LESSON_INSERTED, self.on_lesson_inserted)\n</code></pre> </li> </ol>"},{"location":"howto/howto/","title":"Introduction","text":"<p>In this section, we expand on the getting started guide.</p> <p>Load a Document - Load a PDF and store it as long-term memories.</p> <p>Run Lesson - After loading a document, have Engramic perform a study pass, improving Engramic's understanding of a document.</p> <p>Load Repos - Load a directory of PDFs, select a set of repos to prompt against.</p>"},{"location":"reference/ask/","title":"Ask","text":"<p>               Bases: <code>Retrieval</code></p> <p>Handles Q&amp;A retrieval by transforming prompts into contextual embeddings and returning relevant engram IDs.</p> <p>This class implements the retrieval workflow from raw prompts to vector database queries, supporting conversation analysis and dynamic index generation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for this retrieval session.</p> <code>prompt</code> <code>Prompt</code> <p>The original prompt provided by the user.</p> <code>plugin_manager</code> <code>PluginManager</code> <p>Access to LLM, vector DB, and embedding components.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks operational metrics for observability.</p> <code>db_plugin</code> <code>dict</code> <p>Plugin for document database interactions.</p> <code>service</code> <code>RetrieveService</code> <p>Parent service coordinating this request.</p> <code>library</code> <code>str | None</code> <p>Optional target library to search within.</p> <code>conversation_direction</code> <code>dict[str, str]</code> <p>Stores user intent and working memory.</p> <code>prompt_analysis</code> <code>PromptAnalysis | None</code> <p>Structured analysis of the prompt.</p> <p>Methods:</p> Name Description <code>get_sources</code> <p>Initiates the async pipeline for directional memory retrieval.</p> <code>_fetch_history</code> <p>Retrieves prior user history from the document DB.</p> <code>_retrieve_gen_conversation_direction</code> <p>Extracts user intent and conversational memory.</p> <code>_embed_gen_direction</code> <p>Converts extracted intent into embeddings.</p> <code>_vector_fetch_direction_meta</code> <p>Queries metadata collection using intent embeddings.</p> <code>_fetch_direction_meta</code> <p>Loads Meta objects from metadata store.</p> <code>_analyze_prompt</code> <p>Analyzes user prompt in context of metadata.</p> <code>_generate_indices</code> <p>Generates semantic indices for retrieval.</p> <code>_generate_indicies_embeddings</code> <p>Converts index phrases into embeddings.</p> <code>_query_index_db</code> <p>Searches vector DB to identify related engrams.</p> Source code in <code>src/engramic/application/retrieve/ask.py</code> <pre><code>class Ask(Retrieval):\n    \"\"\"\n    Handles Q&amp;A retrieval by transforming prompts into contextual embeddings and returning relevant engram IDs.\n\n    This class implements the retrieval workflow from raw prompts to vector database queries,\n    supporting conversation analysis and dynamic index generation.\n\n    Attributes:\n        id (str): Unique identifier for this retrieval session.\n        prompt (Prompt): The original prompt provided by the user.\n        plugin_manager (PluginManager): Access to LLM, vector DB, and embedding components.\n        metrics_tracker (MetricsTracker): Tracks operational metrics for observability.\n        db_plugin (dict): Plugin for document database interactions.\n        service (RetrieveService): Parent service coordinating this request.\n        library (str | None): Optional target library to search within.\n        conversation_direction (dict[str, str]): Stores user intent and working memory.\n        prompt_analysis (PromptAnalysis | None): Structured analysis of the prompt.\n\n    Methods:\n        get_sources() -&gt; None:\n            Initiates the async pipeline for directional memory retrieval.\n        _fetch_history() -&gt; list[dict[str, Any]]:\n            Retrieves prior user history from the document DB.\n        _retrieve_gen_conversation_direction(response_array) -&gt; dict[str, str]:\n            Extracts user intent and conversational memory.\n        _embed_gen_direction(main_prompt) -&gt; list[float]:\n            Converts extracted intent into embeddings.\n        _vector_fetch_direction_meta(embedding) -&gt; list[str]:\n            Queries metadata collection using intent embeddings.\n        _fetch_direction_meta(meta_id) -&gt; list[Meta]:\n            Loads Meta objects from metadata store.\n        _analyze_prompt(meta_list) -&gt; dict[str, str]:\n            Analyzes user prompt in context of metadata.\n        _generate_indices(meta_list) -&gt; dict[str, str]:\n            Generates semantic indices for retrieval.\n        _generate_indicies_embeddings(indices) -&gt; list[list[float]]:\n            Converts index phrases into embeddings.\n        _query_index_db(embeddings) -&gt; set[str]:\n            Searches vector DB to identify related engrams.\n    \"\"\"\n\n    def __init__(\n        self,\n        ask_id: str,\n        prompt: Prompt,\n        plugin_manager: PluginManager,\n        metrics_tracker: MetricsTracker[engramic.application.retrieve.retrieve_service.RetrieveMetric],\n        db_plugin: dict[str, Any],\n        service: RetrieveService,\n        library: str | None = None,\n    ) -&gt; None:\n        self.id = ask_id\n        self.service = service\n        self.metrics_tracker: MetricsTracker[engramic.application.retrieve.retrieve_service.RetrieveMetric] = (\n            metrics_tracker\n        )\n        self.library = library\n        self.prompt = prompt\n        self.conversation_direction: dict[str, str]\n        self.prompt_analysis: PromptAnalysis | None = None\n        self.retrieve_gen_conversation_direction_plugin = plugin_manager.get_plugin(\n            'llm', 'retrieve_gen_conversation_direction'\n        )\n        self.prompt_analysis_plugin = plugin_manager.get_plugin('llm', 'retrieve_prompt_analysis')\n        self.prompt_retrieve_indices_plugin = plugin_manager.get_plugin('llm', 'retrieve_gen_index')\n        self.prompt_vector_db_plugin = plugin_manager.get_plugin('vector_db', 'db')\n        self.prompt_db_document_plugin = db_plugin\n        self.embeddings_gen_embed = plugin_manager.get_plugin('embedding', 'gen_embed')\n\n    def get_sources(self) -&gt; None:\n        direction_step = self.service.run_task(self._fetch_history())\n        direction_step.add_done_callback(self.on_fetch_history_complete)\n\n    \"\"\"\n    ### CONVERSATION DIRECTION\n\n    Fetches related domain knowledge based on the prompt intent.\n    \"\"\"\n\n    async def _fetch_history(self) -&gt; list[dict[str, Any]]:\n        plugin = self.prompt_db_document_plugin\n        args = plugin['args']\n        args['history_limit'] = 10\n        args['repo_ids_filters'] = self.prompt.repo_ids_filters\n\n        ret_val = await asyncio.to_thread(plugin['func'].fetch, table=DB.DBTables.HISTORY, ids=[], args=args)\n        history_dict: list[dict[str, Any]] = ret_val[0]\n        return history_dict\n\n    def on_fetch_history_complete(self, fut: Future[Any]) -&gt; None:\n        response_array: dict[str, Any] = fut.result()\n        retrieve_gen_conversation_direction_step = self.service.run_task(\n            self._retrieve_gen_conversation_direction(response_array)\n        )\n        retrieve_gen_conversation_direction_step.add_done_callback(self.on_direction_ret_complete)\n\n    async def _retrieve_gen_conversation_direction(self, response_array: dict[str, Any]) -&gt; dict[str, str]:\n        if __debug__:\n            self.service.send_message_async(self.service.Topic.DEBUG_ASK_CREATED, {'ask_id': self.id})\n\n        input_data = response_array\n        plugin = self.retrieve_gen_conversation_direction_plugin\n\n        if len(self.service.repo_folders.items()) &gt; 0:\n            input_data.update({'all_repos': self.service.repo_folders})\n\n        # add prompt engineering here and submit as the full prompt.\n        prompt_gen = PromptGenConversation(\n            prompt_str=self.prompt.prompt_str, input_data=input_data, repo_ids_filters=self.prompt.repo_ids_filters\n        )\n\n        structured_schema = {\n            'current_user_intent': str,\n            'working_memory_step_1': str,\n            'working_memory_step_2': str,\n            'working_memory_step_3': str,\n            'working_memory_step_4': str,\n        }\n\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt_gen,\n            structured_schema=structured_schema,\n            args=self.service.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        json_parsed: dict[str, str] = json.loads(ret[0]['llm_response'])\n\n        self.conversation_direction = {}\n        self.conversation_direction['current_user_intent'] = json_parsed['current_user_intent']\n\n        self.conversation_direction['working_memory'] = json_parsed['working_memory_step_4']\n\n        if __debug__:\n            self.service.send_message_async(\n                self.service.Topic.DEBUG_CONVERSATION_DIRECTION,\n                {'ask_id': self.id, 'prompt': prompt_gen.render_prompt(), 'working_memory': ret[0]['llm_response']},\n            )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.CONVERSATION_DIRECTION_CALCULATED\n        )\n\n        return json_parsed\n\n    def on_direction_ret_complete(self, fut: Future[Any]) -&gt; None:\n        direction_ret = fut.result()\n\n        logging.debug('current_user_intent: %s', direction_ret)\n        intent_and_direction = direction_ret['current_user_intent']\n\n        embed_step = self.service.run_task(self._embed_gen_direction(intent_and_direction))\n        embed_step.add_done_callback(self.on_embed_direction_complete)\n\n    async def _embed_gen_direction(self, main_prompt: str) -&gt; list[float]:\n        plugin = self.embeddings_gen_embed\n\n        ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=[main_prompt], args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        float_array: list[float] = ret[0]['embeddings_list'][0]\n        return float_array\n\n    def on_embed_direction_complete(self, fut: Future[Any]) -&gt; None:\n        embedding = fut.result()\n        fetch_direction_step = self.service.run_task(self._vector_fetch_direction_meta(embedding))\n        fetch_direction_step.add_done_callback(self.on_vector_fetch_direction_meta_complete)\n\n    async def _vector_fetch_direction_meta(self, embedding: list[float]) -&gt; list[str]:\n        plugin = self.prompt_vector_db_plugin\n\n        ret = await asyncio.to_thread(\n            plugin['func'].query,\n            collection_name='meta',\n            embeddings=embedding,\n            filters=self.prompt.repo_ids_filters,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        list_str: list[str] = ret[0]['query_set']\n        # logging.warning(list_str)\n        return list_str\n\n    def on_vector_fetch_direction_meta_complete(self, fut: Future[Any]) -&gt; None:\n        meta_ids = fut.result()\n        meta_fetch_step = self.service.run_task(self._fetch_direction_meta(meta_ids))\n        meta_fetch_step.add_done_callback(self.on_fetch_direction_meta_complete)\n\n    async def _fetch_direction_meta(self, meta_id: list[str]) -&gt; list[Meta]:\n        meta_list = self.service.meta_repository.load_batch(meta_id)\n\n        if __debug__:\n            dict_meta = [meta.summary_full.text if meta.summary_full is not None else '' for meta in meta_list]\n\n            self.service.send_message_async(\n                self.service.Topic.DEBUG_ASK_META, {'ask_id': self.id, 'ask_meta': dict_meta}\n            )\n\n        return meta_list\n\n    def on_fetch_direction_meta_complete(self, fut: Future[Any]) -&gt; None:\n        meta_list = fut.result()\n        analyze_step = self.service.run_tasks([self._analyze_prompt(meta_list), self._generate_indices(meta_list)])\n        analyze_step.add_done_callback(self.on_analyze_complete)\n\n    \"\"\"\n    ### Prompt Analysis\n\n    Analyzies the prompt and generates lookups that will aid in vector searching of related content\n    \"\"\"\n\n    async def _analyze_prompt(self, meta_list: list[Meta]) -&gt; dict[str, str]:\n        plugin = self.prompt_analysis_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt = PromptAnalyzePrompt(\n            prompt_str=self.prompt.prompt_str,\n            input_data={'meta_list': meta_list, 'working_memory': self.conversation_direction['working_memory']},\n        )\n        structured_response = {'response_length': str, 'user_prompt_type': str, 'thinking_steps': str}\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=structured_response,\n            args=self.service.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        self.metrics_tracker.increment(engramic.application.retrieve.retrieve_service.RetrieveMetric.PROMPTS_ANALYZED)\n\n        if not isinstance(ret[0], dict):\n            error = f'Expected dict[str, str], got {type(ret[0])}'\n            raise TypeError(error)\n\n        return ret[0]\n\n    def on_analyze_complete(self, fut: Future[Any]) -&gt; None:\n        analysis = fut.result()  # This will raise an exception if the coroutine fails\n\n        self.prompt_analysis = PromptAnalysis(\n            json.loads(analysis['_analyze_prompt'][0]['llm_response']),\n            json.loads(analysis['_generate_indices'][0]['llm_response']),\n        )\n\n        genrate_indices_future = self.service.run_task(\n            self._generate_indicies_embeddings(self.prompt_analysis.indices['indices'])\n        )\n        genrate_indices_future.add_done_callback(self.on_indices_embeddings_generated)\n\n    async def _generate_indices(self, meta_list: list[Meta]) -&gt; dict[str, str]:\n        plugin = self.prompt_retrieve_indices_plugin\n        # add prompt engineering here and submit as the full prompt.\n        input_data: dict[str, Any] = {'meta_list': meta_list}\n        if len(self.service.repo_folders.items()) &gt; 0:\n            input_data.update({'all_repos': self.service.repo_folders})\n\n        prompt = PromptGenIndices(\n            prompt_str=self.prompt.prompt_str, input_data=input_data, repo_ids_filters=self.prompt.repo_ids_filters\n        )\n        structured_output = {'indices': list[str]}\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=structured_output,\n            args=self.service.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        if __debug__:\n            prompt_render = prompt.render_prompt()\n            self.service.send_message_async(\n                Service.Topic.DEBUG_ASK_INDICES,\n                {'ask_id': self.id, 'prompt': prompt_render, 'indices': ret[0]['llm_response']},\n            )\n\n        self.service.host.update_mock_data(plugin, ret)\n        response = ret[0]['llm_response']\n        response_json = json.loads(response)\n        count = len(response_json['indices'])\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.DYNAMIC_INDICES_GENERATED, count\n        )\n\n        if not isinstance(ret[0], dict):\n            error = f'Expected dict[str, str], got {type(ret[0])}'\n            raise TypeError(error)\n\n        return ret[0]\n\n    def on_indices_embeddings_generated(self, fut: Future[Any]) -&gt; None:\n        embeddings = fut.result()\n\n        query_index_db_future = self.service.run_task(self._query_index_db(embeddings))\n        query_index_db_future.add_done_callback(self.on_query_index_db)\n\n    async def _generate_indicies_embeddings(self, indices: list[str]) -&gt; list[list[float]]:\n        plugin = self.embeddings_gen_embed\n\n        ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=indices, args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        embeddings_list: list[list[float]] = ret[0]['embeddings_list']\n        return embeddings_list\n\n    \"\"\"\n    ### Fetch Engram IDs\n\n    Use the indices to fetch related Engram IDs\n    \"\"\"\n\n    async def _query_index_db(self, embeddings: list[list[float]]) -&gt; set[str]:\n        plugin = self.prompt_vector_db_plugin\n\n        ids = set()\n\n        ret = await asyncio.to_thread(\n            plugin['func'].query,\n            collection_name='main',\n            embeddings=embeddings,\n            filters=self.prompt.repo_ids_filters,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        ids.update(ret[0]['query_set'])\n\n        num_queries = len(ids)\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.VECTOR_DB_QUERIES, num_queries\n        )\n\n        return ids\n\n    def on_query_index_db(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        logging.debug('Query Result: %s', ret)\n\n        if self.prompt_analysis is None:\n            error = 'on_query_index_db failed: prompt_analysis is None and likely failed during an earlier process.'\n            raise RuntimeError\n\n        retrieve_result = RetrieveResult(\n            self.id,\n            self.prompt.prompt_id,\n            engram_id_array=list(ret),\n            conversation_direction=self.conversation_direction,\n            analysis=asdict(self.prompt_analysis)['prompt_analysis'],\n        )\n\n        if self.prompt_analysis is None:\n            error = 'Prompt analysis None in on_query_index_db'\n            raise RuntimeError(error)\n\n        retrieve_response = {\n            'analysis': asdict(self.prompt_analysis),\n            'prompt': asdict(self.prompt),\n            'retrieve_response': asdict(retrieve_result),\n        }\n\n        if __debug__:\n            self.service.host.update_mock_data_output(self.service, retrieve_response)\n\n        self.service.send_message_async(Service.Topic.RETRIEVE_COMPLETE, retrieve_response)\n</code></pre>"},{"location":"reference/codify_service/","title":"Codify Service","text":"<p>               Bases: <code>Service</code></p> <p>A service responsible for validating and extracting engrams from AI model responses using a TOML-based validation pipeline.</p> <p>This service listens for prompts that have completed processing, and if the system is in training mode, it fetches related engrams and metadata, applies an LLM-based validation process, and stores structured observations. It tracks metrics related to its activity and supports training workflows.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manages access to system plugins such as the LLM and document DB.</p> <code>llm_validate</code> <code>dict</code> <p>Plugin for LLM-based validation.</p> <code>db_document_plugin</code> <code>dict</code> <p>Plugin for document database access.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Repository for accessing and managing engram data.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Repository for associated metadata retrieval.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles validation and normalization of observation data.</p> <code>prompt</code> <code>Prompt</code> <p>Default prompt object used during validation.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks custom CodifyMetric metrics.</p> <code>training_mode</code> <code>bool</code> <p>Flag indicating whether the system is in training mode.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes the service to key topics.</p> <code>stop</code> <p>Stops the service.</p> <code>init_async</code> <p>Initializes async components, including DB connections.</p> <code>on_main_prompt_complete</code> <p>dict[str, Any]) -&gt; None: Main entry point triggered after a model completes a prompt.</p> <code>_fetch_engrams</code> <p>Response) -&gt; dict[str, Any]: Asynchronously fetches engrams associated with a response.</p> <code>on_fetch_engram_complete</code> <p>Future[Any]) -&gt; None: Callback that processes fetched engrams and triggers metadata retrieval.</p> <code>_fetch_meta</code> <p>list[Engram], meta_id_array: list[str], response: Response) -&gt; dict[str, Any]: Asynchronously fetches metadata for given engrams.</p> <code>on_fetch_meta_complete</code> <p>Future[Any]) -&gt; None: Callback that begins the validation process after fetching metadata.</p> <code>_validate</code> <p>list[Engram], meta_array: list[Meta], response: Response) -&gt; dict[str, Any]: Runs the validation plugin on the response and returns an observation.</p> <code>on_validate_complete</code> <p>Future[Any]) -&gt; None: Final step that emits the completed observation to other systems.</p> <code>on_acknowledge</code> <p>str) -&gt; None: Responds to ACK messages by reporting and resetting metrics.</p> Source code in <code>src/engramic/application/codify/codify_service.py</code> <pre><code>class CodifyService(Service):\n    \"\"\"\n    A service responsible for validating and extracting engrams from AI model responses using a TOML-based validation pipeline.\n\n    This service listens for prompts that have completed processing, and if the system is in training mode, it fetches related engrams and metadata, applies an LLM-based validation process, and stores structured observations. It tracks metrics related to its activity and supports training workflows.\n\n    Attributes:\n        plugin_manager (PluginManager): Manages access to system plugins such as the LLM and document DB.\n        llm_validate (dict): Plugin for LLM-based validation.\n        db_document_plugin (dict): Plugin for document database access.\n        engram_repository (EngramRepository): Repository for accessing and managing engram data.\n        meta_repository (MetaRepository): Repository for associated metadata retrieval.\n        observation_repository (ObservationRepository): Handles validation and normalization of observation data.\n        prompt (Prompt): Default prompt object used during validation.\n        metrics_tracker (MetricsTracker): Tracks custom CodifyMetric metrics.\n        training_mode (bool): Flag indicating whether the system is in training mode.\n\n    Methods:\n        start() -&gt; None:\n            Subscribes the service to key topics.\n        stop() -&gt; None:\n            Stops the service.\n        init_async() -&gt; None:\n            Initializes async components, including DB connections.\n        on_main_prompt_complete(response_dict: dict[str, Any]) -&gt; None:\n            Main entry point triggered after a model completes a prompt.\n        _fetch_engrams(response: Response) -&gt; dict[str, Any]:\n            Asynchronously fetches engrams associated with a response.\n        on_fetch_engram_complete(fut: Future[Any]) -&gt; None:\n            Callback that processes fetched engrams and triggers metadata retrieval.\n        _fetch_meta(engram_array: list[Engram], meta_id_array: list[str], response: Response) -&gt; dict[str, Any]:\n            Asynchronously fetches metadata for given engrams.\n        on_fetch_meta_complete(fut: Future[Any]) -&gt; None:\n            Callback that begins the validation process after fetching metadata.\n        _validate(engram_array: list[Engram], meta_array: list[Meta], response: Response) -&gt; dict[str, Any]:\n            Runs the validation plugin on the response and returns an observation.\n        on_validate_complete(fut: Future[Any]) -&gt; None:\n            Final step that emits the completed observation to other systems.\n        on_acknowledge(message_in: str) -&gt; None:\n            Responds to ACK messages by reporting and resetting metrics.\n    \"\"\"\n\n    ACCURACY_CONSTANT = 3\n    RELEVANCY_CONSTANT = 3\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.llm_validate = self.plugin_manager.get_plugin('llm', 'validate')\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.meta_repository: MetaRepository = MetaRepository(self.db_document_plugin)\n        self.observation_repository: ObservationRepository = ObservationRepository(self.db_document_plugin)\n\n        self.prompt = Prompt('Validate the llm.')\n        self.metrics_tracker: MetricsTracker[CodifyMetric] = MetricsTracker[CodifyMetric]()\n        self.training_mode = False\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_main_prompt_complete(self, response_dict: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, response_dict)\n\n        prompt = Prompt(**response_dict['prompt'])\n        if not prompt.training_mode:\n            return\n\n        model = response_dict['model']\n        analysis = PromptAnalysis(**response_dict['analysis'])\n        retrieve_result = RetrieveResult(**response_dict['retrieve_result'])\n        response = Response(\n            response_dict['id'],\n            response_dict['source_id'],\n            response_dict['response'],\n            retrieve_result,\n            prompt,\n            analysis,\n            model,\n        )\n        self.metrics_tracker.increment(CodifyMetric.RESPONSE_RECIEVED)\n        fetch_engram_step = self.run_task(self._fetch_engrams(response))\n        fetch_engram_step.add_done_callback(self.on_fetch_engram_complete)\n\n    \"\"\"\n    ### Fetch Engrams &amp; Meta\n\n    Fetch engrams based on retrieved results.\n    \"\"\"\n\n    async def _fetch_engrams(self, response: Response) -&gt; dict[str, Any]:\n        engram_array: list[Engram] = await asyncio.to_thread(\n            self.engram_repository.load_batch_retrieve_result, response.retrieve_result\n        )\n\n        self.metrics_tracker.increment(CodifyMetric.ENGRAM_FETCHED, len(engram_array))\n\n        meta_array: set[str] = set()\n        for engram in engram_array:\n            if engram.meta_ids is not None:\n                meta_array.update(engram.meta_ids)\n\n        return {'engram_array': engram_array, 'meta_array': list(meta_array), 'response': response}\n\n    def on_fetch_engram_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        fetch_meta_step = self.run_task(self._fetch_meta(ret['engram_array'], ret['meta_array'], ret['response']))\n        fetch_meta_step.add_done_callback(self.on_fetch_meta_complete)\n\n    async def _fetch_meta(\n        self, engram_array: list[Engram], meta_id_array: list[str], response: Response\n    ) -&gt; dict[str, Any]:\n        meta_array: list[Meta] = await asyncio.to_thread(self.meta_repository.load_batch, meta_id_array)\n        # assembled main_prompt, render engrams.\n\n        return {'engram_array': engram_array, 'meta_array': meta_array, 'response': response}\n\n    def on_fetch_meta_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        fetch_meta_step = self.run_task(self._validate(ret['engram_array'], ret['meta_array'], ret['response']))\n        fetch_meta_step.add_done_callback(self.on_validate_complete)\n\n    \"\"\"\n    ### Validate\n\n    Validates and extracts engrams (i.e. memories) from responses.\n    \"\"\"\n\n    async def _validate(self, engram_array: list[Engram], meta_array: list[Meta], response: Response) -&gt; dict[str, Any]:\n        # insert prompt engineering\n\n        del meta_array\n\n        input_data = {\n            'engram_list': engram_array,\n            'response': response.response,\n        }\n\n        prompt = PromptValidatePrompt(\n            response.prompt.prompt_str,\n            input_data=input_data,\n            is_lesson=response.prompt.is_lesson,\n            training_mode=response.prompt.training_mode,\n        )\n\n        plugin = self.llm_validate\n        validate_response = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=None,\n            args=self.host.mock_update_args(plugin),\n            images=None,\n        )\n\n        self.host.update_mock_data(self.llm_validate, validate_response)\n\n        toml_data = None\n\n        try:\n            if __debug__:\n                prompt_render = prompt.render_prompt()\n                self.send_message_async(\n                    Service.Topic.DEBUG_OBSERVATION_TOML_COMPLETE,\n                    {'prompt': prompt_render, 'toml': validate_response[0]['llm_response'], 'response_id': response.id},\n                )\n\n            toml_data = tomli.loads(validate_response[0]['llm_response'])\n\n        except tomli.TOMLDecodeError as e:\n            logging.exception('TOML decode error: %s', validate_response[0]['llm_response'])\n            error = 'Malformed TOML file in codify:validate.'\n            raise TypeError(error) from e\n\n        if 'not_memorable' in toml_data:\n            return {'return_observation': None}\n\n        if not self.observation_repository.validate_toml_dict(toml_data):\n            error = 'Codify TOML did not pass validation.'\n            raise TypeError(error)\n\n        return_observation = self.observation_repository.load_toml_dict(\n            self.observation_repository.normalize_toml_dict(toml_data, response)\n        )\n\n        # if this observation is from multiple sources, it must merge the sources into it's meta.\n        if len(engram_array) &gt; 0:\n            merged_data = return_observation.merge_observation(\n                return_observation,\n                CodifyService.ACCURACY_CONSTANT,\n                CodifyService.RELEVANCY_CONSTANT,\n                self.engram_repository,\n            )\n\n            # Cast merged_data to the same type as return_observation\n            return_observation_merged = type(return_observation)(**asdict(merged_data))\n\n            return_observation = return_observation_merged\n\n        self.metrics_tracker.increment(CodifyMetric.ENGRAM_VALIDATED)\n        self.send_message_async(\n            Service.Topic.OBSERVATION_CREATED, {'id': return_observation.id, 'parent_id': return_observation.parent_id}\n        )\n\n        return {'return_observation': return_observation}\n\n    def on_validate_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n\n        if ret['return_observation'] is not None:\n            self.send_message_async(Service.Topic.OBSERVATION_COMPLETE, asdict(ret['return_observation']))\n\n        if __debug__ and ret['return_observation'] is not None:\n            self.host.update_mock_data_output(self, asdict(ret['return_observation']))\n\n    \"\"\"\n    ### Ack\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/consolidate_service/","title":"Consolidate Service","text":"<p>               Bases: <code>Service</code></p> <p>Orchestrates the post-processing pipeline for completed observations.</p> <p>This service is triggered when an observation is marked complete and coordinates summarization, engram generation, index generation, and embedding creation through the following pipeline stages:</p> <ol> <li>Summarization - Generates a natural language summary from the observation using an LLM plugin.</li> <li>Embedding Summaries - Uses an embedding plugin to create vector embeddings of the summary text.</li> <li>Engram Generation - Extracts or constructs engrams from the observation's content.</li> <li>Index Generation - Applies an LLM to generate meaningful textual indices for each engram.</li> <li>Embedding Indices - Uses an embedding plugin to convert each index into a vector representation.</li> <li>Publishing Results - Emits messages like <code>ENGRAM_COMPLETE</code>, <code>META_COMPLETE</code>, and <code>INDEX_COMPLETE</code>    at various stages to notify downstream systems.</li> </ol> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manages access to all system plugins.</p> <code>llm_summary</code> <code>dict</code> <p>Plugin used for generating summaries.</p> <code>llm_gen_indices</code> <code>dict</code> <p>Plugin used for generating indices from engrams.</p> <code>embedding_gen_embed</code> <code>dict</code> <p>Plugin used for generating embeddings for summaries and indices.</p> <code>db_document</code> <code>dict</code> <p>Plugin for document-level database access.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles deserialization of incoming observations.</p> <code>engram_builder</code> <code>dict[str, Engram]</code> <p>In-memory store of engrams awaiting completion.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks metrics across each processing stage.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes the service to message topics.</p> <code>stop</code> <p>Stops the service and clears subscriptions.</p> <code>on_observation_complete</code> <p>Handles post-processing when an observation completes.</p> <code>_generate_summary_embeddings</code> <p>Creates and attaches embeddings for a summary.</p> <code>process_engrams</code> <p>Orchestrates the generation of indices and embeddings for engrams.</p> <code>_gen_indices</code> <p>Uses an LLM to create indices from an engram.</p> <code>_gen_embeddings</code> <p>Creates embeddings for generated indices.</p> <code>on_acknowledge</code> <p>Sends a metrics snapshot for observability/debugging.</p> Source code in <code>src/engramic/application/consolidate/consolidate_service.py</code> <pre><code>class ConsolidateService(Service):\n    \"\"\"\n    Orchestrates the post-processing pipeline for completed observations.\n\n    This service is triggered when an observation is marked complete and coordinates summarization,\n    engram generation, index generation, and embedding creation through the following pipeline stages:\n\n    1. **Summarization** - Generates a natural language summary from the observation using an LLM plugin.\n    2. **Embedding Summaries** - Uses an embedding plugin to create vector embeddings of the summary text.\n    3. **Engram Generation** - Extracts or constructs engrams from the observation's content.\n    4. **Index Generation** - Applies an LLM to generate meaningful textual indices for each engram.\n    5. **Embedding Indices** - Uses an embedding plugin to convert each index into a vector representation.\n    6. **Publishing Results** - Emits messages like `ENGRAM_COMPLETE`, `META_COMPLETE`, and `INDEX_COMPLETE`\n       at various stages to notify downstream systems.\n\n    Attributes:\n        plugin_manager (PluginManager): Manages access to all system plugins.\n        llm_summary (dict): Plugin used for generating summaries.\n        llm_gen_indices (dict): Plugin used for generating indices from engrams.\n        embedding_gen_embed (dict): Plugin used for generating embeddings for summaries and indices.\n        db_document (dict): Plugin for document-level database access.\n        observation_repository (ObservationRepository): Handles deserialization of incoming observations.\n        engram_builder (dict[str, Engram]): In-memory store of engrams awaiting completion.\n        metrics_tracker (MetricsTracker): Tracks metrics across each processing stage.\n\n    Methods:\n        start() -&gt; None:\n            Subscribes the service to message topics.\n        stop() -&gt; None:\n            Stops the service and clears subscriptions.\n        on_observation_complete(observation_dict) -&gt; None:\n            Handles post-processing when an observation completes.\n        _generate_summary_embeddings(observation) -&gt; Meta:\n            Creates and attaches embeddings for a summary.\n        process_engrams(observation) -&gt; None:\n            Orchestrates the generation of indices and embeddings for engrams.\n        _gen_indices(index, engram, repo_ids, tracking_id) -&gt; dict:\n            Uses an LLM to create indices from an engram.\n        _gen_embeddings(id_and_index_dict, process_index) -&gt; dict:\n            Creates embeddings for generated indices.\n        on_acknowledge(message_in) -&gt; None:\n            Sends a metrics snapshot for observability/debugging.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.llm_summary: dict[str, Any] = self.plugin_manager.get_plugin('llm', 'summary')\n        self.llm_gen_indices: dict[str, Any] = self.plugin_manager.get_plugin('llm', 'gen_indices')\n        self.embedding_gen_embed: dict[str, Any] = self.plugin_manager.get_plugin('embedding', 'gen_embed')\n        self.db_document: dict[str, Any] = self.plugin_manager.get_plugin('db', 'document')\n        self.observation_repository = ObservationRepository(self.db_document)\n        self.engram_builder: dict[str, Engram] = {}\n        self.metrics_tracker: MetricsTracker[ConsolidateMetric] = MetricsTracker[ConsolidateMetric]()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def on_observation_complete(self, observation_dict: dict[str, Any]) -&gt; None:\n        # print(\"run consolidate\")\n        # should run a task for this.\n        observation = self.observation_repository.load_dict(observation_dict)\n\n        if __debug__:\n            if observation.tracking_id is None:\n                error = 'Tracking id is None but expected not to be.'\n                raise ValueError(error)\n\n            self.host.update_mock_data_input(self, observation_dict, observation.tracking_id)\n\n        self.metrics_tracker.increment(ConsolidateMetric.OBSERVATIONS_RECIEVED)\n\n        # So, a bit of a race condition here. If meta lags engrams could signal inserterd before the meta.\n        # I think this is unlikely to happen practically, but it would be better to fix this and know for sure.\n        future = self.run_task(self._generate_summary_embeddings(observation))\n        future.add_done_callback(self.on_generate_summary_embeddings)\n\n        self.process_engrams(observation)\n\n    \"\"\"\n    ### Generate meta embeddings\n    \"\"\"\n\n    async def _generate_summary_embeddings(self, observation: Observation) -&gt; Meta:\n        if observation.meta.summary_full is None:\n            error = 'Summary full is none.'\n            raise ValueError(error)\n\n        plugin = self.embedding_gen_embed\n        embedding_list_ret = await asyncio.to_thread(\n            plugin['func'].gen_embed,\n            strings=[observation.meta.summary_full.text],\n            args=self.host.mock_update_args(plugin, 0, str(observation.meta.source_ids)),\n        )\n\n        self.host.update_mock_data(plugin, embedding_list_ret, 0, str(observation.meta.source_ids))\n\n        embedding_list = embedding_list_ret[0]['embeddings_list']\n        observation.meta.summary_full.embedding = embedding_list[0]\n        return observation.meta\n\n    def on_generate_summary_embeddings(self, future: Future[Any]) -&gt; None:\n        meta = future.result()\n        self.send_message_async(Service.Topic.META_COMPLETE, asdict(meta))\n\n    \"\"\"\n    ### Generate Engrams\n\n    Create engrams from the observation.\n    \"\"\"\n\n    async def _generate_engrams(self, observation: Observation) -&gt; Observation:\n        self.metrics_tracker.increment(ConsolidateMetric.ENGRAMS_GENERATED, len(observation.engram_list))\n\n        return observation\n\n    def process_engrams(self, observation: Observation) -&gt; None:\n        engram_list = observation.engram_list\n        engram_ids = [engram.id for engram in engram_list]\n\n        self.send_message_async(\n            Service.Topic.ENGRAMS_CREATED,\n            {'engram_id_array': engram_ids, 'parent_id': observation.id, 'tracking_id': observation.tracking_id},\n        )\n\n        # Keep references so we can fill them in later\n        for engram in engram_list:\n            if self.engram_builder.get(engram.id) is None:\n                self.engram_builder[engram.id] = engram\n            else:\n                error = 'Engram ID Collision. During conslidation, two Engrams with the same IDs were detected.'\n                raise RuntimeError(error)\n\n        # 1) Generate indices for each engram\n        index_tasks = [\n            self._gen_indices(i, engram, engram.repo_ids, observation.tracking_id)\n            for i, engram in enumerate(engram_list)\n        ]\n\n        indices_future = self.run_tasks(index_tasks)\n\n        indices_future.add_done_callback(self.on_indices_done)\n\n    async def _gen_indices(\n        self, index: int, engram: Engram, repo_ids: list[str] | None, tracking_id: str | None\n    ) -&gt; dict[str, Any]:\n        data_input = {'engram': engram}\n\n        prompt = PromptGenIndices(prompt_str='', input_data=data_input)\n        plugin = self.llm_gen_indices\n\n        response_schema = {'index_text_array': list[str]}\n\n        indices = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=response_schema,\n            args=self.host.mock_update_args(plugin, index, str(tracking_id)),\n            images=None,\n        )\n\n        load_json = json.loads(indices[0]['llm_response'])\n        response_json: dict[str, Any] = {'index_text_array': []}\n\n        # generate context\n        context_string = 'Context: '\n\n        if engram.context is None:\n            error = 'None context found in engram.'\n            raise RuntimeError(error)\n\n        for item, key in engram.context.items():\n            if key != 'null':\n                context_string += f'{item}: {key}\\n'\n\n        # add in the context to each index.\n        for index_item in load_json['index_text_array']:\n            response_json['index_text_array'].append(context_string + ' Content: ' + index_item)\n\n        self.host.update_mock_data(plugin, indices, index, str(tracking_id))\n\n        self.metrics_tracker.increment(ConsolidateMetric.INDICES_GENERATED, len(indices))\n\n        if len(response_json['index_text_array']) == 0:\n            error = 'An empty index was created.'\n            raise RuntimeError(error)\n\n        return {\n            'engram_id': engram.id,\n            'indices': response_json['index_text_array'],\n            'repo_ids': repo_ids,\n            'tracking_id': tracking_id,\n        }\n\n    # Once all indices are generated, generate embeddings\n    def on_indices_done(self, indices_list_fut: Future[Any]) -&gt; None:\n        # This is the accumulated result of each gen_indices(...) call\n        indices_list: dict[str, Any] = indices_list_fut.result()\n        # indices_list should have a key like 'gen_indices' -&gt; list[dict[str, Any]]\n        index_sets: list[dict[str, Any]] = indices_list['_gen_indices']\n\n        # 2) Generate embeddings for each index set\n        embed_tasks = [self._gen_embeddings(index_set, i) for i, index_set in enumerate(index_sets)]\n\n        embed_future = self.run_tasks(embed_tasks)\n\n        embed_future.add_done_callback(self.on_embeddings_done)\n\n    async def _gen_embeddings(self, id_and_index_dict: dict[str, Any], process_index: int) -&gt; dict[str, Any]:\n        indices = id_and_index_dict['indices']\n        engram_id: str = id_and_index_dict['engram_id']\n        repo_ids: str = id_and_index_dict['repo_ids']\n        tracking_id: str = id_and_index_dict['tracking_id']\n\n        plugin = self.embedding_gen_embed\n\n        embedding_list_ret = await asyncio.to_thread(\n            plugin['func'].gen_embed,\n            strings=indices,\n            args=self.host.mock_update_args(plugin, process_index, tracking_id),\n        )\n\n        self.host.update_mock_data(plugin, embedding_list_ret, process_index, tracking_id)\n\n        embedding_list = embedding_list_ret[0]['embeddings_list']\n\n        self.metrics_tracker.increment(ConsolidateMetric.EMBEDDINGS_GENERATED, len(embedding_list))\n\n        index_id_array = []\n\n        # Convert raw embeddings to Index objects and attach them\n        try:\n            index_array: list[Index] = []\n            for i, vec in enumerate(embedding_list):\n                index = Index(indices[i], vec)\n                index_array.append(index)\n                index_id_array.append(index.id)\n        except Exception:\n            logging.exception('Exception caught.')\n\n        self.send_message_async(\n            Service.Topic.INDICES_CREATED,\n            {'parent_id': engram_id, 'index_id_array': index_id_array, 'tracking_id': tracking_id},\n        )\n\n        self.engram_builder[engram_id].indices = index_array\n        serialized_index_array = [asdict(index) for index in index_array]\n\n        # Return the ID so we know which engram was updated\n        return {\n            'engram_id': engram_id,\n            'tracking_id': tracking_id,\n            'index_array': serialized_index_array,\n            'repo_ids': repo_ids,\n        }\n\n    # Once embeddings are generated, then we're truly done\n    def on_embeddings_done(self, embed_fut: Future[Any]) -&gt; None:\n        ret = embed_fut.result()  # ret should have 'gen_embeddings' -&gt; list of engram IDs\n\n        ret_dict = ret['_gen_embeddings']  # which IDs got their embeddings updated\n\n        # Now that embeddings exist, we can send \"ENGRAM_COMPLETE\" for each\n        engram_dict: list[dict[str, Any]] = []\n\n        for engram in ret_dict:\n            builder_data: dict[str, Any] = asdict(self.engram_builder[engram['engram_id']])\n            engram_dict.append(builder_data)\n\n            # We can optionally notify about newly attached indices\n            self.send_message_async(\n                Service.Topic.INDICES_COMPLETE,\n                {\n                    'index': engram['index_array'],\n                    'engram_id': engram['engram_id'],\n                    'tracking_id': engram['tracking_id'],\n                    'repo_ids': engram['repo_ids'],\n                },\n            )\n\n        self.send_message_async(\n            Service.Topic.ENGRAM_COMPLETE, {'engram_array': engram_dict, 'tracking_id': ret_dict[0]['tracking_id']}\n        )\n\n        if __debug__:\n            self.host.update_mock_data_output(self, {'engram_array': engram_dict}, ret_dict[0]['tracking_id'])\n\n        for eid in ret_dict:\n            del self.engram_builder[eid['engram_id']]\n\n    \"\"\"\n    ### Acknowledge\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/emgram/","title":"Engram Class","text":"<p>Represents a unit of memory that encapsulates a text fragment with rich metadata for semantic indexing and contextual relevance.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the engram.</p> <code>locations</code> <code>list[str]</code> <p>One or more file paths, URLs, or other locations associated with the engram.</p> <code>source_ids</code> <code>list[str]</code> <p>Identifiers of the original source documents from which the engram was derived.</p> <code>content</code> <code>str</code> <p>The main textual content of the engram.</p> <code>is_native_source</code> <code>bool</code> <p>Indicates if the content was directly extracted (True) or generated (False).</p> <code>context</code> <code>dict[str, str] | None</code> <p>Optional contextual metadata in key-value format to enhance retrieval or classification.</p> <code>indices</code> <code>list[Index] | None</code> <p>Optional semantic indices, typically for vector-based retrieval.</p> <code>meta_ids</code> <code>list[str] | None</code> <p>Optional list of metadata tags or identifiers relevant to the engram.</p> <code>repo_ids</code> <code>list[str] | None</code> <p>Optional identifiers linking this engram to repositories or code bases.</p> <code>accuracy</code> <code>int | None</code> <p>Optional accuracy score assigned during validation (e.g., via Codify Service).</p> <code>relevancy</code> <code>int | None</code> <p>Optional relevancy score assigned during validation (e.g., via Codify Service).</p> <code>created_date</code> <code>int | None</code> <p>Optional Unix timestamp representing the creation time of the engram.</p> <p>Methods:</p> Name Description <code>generate_toml</code> <p>Serializes the engram into a TOML-formatted string, including non-null fields. Nested indices are flattened, and context is rendered as an inline TOML table.</p> Source code in <code>src/engramic/core/engram.py</code> <pre><code>@dataclass()\nclass Engram:\n    \"\"\"\n    Represents a unit of memory that encapsulates a text fragment with rich metadata\n    for semantic indexing and contextual relevance.\n\n    Attributes:\n        id (str): Unique identifier for the engram.\n        locations (list[str]): One or more file paths, URLs, or other locations associated with the engram.\n        source_ids (list[str]): Identifiers of the original source documents from which the engram was derived.\n        content (str): The main textual content of the engram.\n        is_native_source (bool): Indicates if the content was directly extracted (True) or generated (False).\n        context (dict[str, str] | None): Optional contextual metadata in key-value format to enhance retrieval or classification.\n        indices (list[Index] | None): Optional semantic indices, typically for vector-based retrieval.\n        meta_ids (list[str] | None): Optional list of metadata tags or identifiers relevant to the engram.\n        repo_ids (list[str] | None): Optional identifiers linking this engram to repositories or code bases.\n        accuracy (int | None): Optional accuracy score assigned during validation (e.g., via Codify Service).\n        relevancy (int | None): Optional relevancy score assigned during validation (e.g., via Codify Service).\n        created_date (int | None): Optional Unix timestamp representing the creation time of the engram.\n\n    Methods:\n        generate_toml() -&gt; str:\n            Serializes the engram into a TOML-formatted string, including non-null fields.\n            Nested indices are flattened, and context is rendered as an inline TOML table.\n    \"\"\"\n\n    id: str\n    locations: list[str]\n    source_ids: list[str]\n    content: str\n    is_native_source: bool\n    context: dict[str, str] | None = None\n    indices: list[Index] | None = None\n    meta_ids: list[str] | None = None\n    repo_ids: list[str] | None = None\n    accuracy: int | None = 0\n    relevancy: int | None = 0\n    created_date: int | None = None\n\n    def generate_toml(self) -&gt; str:\n        def toml_escape(value: str) -&gt; str:\n            return f'\"{value}\"'\n\n        def toml_list(values: list[str]) -&gt; str:\n            return '[' + ', '.join(toml_escape(v) for v in values) + ']'\n\n        lines = [\n            f'id = {toml_escape(self.id)}',\n            f'content = {toml_escape(self.content)}',\n            f'is_native_source = {str(self.is_native_source).lower()}',\n            f'locations = {toml_list(self.locations)}',\n            f'source_ids = {toml_list(self.source_ids)}',\n        ]\n\n        if self.meta_ids:\n            lines.append(f'meta_ids = {toml_list(self.meta_ids)}')\n\n        if self.repo_ids:\n            lines.append(f'repo_ids = {toml_list(self.repo_ids)}')\n\n        if self.context:\n            # Assuming context has a render_toml() method or can be represented as a dict\n            inline = ', '.join(f'{k} = {toml_escape(v)}' for k, v in self.context.items())\n            lines.append(f'context = {{ {inline} }}')\n\n        if self.indices:\n            # Flatten the index section\n            for index in self.indices:\n                # Assuming index has `text` and `embedding` attributes\n                if index.text is None:\n                    error = 'Null text in generate_toml.'\n                    raise ValueError(error)\n\n                lines.extend([\n                    '[[indices]]',\n                    f'text = {toml_escape(index.text)}',\n                    f'embedding = {toml_escape(str(index.embedding))}',\n                ])\n\n        return '\\n'.join(lines)\n</code></pre>"},{"location":"reference/lesson/","title":"lesson","text":"<p>Represents an educational lesson generated from a document.</p> <p>Handles the generation of questions and educational prompts based on document content and metadata.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the lesson.</p> <code>doc_id</code> <code>str</code> <p>Identifier of the source document.</p> <code>tracking_id</code> <code>str</code> <p>ID for tracking the lesson's progress.</p> <code>service</code> <code>TeachService</code> <p>Parent service managing this lesson.</p> <code>meta</code> <code>Meta</code> <p>Metadata about the source document.</p> <p>Methods:</p> Name Description <code>run_lesson</code> <p>Initiates the lesson generation process.</p> <code>generate_questions</code> <p>Generates educational questions based on document content.</p> <code>on_questions_generated</code> <p>Processes generated questions and creates prompts.</p> <code>_on_send_prompt_complete</code> <p>Handles completion of prompt submission.</p> Source code in <code>src/engramic/application/teach/lesson.py</code> <pre><code>class Lesson:\n    \"\"\"\n    Represents an educational lesson generated from a document.\n\n    Handles the generation of questions and educational prompts based on\n    document content and metadata.\n\n    Attributes:\n        id (str): Unique identifier for the lesson.\n        doc_id (str): Identifier of the source document.\n        tracking_id (str): ID for tracking the lesson's progress.\n        service (TeachService): Parent service managing this lesson.\n        meta (Meta): Metadata about the source document.\n\n    Methods:\n        run_lesson(meta_in) -&gt; None:\n            Initiates the lesson generation process.\n        generate_questions() -&gt; Any:\n            Generates educational questions based on document content.\n        on_questions_generated(future) -&gt; None:\n            Processes generated questions and creates prompts.\n        _on_send_prompt_complete(ret) -&gt; None:\n            Handles completion of prompt submission.\n    \"\"\"\n\n    def __init__(self, parent_service: TeachService, lesson_id: str, tracking_id: str, doc_id: str) -&gt; None:\n        \"\"\"\n        Initializes a new Lesson instance.\n\n        Args:\n            parent_service (TeachService): The service managing this lesson.\n            lesson_id (str): Unique identifier for the lesson.\n            tracking_id (str): ID for tracking the lesson's progress.\n            doc_id (str): Identifier of the source document.\n        \"\"\"\n        self.id = lesson_id\n        self.doc_id = doc_id\n        self.tracking_id = tracking_id\n        self.service = parent_service\n\n    def run_lesson(self, meta_in: Meta) -&gt; None:\n        \"\"\"\n        Initiates the lesson generation process.\n\n        Stores the document metadata and starts the question generation task.\n\n        Args:\n            meta_in (Meta): Metadata about the source document.\n        \"\"\"\n        self.meta = meta_in\n        future = self.service.run_task(self.generate_questions())\n        future.add_done_callback(self.on_questions_generated)\n\n    async def generate_questions(self) -&gt; Any:\n        \"\"\"\n        Generates educational questions based on document content.\n\n        Uses an LLM plugin to generate study questions from the document metadata.\n\n        Returns:\n            dict: The generated questions and study actions.\n        \"\"\"\n        plugin = self.service.teach_generate_questions\n\n        prompt = PromptGenQuestions(input_data={'meta': asdict(self.meta)})\n\n        structured_response = {'study_actions': list[str]}\n\n        ret = plugin['func'].submit(\n            prompt=prompt,\n            images=None,\n            structured_schema=structured_response,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        initial_scan = json.loads(ret[0]['llm_response'])\n\n        return initial_scan\n\n    def on_questions_generated(self, future: Future[Any]) -&gt; None:\n        \"\"\"\n        Processes generated questions and creates learning prompts.\n\n        Takes the questions from the generator and submits them as learning prompts.\n        Also adds document-specific questions and notifies about lesson creation.\n\n        Args:\n            future (Future[Any]): Future containing the generated questions.\n        \"\"\"\n        res = future.result()\n        text_prompts = res['study_actions']\n\n        if self.meta.type == self.meta.SourceType.DOCUMENT.value:\n            location = self.meta.locations[0]\n\n            # generate some static question for file discovery.\n            text_prompts.append(f'Tell me about the file {location}')\n\n        async def send_prompt(question: str) -&gt; None:\n            self.service.send_message_async(\n                Service.Topic.SUBMIT_PROMPT,\n                {\n                    'prompt_str': question,\n                    'parent_id': self.id,\n                    'training_mode': True,\n                    'is_lesson': True,\n                    'tracking_id': self.tracking_id,\n                    'repo_ids_filters': self.meta.repo_ids,\n                },\n            )\n\n        for text_prompt in reversed(text_prompts):\n            future = self.service.run_task(send_prompt(text_prompt))\n            future.add_done_callback(self._on_send_prompt_complete)\n\n        async def send_lesson() -&gt; None:\n            self.service.send_message_async(\n                Service.Topic.LESSON_CREATED,\n                {'id': self.id, 'tracking_id': self.tracking_id, 'doc_id': self.doc_id},\n            )\n\n        self.service.run_task(send_lesson())\n\n    def _on_send_prompt_complete(self, ret: Future[Any]) -&gt; None:\n        \"\"\"\n        Handles completion of prompt submission.\n\n        Args:\n            ret (Future[Any]): Future representing the completed prompt submission.\n        \"\"\"\n        ret.result()\n</code></pre>"},{"location":"reference/lesson/#engramic.application.teach.lesson.Lesson.__init__","title":"<code>__init__(parent_service, lesson_id, tracking_id, doc_id)</code>","text":"<p>Initializes a new Lesson instance.</p> <p>Parameters:</p> Name Type Description Default <code>parent_service</code> <code>TeachService</code> <p>The service managing this lesson.</p> required <code>lesson_id</code> <code>str</code> <p>Unique identifier for the lesson.</p> required <code>tracking_id</code> <code>str</code> <p>ID for tracking the lesson's progress.</p> required <code>doc_id</code> <code>str</code> <p>Identifier of the source document.</p> required Source code in <code>src/engramic/application/teach/lesson.py</code> <pre><code>def __init__(self, parent_service: TeachService, lesson_id: str, tracking_id: str, doc_id: str) -&gt; None:\n    \"\"\"\n    Initializes a new Lesson instance.\n\n    Args:\n        parent_service (TeachService): The service managing this lesson.\n        lesson_id (str): Unique identifier for the lesson.\n        tracking_id (str): ID for tracking the lesson's progress.\n        doc_id (str): Identifier of the source document.\n    \"\"\"\n    self.id = lesson_id\n    self.doc_id = doc_id\n    self.tracking_id = tracking_id\n    self.service = parent_service\n</code></pre>"},{"location":"reference/lesson/#engramic.application.teach.lesson.Lesson.generate_questions","title":"<code>generate_questions()</code>  <code>async</code>","text":"<p>Generates educational questions based on document content.</p> <p>Uses an LLM plugin to generate study questions from the document metadata.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Any</code> <p>The generated questions and study actions.</p> Source code in <code>src/engramic/application/teach/lesson.py</code> <pre><code>async def generate_questions(self) -&gt; Any:\n    \"\"\"\n    Generates educational questions based on document content.\n\n    Uses an LLM plugin to generate study questions from the document metadata.\n\n    Returns:\n        dict: The generated questions and study actions.\n    \"\"\"\n    plugin = self.service.teach_generate_questions\n\n    prompt = PromptGenQuestions(input_data={'meta': asdict(self.meta)})\n\n    structured_response = {'study_actions': list[str]}\n\n    ret = plugin['func'].submit(\n        prompt=prompt,\n        images=None,\n        structured_schema=structured_response,\n        args=self.service.host.mock_update_args(plugin),\n    )\n\n    self.service.host.update_mock_data(plugin, ret)\n\n    initial_scan = json.loads(ret[0]['llm_response'])\n\n    return initial_scan\n</code></pre>"},{"location":"reference/lesson/#engramic.application.teach.lesson.Lesson.on_questions_generated","title":"<code>on_questions_generated(future)</code>","text":"<p>Processes generated questions and creates learning prompts.</p> <p>Takes the questions from the generator and submits them as learning prompts. Also adds document-specific questions and notifies about lesson creation.</p> <p>Parameters:</p> Name Type Description Default <code>future</code> <code>Future[Any]</code> <p>Future containing the generated questions.</p> required Source code in <code>src/engramic/application/teach/lesson.py</code> <pre><code>def on_questions_generated(self, future: Future[Any]) -&gt; None:\n    \"\"\"\n    Processes generated questions and creates learning prompts.\n\n    Takes the questions from the generator and submits them as learning prompts.\n    Also adds document-specific questions and notifies about lesson creation.\n\n    Args:\n        future (Future[Any]): Future containing the generated questions.\n    \"\"\"\n    res = future.result()\n    text_prompts = res['study_actions']\n\n    if self.meta.type == self.meta.SourceType.DOCUMENT.value:\n        location = self.meta.locations[0]\n\n        # generate some static question for file discovery.\n        text_prompts.append(f'Tell me about the file {location}')\n\n    async def send_prompt(question: str) -&gt; None:\n        self.service.send_message_async(\n            Service.Topic.SUBMIT_PROMPT,\n            {\n                'prompt_str': question,\n                'parent_id': self.id,\n                'training_mode': True,\n                'is_lesson': True,\n                'tracking_id': self.tracking_id,\n                'repo_ids_filters': self.meta.repo_ids,\n            },\n        )\n\n    for text_prompt in reversed(text_prompts):\n        future = self.service.run_task(send_prompt(text_prompt))\n        future.add_done_callback(self._on_send_prompt_complete)\n\n    async def send_lesson() -&gt; None:\n        self.service.send_message_async(\n            Service.Topic.LESSON_CREATED,\n            {'id': self.id, 'tracking_id': self.tracking_id, 'doc_id': self.doc_id},\n        )\n\n    self.service.run_task(send_lesson())\n</code></pre>"},{"location":"reference/lesson/#engramic.application.teach.lesson.Lesson.run_lesson","title":"<code>run_lesson(meta_in)</code>","text":"<p>Initiates the lesson generation process.</p> <p>Stores the document metadata and starts the question generation task.</p> <p>Parameters:</p> Name Type Description Default <code>meta_in</code> <code>Meta</code> <p>Metadata about the source document.</p> required Source code in <code>src/engramic/application/teach/lesson.py</code> <pre><code>def run_lesson(self, meta_in: Meta) -&gt; None:\n    \"\"\"\n    Initiates the lesson generation process.\n\n    Stores the document metadata and starts the question generation task.\n\n    Args:\n        meta_in (Meta): Metadata about the source document.\n    \"\"\"\n    self.meta = meta_in\n    future = self.service.run_task(self.generate_questions())\n    future.add_done_callback(self.on_questions_generated)\n</code></pre>"},{"location":"reference/message_service/","title":"Message Service","text":"<p>               Bases: <code>BaseMessageService</code></p> <p>A system-level message handling service that provides runtime CPU profiling and metrics reporting.</p> <p>MessageService extends BaseMessageService to handle system-level control messages, enabling dynamic profiling using Python's built-in <code>cProfile</code> module and responding to acknowledgment messages for metrics tracking. It operates by subscribing to control topics and exposing runtime observability features.</p> <p>Attributes:</p> Name Type Description <code>profiler</code> <code>Profile | None</code> <p>An optional CPU profiler used to capture runtime performance data. The profiler is started and stopped in response to specific control messages and saves output to a file named 'profile_output.prof'.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Resets the profiler during asynchronous initialization.</p> <code>start</code> <p>Subscribes to relevant system topics for profiler control and metric acknowledgment.</p> <code>stop</code> <p>Gracefully shuts down the message service.</p> <code>start_profiler</code> <p>dict[Any, Any]) -&gt; None: Initializes and starts the CPU profiler.</p> <code>end_profiler</code> <p>dict[Any, Any]) -&gt; None: Stops the profiler and dumps results to a profile file.</p> <code>on_acknowledge</code> <p>str) -&gt; None: Sends a metric snapshot and service status in response to ACKNOWLEDGE messages.</p> Source code in <code>src/engramic/application/message/message_service.py</code> <pre><code>class MessageService(BaseMessageService):\n    \"\"\"\n    A system-level message handling service that provides runtime CPU profiling and metrics reporting.\n\n    MessageService extends BaseMessageService to handle system-level control messages, enabling\n    dynamic profiling using Python's built-in `cProfile` module and responding to acknowledgment\n    messages for metrics tracking. It operates by subscribing to control topics and exposing\n    runtime observability features.\n\n    Attributes:\n        profiler (cProfile.Profile | None): An optional CPU profiler used to capture runtime performance data.\n            The profiler is started and stopped in response to specific control messages and saves output\n            to a file named 'profile_output.prof'.\n\n    Methods:\n        init_async() -&gt; None:\n            Resets the profiler during asynchronous initialization.\n        start() -&gt; None:\n            Subscribes to relevant system topics for profiler control and metric acknowledgment.\n        stop() -&gt; None:\n            Gracefully shuts down the message service.\n        start_profiler(data: dict[Any, Any]) -&gt; None:\n            Initializes and starts the CPU profiler.\n        end_profiler(data: dict[Any, Any]) -&gt; None:\n            Stops the profiler and dumps results to a profile file.\n        on_acknowledge(message_in: str) -&gt; None:\n            Sends a metric snapshot and service status in response to ACKNOWLEDGE messages.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.profiler: cProfile.Profile | None = None\n\n    def init_async(self) -&gt; None:\n        super().init_async()\n        self.profiler = None\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.START_PROFILER, self.start_profiler)\n        self.subscribe(Service.Topic.END_PROFILER, self.end_profiler)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def start_profiler(self, data: dict[Any, Any]) -&gt; None:\n        if data is not None:\n            del data\n        logging.info('Start Profiler')\n        self.profiler = cProfile.Profile()\n        if self.profiler:\n            self.profiler.enable()\n\n    def end_profiler(self, data: dict[Any, Any]) -&gt; None:\n        if data is not None:\n            del data\n        logging.info('Stop Profiler')\n        if self.profiler:\n            self.profiler.disable()\n            self.profiler.dump_stats('profile_output.prof')\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/progress_service/","title":"Progress Service","text":"<p>               Bases: <code>Service</code></p> <p>Monitors and reports progress of various components through the system pipeline.</p> <p>Tracks the creation and completion status of multiple object types (lessons, prompts, documents, observations, engrams, indices) in parent-child hierarchies, calculates completion percentages, and notifies the system when objects are fully processed.</p> <p>Attributes:</p> Name Type Description <code>progress_array</code> <code>dict[str, ProgressArray]</code> <p>Maps object IDs to their progress tracking data.</p> <code>lookup_array</code> <code>dict[str, str]</code> <p>Quick reverse lookup from child-id to parent-id.</p> <code>tracking_array</code> <code>dict[str, BubbleReturn]</code> <p>Stores progress aggregation data by tracking ID.</p> <p>Methods:</p> Name Description <code>on_lesson_created</code> <p>Handles lesson creation events.</p> <code>on_prompt_created</code> <p>Handles prompt creation events.</p> <code>on_document_created</code> <p>Handles document creation events.</p> <code>on_observation_created</code> <p>Handles observation creation events.</p> <code>on_engrams_created</code> <p>Handles engrams creation events.</p> <code>on_indices_created</code> <p>Handles indices creation events.</p> Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>class ProgressService(Service):\n    \"\"\"\n    Monitors and reports progress of various components through the system pipeline.\n\n    Tracks the creation and completion status of multiple object types (lessons, prompts,\n    documents, observations, engrams, indices) in parent-child hierarchies, calculates\n    completion percentages, and notifies the system when objects are fully processed.\n\n    Attributes:\n        progress_array (dict[str, ProgressArray]): Maps object IDs to their progress tracking data.\n        lookup_array (dict[str, str]): Quick reverse lookup from child-id to parent-id.\n        tracking_array (dict[str, BubbleReturn]): Stores progress aggregation data by tracking ID.\n\n    Methods:\n        on_lesson_created(msg): Handles lesson creation events.\n        on_prompt_created(msg): Handles prompt creation events.\n        on_document_created(msg): Handles document creation events.\n        on_observation_created(msg): Handles observation creation events.\n        on_engrams_created(msg): Handles engrams creation events.\n        on_indices_created(msg): Handles indices creation events.\n    \"\"\"\n\n    @dataclass(slots=True)\n    class ProgressArray:\n        \"\"\"\n        Stores progress tracking data for a single object in the system.\n\n        Attributes:\n            item_type (str): Type of the object being tracked (lesson, prompt, document, etc).\n            tracking_id (str | None): Identifier used to track a processing chain.\n            children_is_complete_array (dict[str, bool]): Maps child IDs to completion status.\n            target_id (str | None): ID of the target object (usually a document).\n        \"\"\"\n\n        item_type: str\n        tracking_id: str | None = None\n        children_is_complete_array: dict[str, bool] = field(default_factory=dict)\n        target_id: str | None = None\n\n    @dataclass(slots=True)\n    class BubbleReturn:\n        \"\"\"\n        Stores aggregated progress data during bubble-up operations.\n\n        Used to track completion metrics as progress propagates up the object hierarchy.\n\n        Attributes:\n            total_indices (int): Total number of indices to be processed.\n            completed_indices (int): Number of indices already processed.\n            is_complete (bool): Whether the entire processing chain is complete.\n            root_node (str): ID of the root node in the processing hierarchy.\n            target_id (str | None): ID of the target object.\n        \"\"\"\n\n        total_indices: int = 0\n        completed_indices: int = 0\n        is_complete: bool = False\n        root_node: str = ''\n        target_id: str | None = None\n\n    # --------------------------------------------------------------------- #\n    # life-cycle                                                            #\n    # --------------------------------------------------------------------- #\n    def __init__(self, host: Host) -&gt; None:\n        \"\"\"\n        Initializes the ProgressService.\n\n        Args:\n            host (Host): The host environment providing access to system resources.\n        \"\"\"\n        super().__init__(host)\n        self.progress_array: dict[str, ProgressService.ProgressArray] = {}\n        # quick reverse lookup: child-id \u2192 parent-id\n        self.lookup_array: dict[str, str] = {}\n        self.tracking_array: dict[str, ProgressService.BubbleReturn] = {}\n\n    def start(self) -&gt; None:\n        \"\"\"\n        Starts the progress service by subscribing to relevant system events.\n\n        Subscribes to creation events for lessons, prompts, documents, observations,\n        engrams, and indices to begin tracking their progress through the system.\n        \"\"\"\n        self.subscribe(Service.Topic.LESSON_CREATED, self.on_lesson_created)\n        self.subscribe(Service.Topic.PROMPT_CREATED, self.on_prompt_created)\n        self.subscribe(Service.Topic.DOCUMENT_CREATED, self.on_document_created)\n        self.subscribe(Service.Topic.OBSERVATION_CREATED, self.on_observation_created)\n        self.subscribe(Service.Topic.ENGRAMS_CREATED, self.on_engrams_created)\n        self.subscribe(Service.Topic.INDICES_CREATED, self.on_indices_created)\n        self.subscribe(Service.Topic.INDICES_INSERTED, self._on_indices_inserted)\n\n        super().start()\n\n    # --------------------------------------------------------------------- #\n    # message handlers                                                      #\n    # --------------------------------------------------------------------- #\n    def on_lesson_created(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the creation of a new lesson in the system.\n\n        Sets up progress tracking for the lesson and connects it to its parent if one exists.\n\n        Args:\n            msg (dict[str, Any]): Message containing lesson creation details.\n        \"\"\"\n        lesson_id = msg['id']\n        parent_id = msg.get('parent_id', '')\n        tracking_id = msg['tracking_id']\n        doc_id = msg['doc_id']\n\n        self.progress_array.setdefault(lesson_id, ProgressService.ProgressArray('lesson'))\n\n        parent_id = None\n        if 'parent_id' in msg:\n            parent_id = msg['parent_id']\n\n        if parent_id:\n            self.progress_array[parent_id].children_is_complete_array[lesson_id] = False\n            self.progress_array[parent_id].tracking_id = tracking_id\n            self.lookup_array[lesson_id] = parent_id\n\n        else:\n            self.progress_array[lesson_id].tracking_id = tracking_id\n            self.progress_array[lesson_id].target_id = doc_id\n            self.send_message_async(\n                Service.Topic.PROGRESS_UPDATED,\n                {\n                    'progress_type': 'lesson',\n                    'id': lesson_id,\n                    'target_id': doc_id,\n                    'percent_complete': 0.05,\n                    'tracking_id': tracking_id,\n                },\n            )\n\n    def on_prompt_created(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the creation of a new prompt in the system.\n\n        Sets up progress tracking for the prompt and connects it to its parent if one exists.\n\n        Args:\n            msg (dict[str, Any): Message containing prompt creation details.\n        \"\"\"\n        prompt_id = msg['id']\n        parent_id = msg.get('parent_id', '')\n        tracking_id = msg['tracking_id']\n\n        self.progress_array.setdefault(prompt_id, ProgressService.ProgressArray('prompt'))\n\n        if parent_id:\n            self.progress_array[parent_id].children_is_complete_array[prompt_id] = False\n            self.progress_array[parent_id].tracking_id = tracking_id\n            self.lookup_array[prompt_id] = parent_id\n        else:\n            self.progress_array[prompt_id].tracking_id = tracking_id\n\n            self.send_message_async(\n                Service.Topic.PROGRESS_UPDATED,\n                {\n                    'progress_type': 'lesson',\n                    'id': prompt_id,\n                    'target_id': prompt_id,\n                    'percent_complete': 0.05,\n                    'tracking_id': tracking_id,\n                },\n            )\n\n    def on_document_created(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the creation of a new document in the system.\n\n        Sets up progress tracking for the document and connects it to its parent if one exists.\n\n        Args:\n            msg (dict[str, Any]): Message containing document creation details.\n        \"\"\"\n        doc_id = msg['id']\n        tracking_id = msg['tracking_id']\n\n        parent_id = None\n        if 'parent_id' in msg:\n            parent_id = msg['parent_id']\n\n        self.progress_array.setdefault(doc_id, ProgressService.ProgressArray('document'))\n\n        if parent_id:\n            self.progress_array[parent_id].children_is_complete_array[doc_id] = False\n            self.progress_array[parent_id].tracking_id = tracking_id\n            self.progress_array[parent_id].target_id = doc_id\n            self.lookup_array[doc_id] = parent_id\n        else:  # an originating node\n            self.progress_array[doc_id].tracking_id = tracking_id\n            self.progress_array[doc_id].target_id = doc_id\n            self.send_message_async(\n                Service.Topic.PROGRESS_UPDATED,\n                {\n                    'progress_type': 'document',\n                    'id': doc_id,\n                    'target_id': doc_id,\n                    'percent_complete': 0.05,\n                    'tracking_id': tracking_id,\n                },\n            )\n\n    def on_observation_created(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the creation of a new observation in the system.\n\n        Sets up progress tracking for the observation and connects it to its parent.\n\n        Args:\n            msg (dict[str, Any]): Message containing observation creation details.\n        \"\"\"\n        obs_id = msg['id']\n        parent_id = msg['parent_id']\n\n        self.progress_array.setdefault(obs_id, ProgressService.ProgressArray('observation'))\n        self.progress_array[parent_id].children_is_complete_array[obs_id] = False\n        self.lookup_array[obs_id] = parent_id\n\n    def on_engrams_created(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the creation of new engrams in the system.\n\n        Sets up progress tracking for multiple engrams and connects them to their parent.\n\n        Args:\n            msg (dict[str, Any]): Message containing engram creation details.\n        \"\"\"\n        parent_id = msg['parent_id']\n        for engram_id in msg['engram_id_array']:\n            self.progress_array.setdefault(engram_id, ProgressService.ProgressArray('engram'))\n            self.progress_array[parent_id].children_is_complete_array[engram_id] = False\n            self.lookup_array[engram_id] = parent_id\n\n    def on_indices_created(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the creation of new indices in the system.\n\n        Sets up progress tracking for multiple indices and connects them to their parent.\n        Updates tracking metrics for the processing chain.\n\n        Args:\n            msg (dict[str, Any]): Message containing index creation details.\n        \"\"\"\n        parent_id = msg['parent_id']\n        tracking_id = msg['tracking_id']\n\n        for index_id in msg['index_id_array']:\n            self.progress_array[parent_id].children_is_complete_array[index_id] = False\n            self.lookup_array[index_id] = parent_id\n\n        if tracking_id not in self.tracking_array:\n            bubble_return = ProgressService.BubbleReturn()\n            self._get_root_node(parent_id, bubble_return)\n            self.tracking_array[tracking_id] = bubble_return\n\n        self.tracking_array[tracking_id].total_indices += len(msg['index_id_array'])\n\n    # ------------------------------------------------------------------ #\n    # propagation logic                                                  #\n    # ------------------------------------------------------------------ #\n    def _on_indices_inserted(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the insertion of indices into the system.\n\n        Marks indices as complete and triggers the bubble-up process to update\n        progress metrics and potentially mark parent objects as complete.\n\n        Args:\n            msg (dict[str, Any]): Message containing index insertion details.\n        \"\"\"\n        parent_id = msg['parent_id']\n        tracking_id = msg['tracking_id']\n\n        for index_id in msg['index_id_array']:\n            self.progress_array[parent_id].children_is_complete_array[index_id] = True\n            # (no need to fill lookup_array here it was done in on_indices_created)\n\n        bubble_return = self.tracking_array[tracking_id]\n\n        # Kick off bubble-up test from the *parent* node\n        self._bubble_up_if_complete(parent_id, bubble_return)\n        originating_object = self.progress_array[bubble_return.root_node]\n\n        self.send_message_async(\n            Service.Topic.PROGRESS_UPDATED,\n            {\n                'progress_type': originating_object.item_type,\n                'id': bubble_return.root_node,\n                'target_id': originating_object.target_id,\n                'percent_complete': bubble_return.completed_indices / bubble_return.total_indices,\n                'tracking_id': tracking_id,\n            },\n        )\n\n        if bubble_return.is_complete:\n            self._cleanup_subtree(bubble_return.root_node)\n            del self.tracking_array[tracking_id]\n\n    def _bubble_up_if_complete(self, node_id: str, bubble_return: ProgressService.BubbleReturn) -&gt; None:\n        \"\"\"\n        Recursively marks nodes as complete and propagates completion status upward.\n\n        Checks if all children of a node are complete, and if so, marks the node as complete\n        in its parent. This process continues up the hierarchy until reaching the root node.\n\n        Args:\n            node_id (str): ID of the node to check for completion.\n            bubble_return (BubbleReturn): Object to track aggregated progress metrics.\n        \"\"\"\n        progress = self.progress_array[node_id]\n\n        if progress.item_type == 'engram':\n            bubble_return.completed_indices += sum(progress.children_is_complete_array.values())\n\n        if not progress.children_is_complete_array:\n            return\n\n        if all(progress.children_is_complete_array.values()):\n            # Notify whoever cares that this node is done\n            parent_id: str | None = self.lookup_array.get(node_id)\n\n            if progress.item_type == 'document':\n                self.send_message_async(Service.Topic.DOCUMENT_INSERTED, {'id': node_id})\n            elif progress.item_type == 'lesson':\n                self.send_message_async(Service.Topic.LESSON_INSERTED, {'id': node_id})\n            elif progress.item_type == 'prompt':\n                self.send_message_async(Service.Topic.PROMPT_INSERTED, {'id': node_id})\n\n            # mark completion in the parent (if any)\n            if parent_id is not None:\n                self.progress_array[parent_id].children_is_complete_array[node_id] = True\n            else:\n                bubble_return.is_complete = True\n                bubble_return.target_id = progress.target_id\n                return\n\n            self._bubble_up_if_complete(parent_id, bubble_return)\n\n        return\n\n    def _get_root_node(self, node_id: str, bubble_return: ProgressService.BubbleReturn) -&gt; None:\n        \"\"\"\n        Recursively finds the root node of a processing hierarchy.\n\n        Args:\n            node_id (str): ID of the node to start the search from.\n            bubble_return (BubbleReturn): Object to store the root node ID once found.\n        \"\"\"\n        parent_id: str | None = self.lookup_array.get(node_id)\n        if parent_id is None:\n            bubble_return.root_node = node_id\n        else:\n            self._get_root_node(parent_id, bubble_return)\n\n    def _cleanup_subtree(self, root_node_id: str) -&gt; None:\n        \"\"\"\n        Recursively removes completed nodes and their children from tracking structures.\n\n        Cleans up memory by removing objects that have completed processing.\n\n        Args:\n            root_node_id (str): ID of the root node of the subtree to clean up.\n        \"\"\"\n        node = self.progress_array.get(root_node_id)\n        if node is None:\n            return\n\n        # Defensive copy because we mutate inside the loop\n        for child_id in list(node.children_is_complete_array):\n            if child_id in self.progress_array:\n                self._cleanup_subtree(child_id)\n\n            self.lookup_array.pop(child_id, None)\n            self.progress_array.pop(child_id, None)\n\n        # Remove the node itself\n        self.lookup_array.pop(root_node_id, None)\n        self.progress_array.pop(root_node_id, None)\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.BubbleReturn","title":"<code>BubbleReturn</code>  <code>dataclass</code>","text":"<p>Stores aggregated progress data during bubble-up operations.</p> <p>Used to track completion metrics as progress propagates up the object hierarchy.</p> <p>Attributes:</p> Name Type Description <code>total_indices</code> <code>int</code> <p>Total number of indices to be processed.</p> <code>completed_indices</code> <code>int</code> <p>Number of indices already processed.</p> <code>is_complete</code> <code>bool</code> <p>Whether the entire processing chain is complete.</p> <code>root_node</code> <code>str</code> <p>ID of the root node in the processing hierarchy.</p> <code>target_id</code> <code>str | None</code> <p>ID of the target object.</p> Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>@dataclass(slots=True)\nclass BubbleReturn:\n    \"\"\"\n    Stores aggregated progress data during bubble-up operations.\n\n    Used to track completion metrics as progress propagates up the object hierarchy.\n\n    Attributes:\n        total_indices (int): Total number of indices to be processed.\n        completed_indices (int): Number of indices already processed.\n        is_complete (bool): Whether the entire processing chain is complete.\n        root_node (str): ID of the root node in the processing hierarchy.\n        target_id (str | None): ID of the target object.\n    \"\"\"\n\n    total_indices: int = 0\n    completed_indices: int = 0\n    is_complete: bool = False\n    root_node: str = ''\n    target_id: str | None = None\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.ProgressArray","title":"<code>ProgressArray</code>  <code>dataclass</code>","text":"<p>Stores progress tracking data for a single object in the system.</p> <p>Attributes:</p> Name Type Description <code>item_type</code> <code>str</code> <p>Type of the object being tracked (lesson, prompt, document, etc).</p> <code>tracking_id</code> <code>str | None</code> <p>Identifier used to track a processing chain.</p> <code>children_is_complete_array</code> <code>dict[str, bool]</code> <p>Maps child IDs to completion status.</p> <code>target_id</code> <code>str | None</code> <p>ID of the target object (usually a document).</p> Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>@dataclass(slots=True)\nclass ProgressArray:\n    \"\"\"\n    Stores progress tracking data for a single object in the system.\n\n    Attributes:\n        item_type (str): Type of the object being tracked (lesson, prompt, document, etc).\n        tracking_id (str | None): Identifier used to track a processing chain.\n        children_is_complete_array (dict[str, bool]): Maps child IDs to completion status.\n        target_id (str | None): ID of the target object (usually a document).\n    \"\"\"\n\n    item_type: str\n    tracking_id: str | None = None\n    children_is_complete_array: dict[str, bool] = field(default_factory=dict)\n    target_id: str | None = None\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.__init__","title":"<code>__init__(host)</code>","text":"<p>Initializes the ProgressService.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>Host</code> <p>The host environment providing access to system resources.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def __init__(self, host: Host) -&gt; None:\n    \"\"\"\n    Initializes the ProgressService.\n\n    Args:\n        host (Host): The host environment providing access to system resources.\n    \"\"\"\n    super().__init__(host)\n    self.progress_array: dict[str, ProgressService.ProgressArray] = {}\n    # quick reverse lookup: child-id \u2192 parent-id\n    self.lookup_array: dict[str, str] = {}\n    self.tracking_array: dict[str, ProgressService.BubbleReturn] = {}\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.on_document_created","title":"<code>on_document_created(msg)</code>","text":"<p>Handles the creation of a new document in the system.</p> <p>Sets up progress tracking for the document and connects it to its parent if one exists.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any]</code> <p>Message containing document creation details.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def on_document_created(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the creation of a new document in the system.\n\n    Sets up progress tracking for the document and connects it to its parent if one exists.\n\n    Args:\n        msg (dict[str, Any]): Message containing document creation details.\n    \"\"\"\n    doc_id = msg['id']\n    tracking_id = msg['tracking_id']\n\n    parent_id = None\n    if 'parent_id' in msg:\n        parent_id = msg['parent_id']\n\n    self.progress_array.setdefault(doc_id, ProgressService.ProgressArray('document'))\n\n    if parent_id:\n        self.progress_array[parent_id].children_is_complete_array[doc_id] = False\n        self.progress_array[parent_id].tracking_id = tracking_id\n        self.progress_array[parent_id].target_id = doc_id\n        self.lookup_array[doc_id] = parent_id\n    else:  # an originating node\n        self.progress_array[doc_id].tracking_id = tracking_id\n        self.progress_array[doc_id].target_id = doc_id\n        self.send_message_async(\n            Service.Topic.PROGRESS_UPDATED,\n            {\n                'progress_type': 'document',\n                'id': doc_id,\n                'target_id': doc_id,\n                'percent_complete': 0.05,\n                'tracking_id': tracking_id,\n            },\n        )\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.on_engrams_created","title":"<code>on_engrams_created(msg)</code>","text":"<p>Handles the creation of new engrams in the system.</p> <p>Sets up progress tracking for multiple engrams and connects them to their parent.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any]</code> <p>Message containing engram creation details.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def on_engrams_created(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the creation of new engrams in the system.\n\n    Sets up progress tracking for multiple engrams and connects them to their parent.\n\n    Args:\n        msg (dict[str, Any]): Message containing engram creation details.\n    \"\"\"\n    parent_id = msg['parent_id']\n    for engram_id in msg['engram_id_array']:\n        self.progress_array.setdefault(engram_id, ProgressService.ProgressArray('engram'))\n        self.progress_array[parent_id].children_is_complete_array[engram_id] = False\n        self.lookup_array[engram_id] = parent_id\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.on_indices_created","title":"<code>on_indices_created(msg)</code>","text":"<p>Handles the creation of new indices in the system.</p> <p>Sets up progress tracking for multiple indices and connects them to their parent. Updates tracking metrics for the processing chain.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any]</code> <p>Message containing index creation details.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def on_indices_created(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the creation of new indices in the system.\n\n    Sets up progress tracking for multiple indices and connects them to their parent.\n    Updates tracking metrics for the processing chain.\n\n    Args:\n        msg (dict[str, Any]): Message containing index creation details.\n    \"\"\"\n    parent_id = msg['parent_id']\n    tracking_id = msg['tracking_id']\n\n    for index_id in msg['index_id_array']:\n        self.progress_array[parent_id].children_is_complete_array[index_id] = False\n        self.lookup_array[index_id] = parent_id\n\n    if tracking_id not in self.tracking_array:\n        bubble_return = ProgressService.BubbleReturn()\n        self._get_root_node(parent_id, bubble_return)\n        self.tracking_array[tracking_id] = bubble_return\n\n    self.tracking_array[tracking_id].total_indices += len(msg['index_id_array'])\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.on_lesson_created","title":"<code>on_lesson_created(msg)</code>","text":"<p>Handles the creation of a new lesson in the system.</p> <p>Sets up progress tracking for the lesson and connects it to its parent if one exists.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any]</code> <p>Message containing lesson creation details.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def on_lesson_created(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the creation of a new lesson in the system.\n\n    Sets up progress tracking for the lesson and connects it to its parent if one exists.\n\n    Args:\n        msg (dict[str, Any]): Message containing lesson creation details.\n    \"\"\"\n    lesson_id = msg['id']\n    parent_id = msg.get('parent_id', '')\n    tracking_id = msg['tracking_id']\n    doc_id = msg['doc_id']\n\n    self.progress_array.setdefault(lesson_id, ProgressService.ProgressArray('lesson'))\n\n    parent_id = None\n    if 'parent_id' in msg:\n        parent_id = msg['parent_id']\n\n    if parent_id:\n        self.progress_array[parent_id].children_is_complete_array[lesson_id] = False\n        self.progress_array[parent_id].tracking_id = tracking_id\n        self.lookup_array[lesson_id] = parent_id\n\n    else:\n        self.progress_array[lesson_id].tracking_id = tracking_id\n        self.progress_array[lesson_id].target_id = doc_id\n        self.send_message_async(\n            Service.Topic.PROGRESS_UPDATED,\n            {\n                'progress_type': 'lesson',\n                'id': lesson_id,\n                'target_id': doc_id,\n                'percent_complete': 0.05,\n                'tracking_id': tracking_id,\n            },\n        )\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.on_observation_created","title":"<code>on_observation_created(msg)</code>","text":"<p>Handles the creation of a new observation in the system.</p> <p>Sets up progress tracking for the observation and connects it to its parent.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any]</code> <p>Message containing observation creation details.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def on_observation_created(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the creation of a new observation in the system.\n\n    Sets up progress tracking for the observation and connects it to its parent.\n\n    Args:\n        msg (dict[str, Any]): Message containing observation creation details.\n    \"\"\"\n    obs_id = msg['id']\n    parent_id = msg['parent_id']\n\n    self.progress_array.setdefault(obs_id, ProgressService.ProgressArray('observation'))\n    self.progress_array[parent_id].children_is_complete_array[obs_id] = False\n    self.lookup_array[obs_id] = parent_id\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.on_prompt_created","title":"<code>on_prompt_created(msg)</code>","text":"<p>Handles the creation of a new prompt in the system.</p> <p>Sets up progress tracking for the prompt and connects it to its parent if one exists.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any</code> <p>Message containing prompt creation details.</p> required Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def on_prompt_created(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the creation of a new prompt in the system.\n\n    Sets up progress tracking for the prompt and connects it to its parent if one exists.\n\n    Args:\n        msg (dict[str, Any): Message containing prompt creation details.\n    \"\"\"\n    prompt_id = msg['id']\n    parent_id = msg.get('parent_id', '')\n    tracking_id = msg['tracking_id']\n\n    self.progress_array.setdefault(prompt_id, ProgressService.ProgressArray('prompt'))\n\n    if parent_id:\n        self.progress_array[parent_id].children_is_complete_array[prompt_id] = False\n        self.progress_array[parent_id].tracking_id = tracking_id\n        self.lookup_array[prompt_id] = parent_id\n    else:\n        self.progress_array[prompt_id].tracking_id = tracking_id\n\n        self.send_message_async(\n            Service.Topic.PROGRESS_UPDATED,\n            {\n                'progress_type': 'lesson',\n                'id': prompt_id,\n                'target_id': prompt_id,\n                'percent_complete': 0.05,\n                'tracking_id': tracking_id,\n            },\n        )\n</code></pre>"},{"location":"reference/progress_service/#engramic.application.progress.progress_service.ProgressService.start","title":"<code>start()</code>","text":"<p>Starts the progress service by subscribing to relevant system events.</p> <p>Subscribes to creation events for lessons, prompts, documents, observations, engrams, and indices to begin tracking their progress through the system.</p> Source code in <code>src/engramic/application/progress/progress_service.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starts the progress service by subscribing to relevant system events.\n\n    Subscribes to creation events for lessons, prompts, documents, observations,\n    engrams, and indices to begin tracking their progress through the system.\n    \"\"\"\n    self.subscribe(Service.Topic.LESSON_CREATED, self.on_lesson_created)\n    self.subscribe(Service.Topic.PROMPT_CREATED, self.on_prompt_created)\n    self.subscribe(Service.Topic.DOCUMENT_CREATED, self.on_document_created)\n    self.subscribe(Service.Topic.OBSERVATION_CREATED, self.on_observation_created)\n    self.subscribe(Service.Topic.ENGRAMS_CREATED, self.on_engrams_created)\n    self.subscribe(Service.Topic.INDICES_CREATED, self.on_indices_created)\n    self.subscribe(Service.Topic.INDICES_INSERTED, self._on_indices_inserted)\n\n    super().start()\n</code></pre>"},{"location":"reference/repo_service/","title":"Repo Service","text":"<p>               Bases: <code>Service</code></p> <p>Service for managing repositories and their document contents.</p> <p>Handles repository discovery, file indexing, and document submission for processing. Maintains in-memory indices of repositories and their files.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manager for system plugins.</p> <code>db_document_plugin</code> <code>Any</code> <p>Plugin for document database operations.</p> <code>document_repository</code> <code>DocumentRepository</code> <p>Repository for document storage and retrieval.</p> <code>repos</code> <code>dict[str, str]</code> <p>Mapping of repository IDs to folder names.</p> <code>file_index</code> <code>dict[str, Any]</code> <p>Index of all files by document ID.</p> <code>file_repos</code> <code>dict[str, Any]</code> <p>Mapping of repository IDs to lists of document IDs.</p> <code>submitted_documents</code> <code>set[str]</code> <p>Set of document IDs that have been submitted for processing.</p> <p>Methods:</p> Name Description <code>start</code> <p>Starts the service and subscribes to relevant topics.</p> <code>submit_ids</code> <p>Submits documents for processing by their IDs.</p> <code>scan_folders</code> <p>Discovers repositories and indexes their files.</p> Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>class RepoService(Service):\n    \"\"\"\n    Service for managing repositories and their document contents.\n\n    Handles repository discovery, file indexing, and document submission for processing.\n    Maintains in-memory indices of repositories and their files.\n\n    Attributes:\n        plugin_manager (PluginManager): Manager for system plugins.\n        db_document_plugin (Any): Plugin for document database operations.\n        document_repository (DocumentRepository): Repository for document storage and retrieval.\n        repos (dict[str, str]): Mapping of repository IDs to folder names.\n        file_index (dict[str, Any]): Index of all files by document ID.\n        file_repos (dict[str, Any]): Mapping of repository IDs to lists of document IDs.\n        submitted_documents (set[str]): Set of document IDs that have been submitted for processing.\n\n    Methods:\n        start() -&gt; None:\n            Starts the service and subscribes to relevant topics.\n        submit_ids(id_array, overwrite) -&gt; None:\n            Submits documents for processing by their IDs.\n        scan_folders() -&gt; None:\n            Discovers repositories and indexes their files.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        \"\"\"\n        Initializes the repository service.\n\n        Args:\n            host (Host): The host system that this service is attached to.\n        \"\"\"\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.document_repository: DocumentRepository = DocumentRepository(self.db_document_plugin)\n        self.repos: dict[str, str] = {}  # memory copy of all folders\n        self.file_index: dict[str, Any] = {}  # memory copy of all files\n        self.file_repos: dict[str, Any] = {}  # memory copy of all files in repos\n        self.submitted_documents: set[str] = set()\n\n    def start(self) -&gt; None:\n        \"\"\"\n        Starts the repository service and subscribes to relevant topics.\n        \"\"\"\n        self.subscribe(Service.Topic.REPO_SUBMIT_IDS, self._on_submit_ids)\n        self.subscribe(Service.Topic.DOCUMENT_COMPLETE, self.on_document_complete)\n        super().start()\n\n    def init_async(self) -&gt; None:\n        \"\"\"\n        Initializes asynchronous components of the service.\n        \"\"\"\n        return super().init_async()\n\n    def _on_submit_ids(self, msg: str) -&gt; None:\n        \"\"\"\n        Handles the REPO_SUBMIT_IDS message.\n\n        Args:\n            msg (str): JSON message containing document IDs to submit.\n        \"\"\"\n        json_msg = json.loads(msg)\n        id_array = json_msg['submit_ids']\n        overwrite = False\n        if 'overwrite' in json_msg:\n            overwrite = json_msg['overwrite']\n        self.submit_ids(id_array, overwrite=overwrite)\n\n    def submit_ids(self, id_array: list[str], *, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Submits documents for processing by their IDs.\n\n        Args:\n            id_array (list[str]): List of document IDs to submit.\n            overwrite (bool): Whether to overwrite existing documents. Defaults to False.\n        \"\"\"\n        for sub_id in id_array:\n            document = self.file_index[sub_id]\n            self.send_message_async(\n                Service.Topic.SUBMIT_DOCUMENT, {'document': asdict(document), 'overwrite': overwrite}\n            )\n            self.submitted_documents.add(document.id)\n\n    def on_document_complete(self, msg: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles the DOCUMENT_COMPLETE message.\n\n        Updates the document index and repository files when a document has been processed.\n\n        Args:\n            msg (dict[str, Any]): Message containing the completed document.\n        \"\"\"\n        document_id = msg['id']\n        document = Document(**msg)\n\n        if document_id in self.submitted_documents:\n            self.submitted_documents.remove(document_id)\n\n            document.is_scanned = True  # Create a Document instance to validate the data\n            self.document_repository.save(document)\n\n            self.file_index[document_id] = document\n\n        if document.repo_id:\n            self.run_task(self.update_repo_files(document.repo_id, [document_id]))\n\n    def _load_repository_id(self, folder_path: Path) -&gt; str:\n        \"\"\"\n        Loads the repository ID from a .repo file.\n\n        Args:\n            folder_path (Path): Path to the repository folder.\n\n        Returns:\n            str: The repository ID.\n\n        Raises:\n            RuntimeError: If the .repo file is missing or invalid.\n            TypeError: If the repository ID is not a string.\n        \"\"\"\n        repo_file = folder_path / '.repo'\n        if not repo_file.is_file():\n            error = f\"Repository config file '.repo' not found in folder '{folder_path}'.\"\n            raise RuntimeError(error)\n        with repo_file.open('rb') as f:\n            data = tomli.load(f)\n        try:\n            repository_id = data['repository']['id']\n        except KeyError as err:\n            error = f\"Missing 'repository.id' entry in .repo file at '{repo_file}'.\"\n            raise RuntimeError(error) from err\n        if not isinstance(repository_id, str):\n            error = f\"'repository.id' must be a string in '{repo_file}'.\"\n            raise TypeError\n        return repository_id\n\n    def _discover_repos(self, repo_root: Path) -&gt; None:\n        \"\"\"\n        Discovers repositories in the specified root directory.\n\n        Args:\n            repo_root (Path): Root directory containing repositories.\n\n        Raises:\n            ValueError: If a repository is named 'null'.\n        \"\"\"\n        for name in os.listdir(repo_root):\n            folder_path = repo_root / name\n            if folder_path.is_dir():\n                if name == 'null':\n                    error = \"Folder name 'null' is reserved and cannot be used as a repository name.\"\n                    logging.error(error)\n                    raise ValueError(error)\n                try:\n                    repo_id = self._load_repository_id(folder_path)\n                    self.repos[repo_id] = name\n                except (FileNotFoundError, PermissionError, ValueError, OSError) as e:\n                    info = f\"Skipping '{name}': {e}\"\n                    logging.info(info)\n\n    async def update_repo_files(self, repo_id: str, update_ids: list[str] | None = None) -&gt; None:\n        \"\"\"\n        Updates the list of files for a repository.\n\n        Args:\n            repo_id (str): ID of the repository to update.\n            update_ids (list[str] | None): List of document IDs to update. If None, updates all files.\n        \"\"\"\n        document_dicts = []\n\n        folder = self.repos[repo_id]\n\n        update_list = self.file_repos[repo_id] if update_ids is None else update_ids\n\n        document_dicts = [asdict(self.file_index[document_id]) for document_id in update_list]\n\n        self.send_message_async(\n            Service.Topic.REPO_FILES,\n            {'repo': folder, 'repo_id': repo_id, 'files': document_dicts},\n        )\n\n    def scan_folders(self) -&gt; None:\n        \"\"\"\n        Scans repository folders and indexes their files.\n\n        Discovers repositories, indexes their files, and sends messages with the repository information.\n\n        Raises:\n            RuntimeError: If the REPO_ROOT environment variable is not set.\n        \"\"\"\n        repo_root = os.getenv('REPO_ROOT')\n        if repo_root is None:\n            error = \"Environment variable 'REPO_ROOT' is not set.\"\n            raise RuntimeError(error)\n\n        expanded_repo_root = Path(repo_root).expanduser()\n\n        self._discover_repos(expanded_repo_root)\n\n        async def send_message() -&gt; None:\n            self.send_message_async(Service.Topic.REPO_FOLDERS, {'repo_folders': self.repos})\n\n        self.run_task(send_message())\n\n        for repo_id in self.repos:\n            folder = self.repos[repo_id]\n            document_ids = []\n            # Recursively walk through all files in repo\n            for root, dirs, files in os.walk(expanded_repo_root / folder):\n                del dirs\n                for file in files:\n                    if file.startswith('.'):\n                        continue  # Skip hidden files\n                    file_path = Path(root) / file\n                    relative_path = file_path.relative_to(expanded_repo_root / folder)\n                    relative_dir = str(relative_path.parent) if relative_path.parent != Path('.') else ''\n                    doc = Document(\n                        root_directory=Document.Root.DATA.value,\n                        file_path=folder + relative_dir,\n                        file_name=file,\n                        repo_id=repo_id,\n                        tracking_id=str(uuid.uuid4()),\n                    )\n\n                    # Check to see if the document has been loaded before.\n                    fetched_doc: dict[str, Any] = self.document_repository.load(doc.id)\n\n                    # If it has been loaded, add that one to the file_index.\n                    if len(fetched_doc['document']) != 0:\n                        doc = Document(**fetched_doc['document'][0])\n\n                    document_ids.append(doc.id)\n                    self.file_index[doc.id] = doc\n\n            self.file_repos[repo_id] = document_ids\n            future = self.run_task(self.update_repo_files(repo_id))\n            future.add_done_callback(self._on_update_repo_files_complete)\n\n    def _on_update_repo_files_complete(self, ret: Future[Any]) -&gt; None:\n        \"\"\"\n        Callback when the update_repo_files task completes.\n\n        Args:\n            ret (Future[Any]): Future object representing the completed task.\n        \"\"\"\n        ret.result()\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.__init__","title":"<code>__init__(host)</code>","text":"<p>Initializes the repository service.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>Host</code> <p>The host system that this service is attached to.</p> required Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>def __init__(self, host: Host) -&gt; None:\n    \"\"\"\n    Initializes the repository service.\n\n    Args:\n        host (Host): The host system that this service is attached to.\n    \"\"\"\n    super().__init__(host)\n    self.plugin_manager: PluginManager = host.plugin_manager\n    self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n    self.document_repository: DocumentRepository = DocumentRepository(self.db_document_plugin)\n    self.repos: dict[str, str] = {}  # memory copy of all folders\n    self.file_index: dict[str, Any] = {}  # memory copy of all files\n    self.file_repos: dict[str, Any] = {}  # memory copy of all files in repos\n    self.submitted_documents: set[str] = set()\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.init_async","title":"<code>init_async()</code>","text":"<p>Initializes asynchronous components of the service.</p> Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>def init_async(self) -&gt; None:\n    \"\"\"\n    Initializes asynchronous components of the service.\n    \"\"\"\n    return super().init_async()\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.on_document_complete","title":"<code>on_document_complete(msg)</code>","text":"<p>Handles the DOCUMENT_COMPLETE message.</p> <p>Updates the document index and repository files when a document has been processed.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[str, Any]</code> <p>Message containing the completed document.</p> required Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>def on_document_complete(self, msg: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles the DOCUMENT_COMPLETE message.\n\n    Updates the document index and repository files when a document has been processed.\n\n    Args:\n        msg (dict[str, Any]): Message containing the completed document.\n    \"\"\"\n    document_id = msg['id']\n    document = Document(**msg)\n\n    if document_id in self.submitted_documents:\n        self.submitted_documents.remove(document_id)\n\n        document.is_scanned = True  # Create a Document instance to validate the data\n        self.document_repository.save(document)\n\n        self.file_index[document_id] = document\n\n    if document.repo_id:\n        self.run_task(self.update_repo_files(document.repo_id, [document_id]))\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.scan_folders","title":"<code>scan_folders()</code>","text":"<p>Scans repository folders and indexes their files.</p> <p>Discovers repositories, indexes their files, and sends messages with the repository information.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the REPO_ROOT environment variable is not set.</p> Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>def scan_folders(self) -&gt; None:\n    \"\"\"\n    Scans repository folders and indexes their files.\n\n    Discovers repositories, indexes their files, and sends messages with the repository information.\n\n    Raises:\n        RuntimeError: If the REPO_ROOT environment variable is not set.\n    \"\"\"\n    repo_root = os.getenv('REPO_ROOT')\n    if repo_root is None:\n        error = \"Environment variable 'REPO_ROOT' is not set.\"\n        raise RuntimeError(error)\n\n    expanded_repo_root = Path(repo_root).expanduser()\n\n    self._discover_repos(expanded_repo_root)\n\n    async def send_message() -&gt; None:\n        self.send_message_async(Service.Topic.REPO_FOLDERS, {'repo_folders': self.repos})\n\n    self.run_task(send_message())\n\n    for repo_id in self.repos:\n        folder = self.repos[repo_id]\n        document_ids = []\n        # Recursively walk through all files in repo\n        for root, dirs, files in os.walk(expanded_repo_root / folder):\n            del dirs\n            for file in files:\n                if file.startswith('.'):\n                    continue  # Skip hidden files\n                file_path = Path(root) / file\n                relative_path = file_path.relative_to(expanded_repo_root / folder)\n                relative_dir = str(relative_path.parent) if relative_path.parent != Path('.') else ''\n                doc = Document(\n                    root_directory=Document.Root.DATA.value,\n                    file_path=folder + relative_dir,\n                    file_name=file,\n                    repo_id=repo_id,\n                    tracking_id=str(uuid.uuid4()),\n                )\n\n                # Check to see if the document has been loaded before.\n                fetched_doc: dict[str, Any] = self.document_repository.load(doc.id)\n\n                # If it has been loaded, add that one to the file_index.\n                if len(fetched_doc['document']) != 0:\n                    doc = Document(**fetched_doc['document'][0])\n\n                document_ids.append(doc.id)\n                self.file_index[doc.id] = doc\n\n        self.file_repos[repo_id] = document_ids\n        future = self.run_task(self.update_repo_files(repo_id))\n        future.add_done_callback(self._on_update_repo_files_complete)\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.start","title":"<code>start()</code>","text":"<p>Starts the repository service and subscribes to relevant topics.</p> Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starts the repository service and subscribes to relevant topics.\n    \"\"\"\n    self.subscribe(Service.Topic.REPO_SUBMIT_IDS, self._on_submit_ids)\n    self.subscribe(Service.Topic.DOCUMENT_COMPLETE, self.on_document_complete)\n    super().start()\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.submit_ids","title":"<code>submit_ids(id_array, *, overwrite=False)</code>","text":"<p>Submits documents for processing by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>id_array</code> <code>list[str]</code> <p>List of document IDs to submit.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing documents. Defaults to False.</p> <code>False</code> Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>def submit_ids(self, id_array: list[str], *, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Submits documents for processing by their IDs.\n\n    Args:\n        id_array (list[str]): List of document IDs to submit.\n        overwrite (bool): Whether to overwrite existing documents. Defaults to False.\n    \"\"\"\n    for sub_id in id_array:\n        document = self.file_index[sub_id]\n        self.send_message_async(\n            Service.Topic.SUBMIT_DOCUMENT, {'document': asdict(document), 'overwrite': overwrite}\n        )\n        self.submitted_documents.add(document.id)\n</code></pre>"},{"location":"reference/repo_service/#engramic.application.repo.repo_service.RepoService.update_repo_files","title":"<code>update_repo_files(repo_id, update_ids=None)</code>  <code>async</code>","text":"<p>Updates the list of files for a repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>ID of the repository to update.</p> required <code>update_ids</code> <code>list[str] | None</code> <p>List of document IDs to update. If None, updates all files.</p> <code>None</code> Source code in <code>src/engramic/application/repo/repo_service.py</code> <pre><code>async def update_repo_files(self, repo_id: str, update_ids: list[str] | None = None) -&gt; None:\n    \"\"\"\n    Updates the list of files for a repository.\n\n    Args:\n        repo_id (str): ID of the repository to update.\n        update_ids (list[str] | None): List of document IDs to update. If None, updates all files.\n    \"\"\"\n    document_dicts = []\n\n    folder = self.repos[repo_id]\n\n    update_list = self.file_repos[repo_id] if update_ids is None else update_ids\n\n    document_dicts = [asdict(self.file_index[document_id]) for document_id in update_list]\n\n    self.send_message_async(\n        Service.Topic.REPO_FILES,\n        {'repo': folder, 'repo_id': repo_id, 'files': document_dicts},\n    )\n</code></pre>"},{"location":"reference/response_service/","title":"Response Service","text":"<p>               Bases: <code>Service</code></p> <p>Orchestrates AI response generation by integrating retrieval results, historical context, and prompt engineering.</p> <p>Coordinates plugin-managed large language models (LLMs), websockets for streaming responses, and tracks metrics throughout the response generation pipeline.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Provides access to LLM and DB plugins.</p> <code>web_socket_manager</code> <code>WebsocketManager</code> <p>Manages live streaming over websocket.</p> <code>db_document_plugin</code> <code>dict</code> <p>Document store plugin interface.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Access point for loading engrams.</p> <code>llm_main</code> <code>dict</code> <p>Plugin for executing the main LLM-based response generation.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks internal response metrics.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes to service topics and initializes websocket manager.</p> <code>stop</code> <p>Shuts down the websocket manager and stops the service.</p> <code>init_async</code> <p>Initializes the DB plugin connection asynchronously.</p> <code>on_retrieve_complete</code> <p>dict[str, Any]) -&gt; None: Processes retrieval results and initiates engram and history fetch.</p> <code>_fetch_history</code> <p>Prompt) -&gt; dict[str, Any]: Asynchronously fetches historical conversation context.</p> <code>_fetch_retrieval</code> <p>Prompt, source_id: str, analysis: PromptAnalysis, retrieve_result: RetrieveResult) -&gt; dict[str, Any]: Loads engrams using retrieve result.</p> <code>on_fetch_data_complete</code> <p>Future[Any]) -&gt; None: Launches main prompt generation after history and engrams are loaded.</p> <code>main_prompt</code> <p>Prompt, source_id: str, analysis: PromptAnalysis, engram_array: list[Engram], retrieve_result: RetrieveResult, history_array: dict[str, Any]) -&gt; Response: Constructs and submits the main prompt to the LLM plugin.</p> <code>on_main_prompt_complete</code> <p>Future[Any]) -&gt; None: Sends generated response and updates metrics.</p> <code>on_acknowledge</code> <p>str) -&gt; None: Sends current metrics snapshot to monitoring topics.</p> Source code in <code>src/engramic/application/response/response_service.py</code> <pre><code>class ResponseService(Service):\n    \"\"\"\n    Orchestrates AI response generation by integrating retrieval results, historical context, and prompt engineering.\n\n    Coordinates plugin-managed large language models (LLMs), websockets for streaming responses,\n    and tracks metrics throughout the response generation pipeline.\n\n    Attributes:\n        plugin_manager (PluginManager): Provides access to LLM and DB plugins.\n        web_socket_manager (WebsocketManager): Manages live streaming over websocket.\n        db_document_plugin (dict): Document store plugin interface.\n        engram_repository (EngramRepository): Access point for loading engrams.\n        llm_main (dict): Plugin for executing the main LLM-based response generation.\n        metrics_tracker (MetricsTracker): Tracks internal response metrics.\n\n    Methods:\n        start() -&gt; None:\n            Subscribes to service topics and initializes websocket manager.\n        stop() -&gt; None:\n            Shuts down the websocket manager and stops the service.\n        init_async() -&gt; None:\n            Initializes the DB plugin connection asynchronously.\n        on_retrieve_complete(retrieve_result_in: dict[str, Any]) -&gt; None:\n            Processes retrieval results and initiates engram and history fetch.\n        _fetch_history(prompt: Prompt) -&gt; dict[str, Any]:\n            Asynchronously fetches historical conversation context.\n        _fetch_retrieval(prompt: Prompt, source_id: str, analysis: PromptAnalysis, retrieve_result: RetrieveResult) -&gt; dict[str, Any]:\n            Loads engrams using retrieve result.\n        on_fetch_data_complete(fut: Future[Any]) -&gt; None:\n            Launches main prompt generation after history and engrams are loaded.\n        main_prompt(prompt_in: Prompt, source_id: str, analysis: PromptAnalysis, engram_array: list[Engram], retrieve_result: RetrieveResult, history_array: dict[str, Any]) -&gt; Response:\n            Constructs and submits the main prompt to the LLM plugin.\n        on_main_prompt_complete(fut: Future[Any]) -&gt; None:\n            Sends generated response and updates metrics.\n        on_acknowledge(message_in: str) -&gt; None:\n            Sends current metrics snapshot to monitoring topics.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.web_socket_manager: WebsocketManager = WebsocketManager(host)\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.llm_main = self.plugin_manager.get_plugin('llm', 'response_main')\n        self.metrics_tracker: MetricsTracker[ResponseMetric] = MetricsTracker[ResponseMetric]()\n        ##\n        # Many methods are not ready to be until their async component is running.\n        # Do not call async context methods in the constructor.\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.RETRIEVE_COMPLETE, self.on_retrieve_complete)\n        self.web_socket_manager.init_async()\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await self.web_socket_manager.shutdown()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_retrieve_complete(self, retrieve_result_in: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, retrieve_result_in)\n\n        prompt = Prompt(**retrieve_result_in['prompt'])\n        prompt_analysis = PromptAnalysis(**retrieve_result_in['analysis'])\n        retrieve_result = RetrieveResult(**retrieve_result_in['retrieve_response'])\n        source_id = retrieve_result.source_id\n        self.metrics_tracker.increment(ResponseMetric.RETRIEVES_RECIEVED)\n        fetch_engrams_task = self.run_tasks([\n            self._fetch_retrieval(\n                prompt=prompt, source_id=source_id, analysis=prompt_analysis, retrieve_result=retrieve_result\n            ),\n            self._fetch_history(prompt),\n        ])\n        fetch_engrams_task.add_done_callback(self.on_fetch_data_complete)\n\n    \"\"\"\n    ### Fetch History &amp; Engram\n\n    Fetch engrams based on the IDs provided by the retrieve service.\n    \"\"\"\n\n    async def _fetch_history(self, prompt: Prompt) -&gt; dict[str, Any]:\n        plugin = self.db_document_plugin\n        args = plugin['args']\n        args['history'] = 1\n        args['repo_ids_filters'] = prompt.repo_ids_filters\n\n        ret_val = await asyncio.to_thread(plugin['func'].fetch, table=DB.DBTables.HISTORY, ids=[], args=args)\n        history: dict[str, Any] = ret_val[0]\n        return history\n\n    async def _fetch_retrieval(\n        self, prompt: Prompt, source_id: str, analysis: PromptAnalysis, retrieve_result: RetrieveResult\n    ) -&gt; dict[str, Any]:\n        engram_array: list[Engram] = await asyncio.to_thread(\n            self.engram_repository.load_batch_retrieve_result, retrieve_result\n        )\n\n        # assembled main_prompt, render engrams.\n        return {\n            'prompt': prompt,\n            'source_id': source_id,\n            'analysis': analysis,\n            'retrieve_result': retrieve_result,\n            'engram_array': engram_array,\n        }\n\n    def on_fetch_data_complete(self, fut: Future[Any]) -&gt; None:\n        exc = fut.exception()\n        if exc is not None:\n            raise exc\n        result = fut.result()\n        retrieval = result['_fetch_retrieval'][0]\n        history = result['_fetch_history'][0]\n\n        main_prompt_task = self.run_task(\n            self.main_prompt(\n                retrieval['prompt'],\n                retrieval['source_id'],\n                retrieval['analysis'],\n                retrieval['engram_array'],\n                retrieval['retrieve_result'],\n                history,\n            )\n        )\n        main_prompt_task.add_done_callback(self.on_main_prompt_complete)\n\n    \"\"\"\n    ### Main Prompt\n\n    Combine the previous stages to generate the response.\n    \"\"\"\n\n    async def main_prompt(\n        self,\n        prompt_in: Prompt,\n        source_id: str,\n        analysis: PromptAnalysis,\n        engram_array: list[Engram],\n        retrieve_result: RetrieveResult,\n        history_array: dict[str, Any],\n    ) -&gt; Response:\n        self.metrics_tracker.increment(ResponseMetric.ENGRAMS_FETCHED, len(engram_array))\n\n        engram_dict_list = [asdict(engram) for engram in engram_array]\n\n        # build main prompt here\n        prompt = PromptMainPrompt(\n            prompt_str=prompt_in.prompt_str,\n            is_lesson=prompt_in.is_lesson,\n            training_mode=prompt_in.training_mode,\n            repo_ids_filters=prompt_in.repo_ids_filters,\n            input_data={\n                'engram_list': engram_dict_list,\n                'history': history_array['history'],\n                'working_memory': retrieve_result.conversation_direction,\n                'analysis': retrieve_result.analysis,\n            },\n        )\n\n        plugin = self.llm_main\n        args = self.host.mock_update_args(plugin)\n\n        if prompt_in.is_lesson:\n            response = await asyncio.to_thread(\n                plugin['func'].submit, prompt=prompt, args=args, images=None, structured_schema=None\n            )\n        else:\n            response = await asyncio.to_thread(\n                plugin['func'].submit_streaming,\n                prompt=prompt,\n                websocket_manager=self.web_socket_manager,\n                args=args,\n            )\n\n        if __debug__:\n            main_prompt = prompt.render_prompt()\n            self.send_message_async(\n                Service.Topic.DEBUG_MAIN_PROMPT_INPUT, {'main_prompt': main_prompt, 'ask_id': retrieve_result.ask_id}\n            )\n\n        self.host.update_mock_data(self.llm_main, response)\n\n        model = ''\n        if plugin['args'].get('model'):\n            model = plugin['args']['model']\n\n        response = response[0]['llm_response'].replace('$', 'USD ').replace('&lt;context&gt;', '').replace('&lt;/context&gt;', '')\n\n        response_inst = Response(str(uuid.uuid4()), source_id, response, retrieve_result, prompt_in, analysis, model)\n\n        return response_inst\n\n    def on_main_prompt_complete(self, fut: Future[Any]) -&gt; None:\n        result = fut.result()\n        self.metrics_tracker.increment(ResponseMetric.MAIN_PROMPTS_RUN)\n\n        self.send_message_async(Service.Topic.MAIN_PROMPT_COMPLETE, asdict(result))\n\n        if __debug__:\n            self.host.update_mock_data_output(self, asdict(result))\n\n    \"\"\"\n    ### Ack\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/retrieve_service/","title":"Retrieve Service","text":"<p>               Bases: <code>Service</code></p> <p>Manages semantic prompt retrieval and indexing by coordinating between vector/document databases, tracking metrics, and responding to system events.</p> <p>This service is responsible for receiving prompt submissions, retrieving relevant information using vector similarity, and handling the indexing and metadata enrichment process. It interfaces with plugin-managed databases and provides observability through metrics tracking.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Access point for system plugins, including vector and document DBs.</p> <code>vector_db_plugin</code> <code>dict</code> <p>Plugin used for vector database operations (e.g., semantic search).</p> <code>db_plugin</code> <code>dict</code> <p>Plugin for interacting with the document database.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Collects and resets retrieval-related metrics for monitoring.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Handles Meta object persistence and transformation.</p> <code>repo_folders</code> <code>dict[str, Any]</code> <p>Dictionary containing repository folder information.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Initializes database connections and plugin setup asynchronously.</p> <code>start</code> <p>Subscribes to system topics for prompt processing and indexing lifecycle.</p> <code>stop</code> <p>Cleans up the service and halts processing.</p> <code>submit</code> <p>Prompt): Begins the retrieval process and logs submission metrics.</p> <code>on_submit_prompt</code> <p>dict[Any, Any]): Processes a prompt message and submits for processing.</p> <code>_on_repo_folders</code> <p>dict[str, Any]): Updates repository folder information.</p> <code>on_indices_complete</code> <p>dict): Converts index payload into Index objects and queues for insertion.</p> <code>_insert_engram_vector</code> <p>list[Index], engram_id: str, repo_ids: str, tracking_id: str): Asynchronously inserts semantic indices into vector DB with repository filters.</p> <code>on_meta_complete</code> <p>dict): Loads and inserts metadata summary into the vector DB.</p> <code>insert_meta_vector</code> <p>Meta): Runs metadata vector insertion in a background thread.</p> <code>on_acknowledge</code> <p>str): Emits service metrics to the status channel and resets the tracker.</p> Source code in <code>src/engramic/application/retrieve/retrieve_service.py</code> <pre><code>class RetrieveService(Service):\n    \"\"\"\n    Manages semantic prompt retrieval and indexing by coordinating between vector/document databases,\n    tracking metrics, and responding to system events.\n\n    This service is responsible for receiving prompt submissions, retrieving relevant information using\n    vector similarity, and handling the indexing and metadata enrichment process. It interfaces with\n    plugin-managed databases and provides observability through metrics tracking.\n\n    Attributes:\n        plugin_manager (PluginManager): Access point for system plugins, including vector and document DBs.\n        vector_db_plugin (dict): Plugin used for vector database operations (e.g., semantic search).\n        db_plugin (dict): Plugin for interacting with the document database.\n        metrics_tracker (MetricsTracker): Collects and resets retrieval-related metrics for monitoring.\n        meta_repository (MetaRepository): Handles Meta object persistence and transformation.\n        repo_folders (dict[str, Any]): Dictionary containing repository folder information.\n\n    Methods:\n        init_async(): Initializes database connections and plugin setup asynchronously.\n        start(): Subscribes to system topics for prompt processing and indexing lifecycle.\n        stop(): Cleans up the service and halts processing.\n\n        submit(prompt: Prompt): Begins the retrieval process and logs submission metrics.\n        on_submit_prompt(msg: dict[Any, Any]): Processes a prompt message and submits for processing.\n        _on_repo_folders(msg: dict[str, Any]): Updates repository folder information.\n\n        on_indices_complete(index_message: dict): Converts index payload into Index objects and queues for insertion.\n        _insert_engram_vector(index_list: list[Index], engram_id: str, repo_ids: str, tracking_id: str):\n            Asynchronously inserts semantic indices into vector DB with repository filters.\n\n        on_meta_complete(meta_dict: dict): Loads and inserts metadata summary into the vector DB.\n        insert_meta_vector(meta: Meta): Runs metadata vector insertion in a background thread.\n\n        on_acknowledge(message_in: str): Emits service metrics to the status channel and resets the tracker.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.vector_db_plugin = host.plugin_manager.get_plugin('vector_db', 'db')\n        self.db_plugin = host.plugin_manager.get_plugin('db', 'document')\n        self.metrics_tracker: MetricsTracker[RetrieveMetric] = MetricsTracker[RetrieveMetric]()\n        self.meta_repository: MetaRepository = MetaRepository(self.db_plugin)\n        self.repo_folders: dict[str, Any] = {}\n\n    def init_async(self) -&gt; None:\n        self.db_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.SUBMIT_PROMPT, self.on_submit_prompt)\n        self.subscribe(Service.Topic.INDICES_COMPLETE, self.on_indices_complete)\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        self.subscribe(Service.Topic.REPO_FOLDERS, self._on_repo_folders)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def _on_repo_folders(self, msg: dict[str, Any]) -&gt; None:\n        self.repo_folders = msg['repo_folders']\n\n    # when called from monitor service\n    def on_submit_prompt(self, msg: dict[Any, Any]) -&gt; None:\n        self.submit(Prompt(**msg))\n\n    # when used from main\n    def submit(self, prompt: Prompt) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, asdict(prompt))\n\n        self.metrics_tracker.increment(RetrieveMetric.PROMPTS_SUBMITTED)\n        retrieval = Ask(str(uuid.uuid4()), prompt, self.plugin_manager, self.metrics_tracker, self.db_plugin, self)\n        retrieval.get_sources()\n\n        async def send_message() -&gt; None:\n            msg = {'id': prompt.prompt_id, 'parent_id': prompt.parent_id, 'tracking_id': prompt.tracking_id}\n            self.send_message_async(Service.Topic.PROMPT_CREATED, msg)\n\n        self.run_task(send_message())\n\n    def on_indices_complete(self, index_message: dict[str, Any]) -&gt; None:\n        raw_index: list[dict[str, Any]] = index_message['index']\n        engram_id: str = index_message['engram_id']\n        tracking_id: str = index_message['tracking_id']\n        repo_ids: str = index_message['repo_ids']\n        index_list: list[Index] = [Index(**item) for item in raw_index]\n        self.run_task(self._insert_engram_vector(index_list, engram_id, repo_ids, tracking_id))\n\n    async def _insert_engram_vector(\n        self, index_list: list[Index], engram_id: str, repo_ids: str, tracking_id: str\n    ) -&gt; None:\n        plugin = self.vector_db_plugin\n        self.vector_db_plugin['func'].insert(\n            collection_name='main', index_list=index_list, obj_id=engram_id, args=plugin['args'], filters=repo_ids\n        )\n\n        index_id_array = [index.id for index in index_list]\n\n        self.send_message_async(\n            Service.Topic.INDICES_INSERTED,\n            {'parent_id': engram_id, 'index_id_array': index_id_array, 'tracking_id': tracking_id},\n        )\n\n        self.metrics_tracker.increment(RetrieveMetric.EMBEDDINGS_ADDED_TO_VECTOR)\n\n    def on_meta_complete(self, meta_dict: dict[str, Any]) -&gt; None:\n        meta = self.meta_repository.load(meta_dict)\n        self.run_task(self.insert_meta_vector(meta))\n        self.metrics_tracker.increment(RetrieveMetric.META_ADDED_TO_VECTOR)\n\n    async def insert_meta_vector(self, meta: Meta) -&gt; None:\n        plugin = self.vector_db_plugin\n        await asyncio.to_thread(\n            self.vector_db_plugin['func'].insert,\n            collection_name='meta',\n            index_list=[meta.summary_full],\n            obj_id=meta.id,\n            filters=meta.repo_ids,\n            args=plugin['args'],\n        )\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/scan/","title":"Scan","text":"<p>               Bases: <code>Media</code></p> <p>Coordinates the semantic analysis of a submitted document, converting it into engrams and metadata by orchestrating image extraction, LLM-driven summarization, and structured information parsing.</p> <p>This class performs the following operations: - Loads a PDF document and converts each page into Base64-encoded images. - Generates an initial summary using a few preview pages. - Scans each page with an LLM plugin to extract semantic content. - Parses scan results into Engrams and assembles a full summary. - Constructs a Meta object and emits an observation to the system.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the scan session.</p> <code>service</code> <code>SenseService</code> <p>Reference to the parent service orchestrating the scan.</p> <code>page_images</code> <code>list[str]</code> <p>Base64-encoded representations of PDF pages.</p> <code>sense_initial_summary</code> <code>Plugin</code> <p>Plugin used to generate an initial summary from early pages.</p> <code>repo_id</code> <code>str</code> <p>Repository identifier for the document being scanned.</p> <code>repo_ids</code> <code>list[str] | None</code> <p>List of repository IDs or None if no repository is specified.</p> <code>document</code> <code>Document | None</code> <p>The document being processed.</p> <code>tracking_id</code> <code>str</code> <p>ID for tracking the scan process.</p> Constants <p>DPI (int): Resolution used for image extraction. DPI_DIVISOR (int): Used to calculate zoom level for rendering. TEST_PAGE_LIMIT (int): Max number of pages scanned during test runs. MAX_CHUNK_SIZE (int): Max text length before recursive engram chunking. SHORT_SUMMARY_PAGE_COUNT (int): Number of pages used to generate initial summary. MAX_DEPTH (int): Maximum recursion depth for engram processing. SECTION, H1, H3 (int): Enum-like values to manage tag-based engram splitting depth.</p> <p>Methods:</p> Name Description <code>parse_media_resource</code> <p>Loads and validates a PDF, then initiates conversion.</p> <code>_convert_pages_to_images</code> <p>Converts PDF pages to images asynchronously.</p> <code>_page_to_image</code> <p>Converts and encodes a single PDF page to Base64.</p> <code>_on_pages_converted</code> <p>Handles post-conversion logic, triggering initial summary.</p> <code>_generate_short_summary</code> <p>Sends preview pages to the LLM for initial semantic scan.</p> <code>_on_short_summary</code> <p>Kicks off page-by-page scanning after initial summary.</p> <code>_scan_page</code> <p>Sends a single page to the LLM plugin and extracts structured data.</p> <code>_on_pages_scanned</code> <p>Extracts structured context and initiates full summary.</p> <code>_process_engrams</code> <p>Recursively splits and constructs Engrams from HTML text.</p> <code>_generate_full_summary</code> <p>Uses full document content to generate final semantic summary.</p> <code>_on_generate_full_summary</code> <p>Wraps summary and Engrams into a Meta and emits final Observation.</p> Source code in <code>src/engramic/application/sense/scan.py</code> <pre><code>class Scan(Media):\n    \"\"\"\n    Coordinates the semantic analysis of a submitted document, converting it into engrams and metadata\n    by orchestrating image extraction, LLM-driven summarization, and structured information parsing.\n\n    This class performs the following operations:\n    - Loads a PDF document and converts each page into Base64-encoded images.\n    - Generates an initial summary using a few preview pages.\n    - Scans each page with an LLM plugin to extract semantic content.\n    - Parses scan results into Engrams and assembles a full summary.\n    - Constructs a Meta object and emits an observation to the system.\n\n    Attributes:\n        id (str): Unique identifier for the scan session.\n        service (SenseService): Reference to the parent service orchestrating the scan.\n        page_images (list[str]): Base64-encoded representations of PDF pages.\n        sense_initial_summary (Plugin): Plugin used to generate an initial summary from early pages.\n        repo_id (str): Repository identifier for the document being scanned.\n        repo_ids (list[str] | None): List of repository IDs or None if no repository is specified.\n        document (Document | None): The document being processed.\n        tracking_id (str): ID for tracking the scan process.\n\n    Constants:\n        DPI (int): Resolution used for image extraction.\n        DPI_DIVISOR (int): Used to calculate zoom level for rendering.\n        TEST_PAGE_LIMIT (int): Max number of pages scanned during test runs.\n        MAX_CHUNK_SIZE (int): Max text length before recursive engram chunking.\n        SHORT_SUMMARY_PAGE_COUNT (int): Number of pages used to generate initial summary.\n        MAX_DEPTH (int): Maximum recursion depth for engram processing.\n        SECTION, H1, H3 (int): Enum-like values to manage tag-based engram splitting depth.\n\n    Methods:\n        parse_media_resource(document): Loads and validates a PDF, then initiates conversion.\n        _convert_pages_to_images(pdf, start_page, end_page): Converts PDF pages to images asynchronously.\n        _page_to_image(pdf, page_number): Converts and encodes a single PDF page to Base64.\n        _on_pages_converted(future): Handles post-conversion logic, triggering initial summary.\n        _generate_short_summary(): Sends preview pages to the LLM for initial semantic scan.\n        _on_short_summary(future): Kicks off page-by-page scanning after initial summary.\n        _scan_page(page_num): Sends a single page to the LLM plugin and extracts structured data.\n        _on_pages_scanned(future): Extracts structured context and initiates full summary.\n        _process_engrams(text_in, context, depth): Recursively splits and constructs Engrams from HTML text.\n        _generate_full_summary(summary): Uses full document content to generate final semantic summary.\n        _on_generate_full_summary(future): Wraps summary and Engrams into a Meta and emits final Observation.\n    \"\"\"\n\n    DPI = 72\n    DPI_DIVISOR = 72\n    TEST_PAGE_LIMIT = 100\n    MAX_CHUNK_SIZE = 1200\n    SHORT_SUMMARY_PAGE_COUNT = 4\n    MAX_DEPTH = 3\n    SECTION = 0\n    H1 = 1\n    H3 = 2\n\n    def __init__(self, parent_service: SenseService, repo_id: str, tracking_id: str):\n        self.scan_id = str(uuid.uuid4())\n        self.observation_id = str(uuid.uuid4())\n        self.repo_id: str = repo_id\n        if repo_id:\n            self.repo_ids: list[str] | None = [repo_id]\n        else:\n            self.repo_ids = None\n        self.service = parent_service\n        self.document: Document | None = None\n        self.tracking_id = tracking_id\n        self.page_images: list[str] = []\n        self.sense_initial_summary = self.service.sense_initial_summary\n\n    def parse_media_resource(self, document: Document) -&gt; None:\n        async def send_message(observation_id: str, document_id: str) -&gt; None:\n            self.service.send_message_async(\n                Service.Topic.OBSERVATION_CREATED, {'id': observation_id, 'parent_id': document_id}\n            )\n\n        self.service.run_task(send_message(self.observation_id, document.id))\n\n        logging.info('Parsing document: %s', document.file_name)\n        file_path: Traversable | Path\n        if document.root_directory == Document.Root.RESOURCE.value:\n            file_path = files(document.file_path).joinpath(document.file_name)\n        elif document.root_directory == Document.Root.DATA.value:\n            repo_root = os.getenv('REPO_ROOT')\n            if repo_root is None:\n                error = \"Environment variable 'REPO_ROOT' is not set.\"\n                raise RuntimeError(error)\n            expanded_path = Path(repo_root + document.file_path).expanduser()\n            file_path = expanded_path / document.file_name\n\n        self.document = document\n        self.source_ids = [document.id]\n\n        try:\n            with file_path.open('rb') as file_ptr:\n                pdf_document: fitz.Document = fitz.open(stream=file_ptr.read(), filetype='pdf')\n\n                total_pages = pdf_document.page_count\n\n                if total_pages == 0:\n                    error = 'PDF loaded with zero page count.'\n                    raise RuntimeError(error)\n\n                self.page_images = [''] * total_pages\n                self.total_pages = total_pages\n                self._convert_pages_to_images(pdf_document, 0, total_pages)\n        except FileNotFoundError as e:\n            error = f'File {document.file_name} failed to open. {e}'\n            logging.exception(error)\n        except RuntimeError as e:\n            error = f'File {document.file_name} failed to open. {e}'\n            logging.exception(error)\n\n    def _convert_pages_to_images(self, pdf: fitz.Document, start_page: int, end_page: int) -&gt; None:\n        coroutines = [self._page_to_image(pdf, i) for i in range(start_page, end_page)]\n        future = self.service.run_tasks(coroutines)\n        future.add_done_callback(self._on_pages_converted)\n\n    async def _page_to_image(self, pdf: fitz.Document, page_number: int) -&gt; bool:\n        page = pdf.load_page(page_number)\n        zoom = Scan.DPI / Scan.DPI_DIVISOR\n        matrix = fitz.Matrix(zoom, zoom)\n        pix = page.get_pixmap(matrix=matrix)\n\n        # Convert the pixmap to PNG bytes in memory\n        img_bytes = pix.tobytes('png')\n\n        # Encode to Base64\n        encoded_img = base64.b64encode(img_bytes).decode('utf-8')\n\n        # Store the Base64 string in image_array\n        self.page_images[page_number] = encoded_img\n\n        return True\n\n    def _on_pages_converted(self, future: Future[Any]) -&gt; None:\n        ret_functions = future.result()\n        del ret_functions\n        summary_future = self.service.run_task(self._generate_short_summary())\n        summary_future.add_done_callback(self._on_short_summary)\n\n    async def _generate_short_summary(self) -&gt; Any:\n        plugin = self.sense_initial_summary\n        summary_images = self.page_images[: Scan.SHORT_SUMMARY_PAGE_COUNT]\n\n        if self.document is None:\n            error = 'Document must be set before generating prompt.'\n            raise ValueError(error)\n\n        prompt = PromptGenMeta(input_data={'file_path': self.document.file_path, 'file_name': self.document.file_name})\n\n        structured_response = {\n            'file_path': str,\n            'file_name': str,\n            'subject': str,\n            'audience': str,\n            'document_title': str,\n            'document_format': str,\n            'document_type': str,\n            'toc': str,\n            'summary_initial': str,\n            'author': str,\n            'date': str,\n            'version': str,\n        }\n\n        ret = self.sense_initial_summary['func'].submit(\n            prompt=prompt,\n            images=summary_images,\n            structured_schema=structured_response,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        initial_scan = json.loads(ret[0]['llm_response'])\n\n        return initial_scan\n\n    def _on_short_summary(self, future: Future[Any]) -&gt; None:\n        result = future.result()\n        self.inital_scan = result\n\n        self.total_pages = min(Scan.TEST_PAGE_LIMIT, self.total_pages)\n        coroutines = [self._scan_page(i) for i in range(self.total_pages)]\n        future = self.service.run_tasks(coroutines)\n\n        future.add_done_callback(self._on_pages_scanned)\n\n    async def _scan_page(self, page_num: int) -&gt; Any:\n        # logging.info(f\"Scan Page: {page_num}\")\n        plugin = self.service.sense_scan_page\n\n        initial_scan_copy = copy.copy(self.inital_scan)\n        initial_scan_copy.update({'page_number': page_num + 1})\n\n        prompt_scan = PromptScanPage(input_data=initial_scan_copy)\n\n        image = self.page_images[page_num]\n\n        ret = await asyncio.to_thread(\n            self.sense_initial_summary['func'].submit,\n            prompt=prompt_scan,\n            images=[image],\n            structured_schema=None,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        return ret[0]['llm_response']\n\n    def _on_pages_scanned(self, future: Future[Any]) -&gt; None:\n        result = future.result()\n\n        self.meta_id = str(uuid.uuid4())\n\n        context: dict[str, str] = {}\n\n        context = copy.copy(self.inital_scan)\n        del context['summary_initial']\n        del context['toc']\n\n        self.engrams: list[Engram] = []\n\n        assembled = ''\n        for page in result['_scan_page']:\n            assembled += page\n\n        # matches1 = re.findall(r'&lt;h1[^&gt;]*&gt;(.*?)&lt;/h1&gt;', assembled, re.DOTALL | re.IGNORECASE)\n        # matches2 = re.findall(r'&lt;h3[^&gt;]*&gt;(.*?)&lt;/h3&gt;', assembled, re.DOTALL | re.IGNORECASE)\n\n        self._process_engrams(assembled, context)\n\n        future = self.service.run_task(self._generate_full_summary(assembled))\n        future.add_done_callback(self._on_generate_full_summary)\n\n    def _process_engrams(self, text_in: str, context: dict[str, str], depth: int = 0) -&gt; None:\n        if len(text_in) &gt; Scan.MAX_CHUNK_SIZE and depth &lt; Scan.MAX_DEPTH:\n            tag = ''\n            if depth == Scan.SECTION:\n                tag = 'section'\n            if depth == Scan.H1:\n                tag = 'h1'\n            if depth == Scan.H3:\n                tag = 'h3'\n\n            depth += 1\n\n            pattern = rf'(?=&lt;{tag}[^&gt;]*&gt;)'\n            parts = re.split(pattern, text_in, flags=re.IGNORECASE)\n            clean_parts = [part.strip() for part in parts if part.strip()]\n\n            for part in clean_parts:\n                match = re.search(rf'&lt;{tag}[^&gt;]*&gt;(.*?)&lt;/{tag}&gt;', part, re.DOTALL | re.IGNORECASE)\n                context_copy = context\n                if match:\n                    tag_str = match.group(1).strip()\n                    context.update({tag: tag_str})\n                    context_copy = copy.copy(context)\n\n                self._process_engrams(part, context_copy, depth)\n\n        else:\n            if self.document is None:\n                error = 'Document id is None but a value is expected.'\n                raise RuntimeError(error)\n\n            engram = Engram(\n                str(uuid.uuid4()),\n                [self.inital_scan['file_path']],\n                [self.document.id],\n                text_in,\n                True,\n                context,\n                None,\n                [self.meta_id],\n                self.repo_ids,\n                None,  # accuracy\n                None,  # relevancy\n                int(datetime.now(timezone.utc).timestamp()),\n            )\n\n            self.engrams.append(engram)\n\n    async def _generate_full_summary(self, summary: str) -&gt; Any:\n        plugin = self.service.sense_full_summary\n\n        initial_scan_copy = copy.copy(self.inital_scan)\n        initial_scan_copy.update({'full_text': summary})\n\n        prompt = PromptGenFullSummary(input_data=initial_scan_copy)\n\n        structure = {'summary_full': str, 'keywords': str}\n\n        ret = self.service.sense_full_summary['func'].submit(\n            prompt=prompt, images=None, structured_schema=structure, args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        llm_response = ret[0]['llm_response']\n\n        return llm_response\n\n    def _on_generate_full_summary(self, future: Future[Any]) -&gt; None:\n        results = json.loads(future.result())\n\n        if self.document is None:\n            error = 'Document is None but expected not to be.'\n            raise RuntimeError(error)\n\n        meta = Meta(\n            self.meta_id,\n            Meta.SourceType.DOCUMENT.value,\n            [self.inital_scan['file_path'] + self.inital_scan['file_name']],\n            self.source_ids,\n            results['keywords'].split(','),\n            self.repo_ids,\n            self.inital_scan['summary_initial'],\n            Index(results['summary_full']),\n            self.document.id,\n        )\n\n        observation = Observation(\n            self.observation_id,\n            meta,\n            self.engrams,\n            int(datetime.now(timezone.utc).timestamp()),\n            self.document.id,\n            self.tracking_id,\n        )\n\n        self.service.host.update_mock_data_output(self.service, asdict(observation))\n\n        self.service.send_message_async(Service.Topic.OBSERVATION_COMPLETE, asdict(observation))\n        self.service.send_message_async(Service.Topic.DOCUMENT_COMPLETE, asdict(self.document))\n</code></pre>"},{"location":"reference/sense_service/","title":"Sense Service","text":"<p>               Bases: <code>Service</code></p> <p>Processes and analyzes documents to extract semantic content for the Engramic system.</p> <p>This service listens for document submission events, initializes a scan process that parses the media resource, and notifies the system of newly created inputs. Currently only supports document-based inputs, with other formats planned for future releases.</p> <p>Attributes:</p> Name Type Description <code>sense_initial_summary</code> <code>Plugin</code> <p>Plugin for generating an initial summary of the document.</p> <code>sense_scan_page</code> <code>Plugin</code> <p>Plugin for scanning and interpreting document content.</p> <code>sense_full_summary</code> <code>Plugin</code> <p>Plugin for producing full document summaries.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Initializes the service asynchronously and sets up any required connections or state.</p> <code>start</code> <p>Subscribes to the system topic for document submissions.</p> <code>on_document_submit</code> <p>dict[Any, Any]) -&gt; None: Extracts file information from a message and submits the document.</p> <code>submit_document</code> <p>Document, *, overwrite: bool = False) -&gt; Document | None: Triggers scanning of the submitted document and sends async notification.</p> <code>on_document_created_sent</code> <p>Future[Any]) -&gt; None: Callback function that initiates document scanning after creation notification.</p> Source code in <code>src/engramic/application/sense/sense_service.py</code> <pre><code>class SenseService(Service):\n    \"\"\"\n    Processes and analyzes documents to extract semantic content for the Engramic system.\n\n    This service listens for document submission events, initializes a scan process that parses the media\n    resource, and notifies the system of newly created inputs. Currently only supports document-based\n    inputs, with other formats planned for future releases.\n\n    Attributes:\n        sense_initial_summary (Plugin): Plugin for generating an initial summary of the document.\n        sense_scan_page (Plugin): Plugin for scanning and interpreting document content.\n        sense_full_summary (Plugin): Plugin for producing full document summaries.\n\n    Methods:\n        init_async() -&gt; None:\n            Initializes the service asynchronously and sets up any required connections or state.\n        start() -&gt; None:\n            Subscribes to the system topic for document submissions.\n        on_document_submit(msg: dict[Any, Any]) -&gt; None:\n            Extracts file information from a message and submits the document.\n        submit_document(document: Document, *, overwrite: bool = False) -&gt; Document | None:\n            Triggers scanning of the submitted document and sends async notification.\n        on_document_created_sent(ret: Future[Any]) -&gt; None:\n            Callback function that initiates document scanning after creation notification.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.sense_initial_summary = host.plugin_manager.get_plugin('llm', 'sense_initial_summary')\n        self.sense_scan_page = host.plugin_manager.get_plugin('llm', 'sense_scan')\n        self.sense_full_summary = host.plugin_manager.get_plugin('llm', 'sense_full_summary')\n\n    def init_async(self) -&gt; None:\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.SUBMIT_DOCUMENT, self.on_document_submit)\n        super().start()\n\n    def on_document_submit(self, msg: dict[Any, Any]) -&gt; None:\n        document = Document(**msg['document'])\n        overwrite = False\n        if 'overwrite' in msg:\n            overwrite = msg['overwrite']\n\n        self.submit_document(document, overwrite=overwrite)\n\n    def submit_document(self, document: Document, *, overwrite: bool = False) -&gt; Document | None:\n        if document.is_scanned is True and overwrite is False:\n            return None\n\n        self.host.update_mock_data_input(\n            self,\n            asdict(document),\n        )\n\n        async def send_message() -&gt; Document:\n            self.send_message_async(\n                Service.Topic.DOCUMENT_CREATED,\n                {'id': document.id, 'type': 'document', 'tracking_id': document.tracking_id},\n            )\n            return document\n\n        future = self.run_task(send_message())\n        future.add_done_callback(self.on_document_created_sent)\n        return document\n\n    def on_document_created_sent(self, ret: Future[Any]) -&gt; None:\n        document = ret.result()\n        scan = Scan(self, document.repo_id, document.tracking_id)\n        scan.parse_media_resource(document)\n</code></pre>"},{"location":"reference/storage_service/","title":"Storage Service","text":"<p>               Bases: <code>Service</code></p> <p>A service responsible for persisting runtime data artifacts within the Engramic system.</p> <p>StorageService listens for various system events and saves corresponding data\u2014including observations, engrams, metadata, and prompt histories\u2014via plugin-based repositories. It also tracks metrics for each type of saved entity to facilitate performance monitoring and operational insights.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Provides access to system plugins, including database integrations.</p> <code>db_document_plugin</code> <p>Plugin used by repositories for data persistence.</p> <code>history_repository</code> <code>HistoryRepository</code> <p>Manages saving of prompt/response history data.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles saving of Observation entities.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Handles saving of Engram entities.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Handles saving of Meta configuration entities.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks counts of saved items for metric reporting.</p> <p>Methods:</p> Name Description <code>start</code> <p>Registers the service to relevant message topics and begins operation.</p> <code>init_async</code> <p>Connects to the database plugin asynchronously before full service startup.</p> <code>on_engram_complete</code> <p>Callback for storing completed engram batches.</p> <code>on_observation_complete</code> <p>Callback for storing completed observations.</p> <code>on_prompt_complete</code> <p>Callback for storing completed prompt/response history.</p> <code>on_meta_complete</code> <p>Callback for storing finalized meta configuration.</p> <code>save_observation</code> <p>Coroutine to persist observations and update metrics.</p> <code>save_history</code> <p>Coroutine to persist prompt/response history and update metrics.</p> <code>save_engram</code> <p>Coroutine to persist engram data and update metrics.</p> <code>save_meta</code> <p>Coroutine to persist metadata and update metrics.</p> <code>on_acknowledge</code> <p>Collects current metrics and publishes service status.</p> Source code in <code>src/engramic/application/storage/storage_service.py</code> <pre><code>class StorageService(Service):\n    \"\"\"\n    A service responsible for persisting runtime data artifacts within the Engramic system.\n\n    StorageService listens for various system events and saves corresponding data\u2014including observations,\n    engrams, metadata, and prompt histories\u2014via plugin-based repositories. It also tracks metrics for each\n    type of saved entity to facilitate performance monitoring and operational insights.\n\n    Attributes:\n        plugin_manager (PluginManager): Provides access to system plugins, including database integrations.\n        db_document_plugin: Plugin used by repositories for data persistence.\n        history_repository (HistoryRepository): Manages saving of prompt/response history data.\n        observation_repository (ObservationRepository): Handles saving of Observation entities.\n        engram_repository (EngramRepository): Handles saving of Engram entities.\n        meta_repository (MetaRepository): Handles saving of Meta configuration entities.\n        metrics_tracker (MetricsTracker): Tracks counts of saved items for metric reporting.\n\n    Methods:\n        start() -&gt; None:\n            Registers the service to relevant message topics and begins operation.\n        init_async() -&gt; None:\n            Connects to the database plugin asynchronously before full service startup.\n        on_engram_complete(engram_dict) -&gt; None:\n            Callback for storing completed engram batches.\n        on_observation_complete(response) -&gt; None:\n            Callback for storing completed observations.\n        on_prompt_complete(response_dict) -&gt; None:\n            Callback for storing completed prompt/response history.\n        on_meta_complete(meta_dict) -&gt; None:\n            Callback for storing finalized meta configuration.\n        save_observation(response) -&gt; None:\n            Coroutine to persist observations and update metrics.\n        save_history(response) -&gt; None:\n            Coroutine to persist prompt/response history and update metrics.\n        save_engram(engram) -&gt; None:\n            Coroutine to persist engram data and update metrics.\n        save_meta(meta) -&gt; None:\n            Coroutine to persist metadata and update metrics.\n        on_acknowledge(message_in) -&gt; None:\n            Collects current metrics and publishes service status.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.history_repository: HistoryRepository = HistoryRepository(self.db_document_plugin)\n        self.observation_repository: ObservationRepository = ObservationRepository(self.db_document_plugin)\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.meta_repository: MetaRepository = MetaRepository(self.db_document_plugin)\n        self.metrics_tracker: MetricsTracker[StorageMetric] = MetricsTracker[StorageMetric]()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_prompt_complete)\n        self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n        self.subscribe(Service.Topic.ENGRAM_COMPLETE, self.on_engram_complete)\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        super().start()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_engram_complete(self, engram_dict: dict[str, Any]) -&gt; None:\n        engram_batch = self.engram_repository.load_batch_dict(engram_dict['engram_array'])\n        for engram in engram_batch:\n            self.run_task(self.save_engram(engram))\n\n    def on_observation_complete(self, response: Observation) -&gt; None:\n        self.run_task(self.save_observation(response))\n\n    def on_prompt_complete(self, response_dict: dict[Any, Any]) -&gt; None:\n        response_dict['prompt'] = Prompt(**response_dict['prompt'])\n        response = Response(**response_dict)\n\n        if not response.prompt.is_lesson:\n            self.run_task(self.save_history(response))\n\n    def on_meta_complete(self, meta_dict: dict[str, str]) -&gt; None:\n        meta: Meta = self.meta_repository.load(meta_dict)\n        self.run_task(self.save_meta(meta))\n\n    async def save_observation(self, response: Observation) -&gt; None:\n        \"\"\"\n        Persists an observation to the database and updates the observation metrics.\n\n        Args:\n            response (Observation): The observation object to be saved.\n\n        Returns:\n            None\n        \"\"\"\n        self.observation_repository.save(response)\n        self.metrics_tracker.increment(StorageMetric.OBSERVATION_SAVED)\n        logging.debug('Storage service saving observation.')\n\n    async def save_history(self, response: Response) -&gt; None:\n        await asyncio.to_thread(self.history_repository.save_history, response)\n        self.metrics_tracker.increment(StorageMetric.HISTORY_SAVED)\n        logging.debug('Storage service saving history.')\n\n    async def save_engram(self, engram: Engram) -&gt; None:\n        await asyncio.to_thread(self.engram_repository.save_engram, engram)\n        self.metrics_tracker.increment(StorageMetric.ENGRAM_SAVED)\n        logging.debug('Storage service saving engram.')\n\n    async def save_meta(self, meta: Meta) -&gt; None:\n        logging.debug('Storage service saving meta.')\n        await asyncio.to_thread(self.meta_repository.save, meta)\n        self.metrics_tracker.increment(StorageMetric.META_SAVED)\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/storage_service/#engramic.application.storage.storage_service.StorageService.save_observation","title":"<code>save_observation(response)</code>  <code>async</code>","text":"<p>Persists an observation to the database and updates the observation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Observation</code> <p>The observation object to be saved.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/engramic/application/storage/storage_service.py</code> <pre><code>async def save_observation(self, response: Observation) -&gt; None:\n    \"\"\"\n    Persists an observation to the database and updates the observation metrics.\n\n    Args:\n        response (Observation): The observation object to be saved.\n\n    Returns:\n        None\n    \"\"\"\n    self.observation_repository.save(response)\n    self.metrics_tracker.increment(StorageMetric.OBSERVATION_SAVED)\n    logging.debug('Storage service saving observation.')\n</code></pre>"},{"location":"reference/teach/","title":"Teach Service","text":"<p>               Bases: <code>Service</code></p> <p>Service that generates educational lessons from documents.</p> <p>Monitors document insertions and metadata completion to create learning materials based on document content.</p> <p>Attributes:</p> Name Type Description <code>teach_generate_questions</code> <p>Plugin for generating questions from documents.</p> <code>meta_cache</code> <code>dict[str, Meta]</code> <p>Cache for document metadata.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Initializes asynchronous components.</p> <code>start</code> <p>Starts the service and subscribes to relevant topics.</p> <code>on_meta_complete</code> <p>Handles metadata completion events.</p> <code>on_document_inserted</code> <p>Processes newly inserted documents to create lessons.</p> Source code in <code>src/engramic/application/teach/teach_service.py</code> <pre><code>class TeachService(Service):\n    \"\"\"\n    Service that generates educational lessons from documents.\n\n    Monitors document insertions and metadata completion to create learning\n    materials based on document content.\n\n    Attributes:\n        teach_generate_questions: Plugin for generating questions from documents.\n        meta_cache (dict[str, Meta]): Cache for document metadata.\n\n    Methods:\n        init_async() -&gt; None:\n            Initializes asynchronous components.\n        start() -&gt; None:\n            Starts the service and subscribes to relevant topics.\n        on_meta_complete(msg) -&gt; None:\n            Handles metadata completion events.\n        on_document_inserted(msg) -&gt; None:\n            Processes newly inserted documents to create lessons.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        \"\"\"\n        Initializes the TeachService.\n\n        Args:\n            host (Host): The host system providing access to plugins and services.\n        \"\"\"\n        super().__init__(host)\n        self.teach_generate_questions = host.plugin_manager.get_plugin('llm', 'teach_generate_questions')\n        self.meta_cache: dict[str, Meta] = {}\n\n    def init_async(self) -&gt; None:\n        \"\"\"Initializes asynchronous components of the service.\"\"\"\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        \"\"\"\n        Starts the service and subscribes to relevant topics.\n\n        Subscribes to META_COMPLETE and DOCUMENT_INSERTED topics to monitor\n        document processing events.\n        \"\"\"\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        self.subscribe(Service.Topic.DOCUMENT_INSERTED, self.on_document_inserted)\n        super().start()\n\n    def on_meta_complete(self, msg: dict[Any, Any]) -&gt; None:\n        \"\"\"\n        Handles metadata completion events.\n\n        Stores document metadata in the cache for later processing.\n\n        Args:\n            msg (dict[Any, Any]): The metadata message.\n        \"\"\"\n        meta = Meta(**msg)\n        if meta.type == meta.SourceType.DOCUMENT.value and meta.parent_id is not None:\n            self.meta_cache[meta.parent_id] = meta\n\n    def on_document_inserted(self, msg: dict[Any, Any]) -&gt; None:\n        \"\"\"\n        Processes newly inserted documents to create lessons.\n\n        When a document is inserted and its metadata is available, creates\n        a new lesson for the document.\n\n        Args:\n            msg (dict[Any, Any]): The document insertion message.\n        \"\"\"\n        document_id = msg['id']\n        if document_id in self.meta_cache:\n            meta = self.meta_cache[document_id]\n            lesson = Lesson(self, str(uuid.uuid4()), str(uuid.uuid4()), document_id)\n            lesson.run_lesson(meta)\n            del self.meta_cache[document_id]\n</code></pre>"},{"location":"reference/teach/#engramic.application.teach.teach_service.TeachService.__init__","title":"<code>__init__(host)</code>","text":"<p>Initializes the TeachService.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>Host</code> <p>The host system providing access to plugins and services.</p> required Source code in <code>src/engramic/application/teach/teach_service.py</code> <pre><code>def __init__(self, host: Host) -&gt; None:\n    \"\"\"\n    Initializes the TeachService.\n\n    Args:\n        host (Host): The host system providing access to plugins and services.\n    \"\"\"\n    super().__init__(host)\n    self.teach_generate_questions = host.plugin_manager.get_plugin('llm', 'teach_generate_questions')\n    self.meta_cache: dict[str, Meta] = {}\n</code></pre>"},{"location":"reference/teach/#engramic.application.teach.teach_service.TeachService.init_async","title":"<code>init_async()</code>","text":"<p>Initializes asynchronous components of the service.</p> Source code in <code>src/engramic/application/teach/teach_service.py</code> <pre><code>def init_async(self) -&gt; None:\n    \"\"\"Initializes asynchronous components of the service.\"\"\"\n    return super().init_async()\n</code></pre>"},{"location":"reference/teach/#engramic.application.teach.teach_service.TeachService.on_document_inserted","title":"<code>on_document_inserted(msg)</code>","text":"<p>Processes newly inserted documents to create lessons.</p> <p>When a document is inserted and its metadata is available, creates a new lesson for the document.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[Any, Any]</code> <p>The document insertion message.</p> required Source code in <code>src/engramic/application/teach/teach_service.py</code> <pre><code>def on_document_inserted(self, msg: dict[Any, Any]) -&gt; None:\n    \"\"\"\n    Processes newly inserted documents to create lessons.\n\n    When a document is inserted and its metadata is available, creates\n    a new lesson for the document.\n\n    Args:\n        msg (dict[Any, Any]): The document insertion message.\n    \"\"\"\n    document_id = msg['id']\n    if document_id in self.meta_cache:\n        meta = self.meta_cache[document_id]\n        lesson = Lesson(self, str(uuid.uuid4()), str(uuid.uuid4()), document_id)\n        lesson.run_lesson(meta)\n        del self.meta_cache[document_id]\n</code></pre>"},{"location":"reference/teach/#engramic.application.teach.teach_service.TeachService.on_meta_complete","title":"<code>on_meta_complete(msg)</code>","text":"<p>Handles metadata completion events.</p> <p>Stores document metadata in the cache for later processing.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>dict[Any, Any]</code> <p>The metadata message.</p> required Source code in <code>src/engramic/application/teach/teach_service.py</code> <pre><code>def on_meta_complete(self, msg: dict[Any, Any]) -&gt; None:\n    \"\"\"\n    Handles metadata completion events.\n\n    Stores document metadata in the cache for later processing.\n\n    Args:\n        msg (dict[Any, Any]): The metadata message.\n    \"\"\"\n    meta = Meta(**msg)\n    if meta.type == meta.SourceType.DOCUMENT.value and meta.parent_id is not None:\n        self.meta_cache[meta.parent_id] = meta\n</code></pre>"},{"location":"reference/teach/#engramic.application.teach.teach_service.TeachService.start","title":"<code>start()</code>","text":"<p>Starts the service and subscribes to relevant topics.</p> <p>Subscribes to META_COMPLETE and DOCUMENT_INSERTED topics to monitor document processing events.</p> Source code in <code>src/engramic/application/teach/teach_service.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starts the service and subscribes to relevant topics.\n\n    Subscribes to META_COMPLETE and DOCUMENT_INSERTED topics to monitor\n    document processing events.\n    \"\"\"\n    self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n    self.subscribe(Service.Topic.DOCUMENT_INSERTED, self.on_document_inserted)\n    super().start()\n</code></pre>"}]}