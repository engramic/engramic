{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engramic API Reference","text":"<p>Engramic is pre-alpha. It's a great time to start working with these core systems for some developers, but we have yet to complete many important features and even the core systems may not be fully tested. In other words, use of this code base will require some development experience and the ability to work in maturing environments. The flip side, is that a new community is forming and as a pioneer, you have an opportunity to get in early so that you can someday tell your friends, I used Engramic before it was cool.</p> <p>There is currently no support for the following:</p> <ul> <li>There is no support for individual users.</li> <li>There is no HTTP(s) interface at this time. </li> <li>This is no support for documents such as PDFs.</li> <li>Windows and MacOS is not being tested as part of our release process.</li> </ul> <p>These features, along with others, will be available in the near future.</p> <p>For an evergreen overview of Engramic, visit our online Knowledge Base.</p>"},{"location":"#introduction-to-engramic","title":"Introduction to Engramic","text":""},{"location":"#why-engramic","title":"Why Engramic?","text":"<p>Engramic is designed to learn from your unstructured, proprietary data using any large language model (LLM). When we study, we often begin by reading a document from start to finish. But true understanding comes from synthesizing the information, asking questions, identifying what\u2019s meaningful, and connecting it to prior knowledge and related context. Learning is an iterative process\u2014not a linear one. That\u2019s why we believe a large context window alone doesn\u2019t solve the challenge of truly understanding a dataset. This belief, shaped by two years of research, is what inspired Engramic\u2019s design.</p>"},{"location":"#engramic-architecture-philosophy","title":"Engramic Architecture Philosophy","text":"<ul> <li> <p>Modular   The plugin system allows easy switching between LLMs, databases, vector databases, and embedding tools.</p> </li> <li> <p>Scalable   Built as a set of independent services, Engramic can run on a single machine or be distributed across multiple systems.</p> </li> <li> <p>Fast   Optimized for usage patterns involving many blocking API calls, ensuring responsive performance.</p> </li> <li> <p>Extensible   Easily create custom services or plugins to extend functionality.</p> </li> </ul>"},{"location":"#engramic-core-concepts","title":"Engramic Core Concepts","text":"<ul> <li> <p>Memory   Supports both short-term and long-term memory mechanisms.</p> </li> <li> <p>Engram   The fundamental unit of information, which includes content and base and customizable contextual metadata</p> </li> <li> <p>Citable Engrams   External documents or media that are directly referenced. Citable Engrams are high-fidelity textual representations of the media.</p> </li> <li> <p>Long-Term Memory Engrams   Constructed from one or more Citable Engram or other Long-Term Memory Engrams.</p> </li> <li> <p>Learning   Built through the combination of memory, citable external sources, and user interaction or input.</p> </li> <li> <p>Unified Memory   All engrams are stored within a unified system, enabling both full and selective access to memory content.</p> </li> </ul>"},{"location":"getting_started/","title":"Getting Started with Engramic","text":"<p>Please contact us at info@engramic.org if you have any issues with these instructions.</p> 1. Pre-Requisites - OS &amp; IDE <p>We are currently testing Engramic on WSL on Windows using Visual Studio Code With Ubuntu 24.0.1 LTS. It may run on other configurations\u2014we'll begin cross-platform testing soon. If you'd like to help us, please reach out at info@engramic.org.</p> <p>Engramic is availible via pip, however, working from source is recommended for this release.</p> <pre><code>pip install engramic\n</code></pre> <p>To set up your dev environment, you will need the following:</p> <ul> <li>Visual Studio Code</li> <li>Git</li> <li>Pipx</li> <li>Hatch</li> <li>MS VS Code WSL Extension</li> <li>MS Python Debugger</li> <li>Google Gemini API key (optional)</li> </ul> 2. Clone or Fork From GitHub <p>Clone or fork the latest version from the <code>main</code> branch of the Engramic GitHub repository:</p> <p>\ud83d\udcce https://github.com/engramic/engramic</p> 3. Install Hatch <p>We use Hatch as our Python project manager. It handles all dependencies, Python versioning, testing, versioning, scripts, and virtual environments.</p> <p>\ud83d\udd17 Hatch Installation</p> <p>We recommend installing with <code>pipx</code> as described in the Hatch installation instructions. Restart your terminal after running pipx ensurepath.</p> 4. Initialize the Environment <p>Now that Hatch is installed:</p> <ol> <li>Navigate to the root of the Engramic project in your terminal.</li> <li> <p>Run:</p> <pre><code>hatch env create\n</code></pre> <p>Enter into the default shell (\"default\" has no name after \"shell\".) <pre><code>hatch shell\n</code></pre></p> </li> </ol> <p>This will install all dependencies (should be quick\u2014we work hard to minimize dependencies). Watch a video of Hatch in Engramic.</p> <ol> <li> <p>Open Visual Studio Code and install the WSL extension.</p> </li> <li> <p>Launch VS Code from the WSL terminal:</p> <pre><code>code .\n</code></pre> </li> <li> <p>In Windows, in VS Code, install the Python Debugger extension from Microsoft.</p> </li> </ol> 5. Configure the Python Interpreter <p>In Visual Studio Code:</p> <ol> <li>Press <code>Ctrl + Shift + P</code></li> <li>Search for and select \"Python: Select Interpreter\"</li> <li>Choose the environment that looks like: <code>Python X.XX.X ('engramic')</code></li> </ol> <p>Note: If you aren't sure of the path, you can type the following while in the hatch shell:</p> <pre><code>python -c \"import sys;print(sys.executable)\"\n</code></pre> <p>If you are stuck, make sure your top, middle search bar in VS Code reads: engramic [WSL: Ubuntu-24.04] (or your distro). If not, your issue is probably related to the WSL extension.</p> 6. Run the Mock Example 7. Run the Standard Example 8. Create your First Memory."},{"location":"getting_started/#running-the-code","title":"Running The Code","text":"<p>The code is available at: <pre><code>    engramic/examples/mock_profile/mock_profile.py\n</code></pre></p> <ol> <li>Open the Run and Debug sidebar in VS Code.</li> <li>Choose \"Example - Mock\" and run it.</li> </ol> <p>Congrats! \ud83c\udf89 You've just run the mock version of the system using mock plugins. You should see an output message in terminal window.</p>"},{"location":"getting_started/#looking-at-the-code","title":"Looking At The Code","text":"<p>The mock version doesn't actually use AI calls, just emulated API calls that return static responses via the Mock plugins. In this example, you can see how to create a Host, MessageService, RetrieveService, and a ResponseService. Also, we have created a service called TestService whose only job is to recieve the Response call from the subscirbed callback on_main_prompt_complete.</p>"},{"location":"getting_started/#running-the-code_1","title":"Running The Code","text":"<p>Now, let's run an example with actual AI. This example uses Google Gemini.</p> <pre><code>    engramic/examples/standard_profile/standard_profile.py\n</code></pre> <ol> <li> <p>For this example, you'll need a Gemini API key:</p> <ul> <li>Create a Google Cloud account if you don't already have one.</li> <li>Follow Google's documentation to create a Generative Language API key.</li> </ul> </li> <li> <p>Add a <code>.env</code> file to the root of the project with the following content (multiple plugin paths coming soon.):</p> <pre><code>GEMINI_API_KEY=PUT_YOUR_KEY_HERE_WITH_NO_QUOTES_OR_ANYTHING_ELSE\nLOCAL_STORAGE_ROOT_PATH=./local_storage\n</code></pre> <p>Locate this line and change it if you would like to.</p> <pre><code>retrieve_service.submit(Prompt('Briefly tell me about Chamath Palihapitiya.'))\n</code></pre> </li> <li> <p>In Run and Debug, select \"Example - Standard\".</p> <p>Note: this takes some time on first run. Be patient.</p> </li> </ol>"},{"location":"getting_started/#looking-at-the-code_1","title":"Looking At The Code","text":"<p>Run the program. The plugins will automatically download all dependencies on the first run and check for updates on subsequent runs. Configuration for plugins are defined by profiles, in this case, the profile is named \"standard\". Each plugin contains it's dependencies in a plugin.toml file.</p> <p>Hit Run and you'll see the result in the terminal window. This example adds Storage, Codify, and Consolidate service, but doesn't actually use them.</p> <p>Let's try those services in the next demo.</p>"},{"location":"getting_started/#running-the-code_2","title":"Running The Code","text":"<p>Let's generate our first memory.</p> <pre><code>    engramic/examples/create_memory/create_memory.py\n</code></pre> <ol> <li>Run and Debug the profile named \"Example - Create Memory\".</li> </ol> <p>You should see three outputs, Response, Meta Summary, and Engrams.</p>"},{"location":"getting_started/#looking-at-the-code_2","title":"Looking at The Code","text":"<p>This time, we've added another call to our TestService. Services support sync and async threads. Some features that services perform run on the main thread, such as subscribing, while others such as sending messages or running tasks (not demonstrated) must run on the async thread. The Codify service is listening for the SET_TRAINING_MODE call.</p> <pre><code>    class TestService(Service):\n        def start(self):\n            self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE,on_main_prompt_complete)\n            self.subscribe(Service.Topic.OBSERVATION_COMPLETE,on_observation_complete)\n            return super().start()\n\n        def init_async(self):\n            super().init_async()\n            self.send_message_async(Service.Topic.SET_TRAINING_MODE,{\"training_mode\":True})\n            return None\n</code></pre> <p>Let's look at the Observation, the output of the Codify service. Two types of data structures are output on the screen, the first is a set of Engrams, these are the memories extracted from the response of the training. The next is the Meta Summary. Meta data are summary information about all Engrams that were generated. This data structure is created to help the retrieval stage with awareness of it's memory set.</p> <p>To delete all memories, enter into the hatch shell named \"dev\".</p> <pre><code>cd /engramic\nhatch shell dev\nhatch run delete_dbs\n</code></pre>"},{"location":"host_and_services/","title":"Engramic Library Overview","text":"<p>The Engramic Library consists of several services, each designed to facilitate distinct aspects of knowledge processing.</p>"},{"location":"host_and_services/#host","title":"Host","text":"<p>The Host contains all of the executing services on the instance/machine. You may add all services or just one. The host manages initialization and the set of resources and capabilities used by all other systems. The Host is like a very lightweight abstraction layer for all services.</p>"},{"location":"host_and_services/#services","title":"Services","text":"<ul> <li>Retrieve: Analyzes the prompt, manages short-term memory, and performs retrieval of all engrams.</li> <li>Respond: Constructs the response to the user.</li> <li>Codify: While in training mode, assesses the validity of responses, integrating them as long-term memories.</li> <li>Consolidate: Transforms data into observations, contained knowledge units rich in context.</li> <li>Store: Stores long-term, context-aware memory for rapid retrieval.</li> <li>Message: Centeralized message passing between all services.</li> </ul>"},{"location":"host_and_services/#services-in-development","title":"Services In Development","text":"<ul> <li>Sense: Converts raw data into observations.</li> <li>Teach: Encourages the emergence of new engrams by stimulating connections between existing ones.</li> </ul>"},{"location":"how_to/","title":"More How To's Coming Soon...","text":"<p>Build A Web App.</p> <p>Build Memories on a File.</p> <p>Use The Teach Service to Automate Training.</p> <p>Develop Your Own Plugin.</p> <p>Develop Your Own Service.</p>"},{"location":"profiles/","title":"Engramic Plugin and Profile System","text":"<p>Engramic is designed with a plugin layer that allows seamless integration with various components such as LLMs, databases, vector databases, and embedding services. The glue that binds these systems together are called profiles.</p>"},{"location":"profiles/#example-default-profile-configuration","title":"Example: Default Profile Configuration","text":"<pre><code>version = 0.1\n\n[mock]\ntype = \"profile\"\nvector_db.db = {name=\"Mock\"}\nllm.retrieve_gen_conversation_direction = {name=\"Mock\"}\nllm.retrieve_gen_index = {name=\"Mock\"}\nllm.retrieve_prompt_analysis = {name=\"Mock\"}\ndb.document = {name=\"Mock\"}\nllm.response_main = {name=\"Mock\"}\nllm.validate = {name=\"Mock\"}\nllm.summary = {name=\"Mock\"}\nllm.gen_indices = {name=\"Mock\"}\nembedding.gen_embed = {name=\"Mock\"}\n\n[standard]\ntype = \"pointer\"\nptr = \"standard-2025-04-01\"\n\n[standard-2025-04-01]\ntype = \"profile\"\nvector_db.db = {name=\"ChromaDB\"}\nllm.retrieve_gen_conversation_direction = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.retrieve_gen_index = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.retrieve_prompt_analysis = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\ndb.document = {name=\"Sqlite\"}\nllm.response_main = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.validate = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.summary = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nllm.gen_indices = {name=\"Gemini\", model=\"gemini-2.0-flash\"}\nembedding.gen_embed = {name=\"Gemini\", model=\"text-embedding-004\"}\n</code></pre>"},{"location":"profiles/#loading-a-profile-with-the-host","title":"Loading a Profile with the Host","text":"<p>When creating a <code>Host</code>, you should load the initial profile like so:</p> <pre><code>host = Host(\n    'standard',\n    [\n        MessageService,\n        RetrieveService,\n        ResponseService,\n        StorageService,\n        CodifyService,\n        ConsolidateService\n    ]\n)\n</code></pre> <p>In this example, the developer is using the <code>standard</code> profile. This is a pointer profile that redirects to a dated version (e.g., <code>standard-2025-04-01</code>). As Engramic evolves, the <code>standard</code> pointer will be updated to reference the currently recommended profile. By using <code>standard</code>, you're always aligned with the recommended configuration.</p> <p>The <code>mock</code> profile is another key option. When using <code>mock</code>, all plugins are mock implementations \u2014 meaning no API calls are made. This mode is intended exclusively for testing and development.</p>"},{"location":"reference/ask/","title":"Ask","text":"<p>               Bases: <code>Retrieval</code></p> Source code in <code>src/engramic/application/retrieve/ask.py</code> <pre><code>class Ask(Retrieval):\n    class Ask(Retrieval):\n        \"\"\"\n        Ask is a specifc type of retrieval focused on traditional Q&amp;A. It is a single instanc of an ask.\n\n        This class handles the end-to-end workflow of transforming a raw prompt into\n        contextual embeddings, querying the vector database, and returning retrieved engram ids.\n        It also supports various conversation analysis and prompt index generation.\n\n        Attributes:\n            id (str): A unique identifier for this retrieval session.\n            prompt (Prompt): The original prompt provided by the user.\n            plugin_manager (PluginManager): Plugin manager used to access LLM, vector DB, and embedding components.\n            metrics_tracker (MetricsTracker): Tracks operational metrics for observability.\n            db_plugin (dict): Plugin used to interact with the document database.\n            service (RetrieveService): Reference to the parent service coordinating this request.\n            library (str | None): Optional name of the target library to search within.\n            conversation_direction (dict[str, str]): Stores current user intent and working memory.\n            prompt_analysis (PromptAnalysis | None): Stores structured analysis of the prompt after processing.\n\n        Methods:\n            get_sources():\n                Initiates the async pipeline for directional memory retrieval.\n\n            _fetch_history():\n                Asynchronously retrieves prior user history from the document DB.\n\n            _retrieve_gen_conversation_direction():\n                Uses LLM plugin to extract user intent and conversational memory.\n\n            _embed_gen_direction():\n                Converts extracted intent into embeddings. General direction determines intent and manages short term memory.\n\n            _vector_fetch_direction_meta():\n                Queries the metadata collection in the vector DB using intent embeddings.\n\n            _fetch_direction_meta():\n                Loads Meta objects from the metadata store based on query results.\n\n            _analyze_prompt():\n                Uses LLM plugin to analyze the user prompt in the context of metadata.\n\n            _generate_indices():\n                Generates semantic indices from the prompt and metadata for retrieval.\n\n            _generate_indicies_embeddings():\n                Converts generated index phrases into embeddings.\n\n            _query_index_db():\n                Searches the main vector DB with embeddings to identify related engrams.\n\n            on_fetch_history_complete(fut):\n                Callback when prompt history fetch is complete; begins direction generation.\n\n            on_direction_ret_complete(fut):\n                Callback when conversation direction is generated; begins embedding.\n\n            on_embed_direction_complete(fut):\n                Callback when direction embedding is ready; begins vector DB search for metadata.\n\n            on_vector_fetch_direction_meta_complete(fut):\n                Callback when metadata vector search is complete; begins metadata fetch.\n\n            on_fetch_direction_meta_complete(fut):\n                Callback when metadata objects are fetched; begins analysis and index generation.\n\n            on_analyze_complete(fut):\n                Callback when prompt analysis and index generation are complete; begins embedding generation.\n\n            on_indices_embeddings_generated(fut):\n                Callback when index embeddings are ready; triggers main index DB query.\n\n            on_query_index_db(fut):\n                Final callback when engram retrieval is complete; assembles and emits the result.\n        \"\"\"\n\n    def __init__(\n        self,\n        ask_id: str,\n        prompt: Prompt,\n        plugin_manager: PluginManager,\n        metrics_tracker: MetricsTracker[engramic.application.retrieve.retrieve_service.RetrieveMetric],\n        db_plugin: dict[str, Any],\n        service: RetrieveService,\n        library: str | None = None,\n    ) -&gt; None:\n        self.id = ask_id\n        self.service = service\n        self.metrics_tracker: MetricsTracker[engramic.application.retrieve.retrieve_service.RetrieveMetric] = (\n            metrics_tracker\n        )\n        self.library = library\n        self.prompt = prompt\n        self.conversation_direction: dict[str, str]\n        self.prompt_analysis: PromptAnalysis | None = None\n        self.retrieve_gen_conversation_direction_plugin = plugin_manager.get_plugin(\n            'llm', 'retrieve_gen_conversation_direction'\n        )\n        self.prompt_analysis_plugin = plugin_manager.get_plugin('llm', 'retrieve_prompt_analysis')\n        self.prompt_retrieve_indices_plugin = plugin_manager.get_plugin('llm', 'retrieve_gen_index')\n        self.prompt_vector_db_plugin = plugin_manager.get_plugin('vector_db', 'db')\n        self.prompt_db_document_plugin = db_plugin\n        self.embeddings_gen_embed = plugin_manager.get_plugin('embedding', 'gen_embed')\n\n    def get_sources(self) -&gt; None:\n        direction_step = self.service.run_task(self._fetch_history())\n        direction_step.add_done_callback(self.on_fetch_history_complete)\n\n    \"\"\"\n    ### CONVERSATION DIRECTION\n\n    Fetches related domain knowledge based on the prompt intent.\n    \"\"\"\n\n    async def _fetch_history(self) -&gt; list[dict[str, Any]]:\n        plugin = self.prompt_db_document_plugin\n        args = plugin['args']\n        args['history'] = 10\n\n        ret_val = await asyncio.to_thread(plugin['func'].fetch, table=DB.DBTables.HISTORY, ids=[], args=args)\n        history_dict: list[dict[str, Any]] = ret_val[0]\n        return history_dict\n\n    def on_fetch_history_complete(self, fut: Future[Any]) -&gt; None:\n        response_array: list[dict[str, Any]] = fut.result()\n        retrieve_gen_conversation_direction_step = self.service.run_task(\n            self._retrieve_gen_conversation_direction(response_array)\n        )\n        retrieve_gen_conversation_direction_step.add_done_callback(self.on_direction_ret_complete)\n\n    async def _retrieve_gen_conversation_direction(self, response_array: list[dict[str, Any]]) -&gt; dict[str, str]:\n        if __debug__:\n            self.service.send_message_async(self.service.Topic.DEBUG_ASK_CREATED, {'ask_id': self.id})\n\n        input_data = {'history_array': response_array}\n        plugin = self.retrieve_gen_conversation_direction_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt_gen = PromptGenConversation(prompt_str=self.prompt.prompt_str, input_data=input_data)\n\n        structured_schema = {\n            'current_user_intent': str,\n            'working_memory_step_1': str,\n            'working_memory_step_2': str,\n            'working_memory_step_3': str,\n            'working_memory_step_4': str,\n        }\n\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt_gen,\n            structured_schema=structured_schema,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        json_parsed: dict[str, str] = json.loads(ret[0]['llm_response'])\n\n        self.conversation_direction = {}\n        self.conversation_direction['current_user_intent'] = json_parsed['current_user_intent']\n\n        self.conversation_direction['working_memory'] = json_parsed['working_memory_step_4']\n\n        if __debug__:\n            self.service.send_message_async(\n                self.service.Topic.DEBUG_CONVERSATION_DIRECTION,\n                {'ask_id': self.id, 'prompt': prompt_gen.render_prompt(), 'working_memory': ret[0]['llm_response']},\n            )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.CONVERSATION_DIRECTION_CALCULATED\n        )\n\n        return json_parsed\n\n    def on_direction_ret_complete(self, fut: Future[Any]) -&gt; None:\n        direction_ret = fut.result()\n\n        logging.debug('current_user_intent: %s', direction_ret)\n        intent_and_direction = direction_ret['current_user_intent']\n\n        embed_step = self.service.run_task(self._embed_gen_direction(intent_and_direction))\n        embed_step.add_done_callback(self.on_embed_direction_complete)\n\n    async def _embed_gen_direction(self, main_prompt: str) -&gt; list[float]:\n        plugin = self.embeddings_gen_embed\n\n        ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=[main_prompt], args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        float_array: list[float] = ret[0]['embeddings_list'][0]\n        return float_array\n\n    def on_embed_direction_complete(self, fut: Future[Any]) -&gt; None:\n        embedding = fut.result()\n        fetch_direction_step = self.service.run_task(self._vector_fetch_direction_meta(embedding))\n        fetch_direction_step.add_done_callback(self.on_vector_fetch_direction_meta_complete)\n\n    async def _vector_fetch_direction_meta(self, embedding: list[float]) -&gt; list[str]:\n        plugin = self.prompt_vector_db_plugin\n        plugin['args'].update({'threshold': 0.6, 'n_results': 5})\n\n        ret = await asyncio.to_thread(\n            plugin['func'].query,\n            collection_name='meta',\n            embeddings=embedding,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        list_str: list[str] = ret[0]['query_set']\n        # logging.warning(list_str)\n        return list_str\n\n    def on_vector_fetch_direction_meta_complete(self, fut: Future[Any]) -&gt; None:\n        meta_ids = fut.result()\n        meta_fetch_step = self.service.run_task(self._fetch_direction_meta(meta_ids))\n        meta_fetch_step.add_done_callback(self.on_fetch_direction_meta_complete)\n\n    async def _fetch_direction_meta(self, meta_id: list[str]) -&gt; list[Meta]:\n        meta_list = self.service.meta_repository.load_batch(meta_id)\n\n        if __debug__:\n            dict_meta = [meta.summary_full.text if meta.summary_full is not None else '' for meta in meta_list]\n\n            self.service.send_message_async(\n                self.service.Topic.DEBUG_ASK_META, {'ask_id': self.id, 'ask_meta': dict_meta}\n            )\n\n        return meta_list\n\n    def on_fetch_direction_meta_complete(self, fut: Future[Any]) -&gt; None:\n        meta_list = fut.result()\n        analyze_step = self.service.run_tasks([self._analyze_prompt(meta_list), self._generate_indices(meta_list)])\n        analyze_step.add_done_callback(self.on_analyze_complete)\n\n    \"\"\"\n    ### Prompt Analysis\n\n    Analyzies the prompt and generates lookups that will aid in vector searching of related content\n    \"\"\"\n\n    async def _analyze_prompt(self, meta_list: list[Meta]) -&gt; dict[str, str]:\n        plugin = self.prompt_analysis_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt = PromptAnalyzePrompt(\n            prompt_str=self.prompt.prompt_str,\n            input_data={'meta_list': meta_list, 'working_memory': self.conversation_direction['working_memory']},\n        )\n        structured_response = {'response_length': str, 'user_prompt_type': str, 'thinking_steps': str}\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=structured_response,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n\n        self.metrics_tracker.increment(engramic.application.retrieve.retrieve_service.RetrieveMetric.PROMPTS_ANALYZED)\n\n        if not isinstance(ret[0], dict):\n            error = f'Expected dict[str, str], got {type(ret[0])}'\n            raise TypeError(error)\n\n        return ret[0]\n\n    def on_analyze_complete(self, fut: Future[Any]) -&gt; None:\n        analysis = fut.result()  # This will raise an exception if the coroutine fails\n\n        self.prompt_analysis = PromptAnalysis(\n            json.loads(analysis['_analyze_prompt'][0]['llm_response']),\n            json.loads(analysis['_generate_indices'][0]['llm_response']),\n        )\n\n        genrate_indices_future = self.service.run_task(\n            self._generate_indicies_embeddings(self.prompt_analysis.indices['indices'])\n        )\n        genrate_indices_future.add_done_callback(self.on_indices_embeddings_generated)\n\n    async def _generate_indices(self, meta_list: list[Meta]) -&gt; dict[str, str]:\n        plugin = self.prompt_retrieve_indices_plugin\n        # add prompt engineering here and submit as the full prompt.\n        prompt = PromptGenIndices(prompt_str=self.prompt.prompt_str, input_data={'meta_list': meta_list})\n        structured_output = {'indices': list[str]}\n        ret = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=structured_output,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        if __debug__:\n            prompt_render = prompt.render_prompt()\n            self.service.send_message_async(\n                Service.Topic.DEBUG_ASK_INDICES,\n                {'ask_id': self.id, 'prompt': prompt_render, 'indices': ret[0]['llm_response']},\n            )\n\n        self.service.host.update_mock_data(plugin, ret)\n        response = ret[0]['llm_response']\n        response_json = json.loads(response)\n        count = len(response_json['indices'])\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.DYNAMIC_INDICES_GENERATED, count\n        )\n\n        if not isinstance(ret[0], dict):\n            error = f'Expected dict[str, str], got {type(ret[0])}'\n            raise TypeError(error)\n\n        return ret[0]\n\n    def on_indices_embeddings_generated(self, fut: Future[Any]) -&gt; None:\n        embeddings = fut.result()\n\n        query_index_db_future = self.service.run_task(self._query_index_db(embeddings))\n        query_index_db_future.add_done_callback(self.on_query_index_db)\n\n    async def _generate_indicies_embeddings(self, indices: list[str]) -&gt; list[list[float]]:\n        plugin = self.embeddings_gen_embed\n\n        ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=indices, args=self.service.host.mock_update_args(plugin)\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        embeddings_list: list[list[float]] = ret[0]['embeddings_list']\n        return embeddings_list\n\n    \"\"\"\n    ### Fetch Engram IDs\n\n    Use the indices to fetch related Engram IDs\n    \"\"\"\n\n    async def _query_index_db(self, embeddings: list[list[float]]) -&gt; set[str]:\n        plugin = self.prompt_vector_db_plugin\n\n        ids = set()\n\n        ret = await asyncio.to_thread(\n            plugin['func'].query,\n            collection_name='main',\n            embeddings=embeddings,\n            args=self.service.host.mock_update_args(plugin),\n        )\n\n        self.service.host.update_mock_data(plugin, ret)\n        ids.update(ret[0]['query_set'])\n\n        num_queries = len(ids)\n        self.metrics_tracker.increment(\n            engramic.application.retrieve.retrieve_service.RetrieveMetric.VECTOR_DB_QUERIES, num_queries\n        )\n\n        return ids\n\n    def on_query_index_db(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        logging.debug('Query Result: %s', ret)\n\n        if self.prompt_analysis is None:\n            error = 'on_query_index_db failed: prompt_analysis is None and likely failed during an earlier process.'\n            raise RuntimeError\n\n        retrieve_result = RetrieveResult(\n            self.id,\n            engram_id_array=list(ret),\n            conversation_direction=self.conversation_direction,\n            analysis=asdict(self.prompt_analysis)['prompt_analysis'],\n        )\n\n        if self.prompt_analysis is None:\n            error = 'Prompt analysis None in on_query_index_db'\n            raise RuntimeError(error)\n\n        retrieve_response = {\n            'analysis': asdict(self.prompt_analysis),\n            'prompt_str': self.prompt.prompt_str,\n            'retrieve_response': asdict(retrieve_result),\n        }\n\n        if __debug__:\n            self.service.host.update_mock_data_output(self.service, retrieve_response)\n\n        self.service.send_message_async(Service.Topic.RETRIEVE_COMPLETE, retrieve_response)\n</code></pre>"},{"location":"reference/ask/#engramic.application.retrieve.ask.Ask.Ask","title":"<code>Ask</code>","text":"<p>               Bases: <code>Retrieval</code></p> <p>Ask is a specifc type of retrieval focused on traditional Q&amp;A. It is a single instanc of an ask.</p> <p>This class handles the end-to-end workflow of transforming a raw prompt into contextual embeddings, querying the vector database, and returning retrieved engram ids. It also supports various conversation analysis and prompt index generation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for this retrieval session.</p> <code>prompt</code> <code>Prompt</code> <p>The original prompt provided by the user.</p> <code>plugin_manager</code> <code>PluginManager</code> <p>Plugin manager used to access LLM, vector DB, and embedding components.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks operational metrics for observability.</p> <code>db_plugin</code> <code>dict</code> <p>Plugin used to interact with the document database.</p> <code>service</code> <code>RetrieveService</code> <p>Reference to the parent service coordinating this request.</p> <code>library</code> <code>str | None</code> <p>Optional name of the target library to search within.</p> <code>conversation_direction</code> <code>dict[str, str]</code> <p>Stores current user intent and working memory.</p> <code>prompt_analysis</code> <code>PromptAnalysis | None</code> <p>Stores structured analysis of the prompt after processing.</p> <p>Methods:</p> Name Description <code>get_sources</code> <p>Initiates the async pipeline for directional memory retrieval.</p> <code>_fetch_history</code> <p>Asynchronously retrieves prior user history from the document DB.</p> <code>_retrieve_gen_conversation_direction</code> <p>Uses LLM plugin to extract user intent and conversational memory.</p> <code>_embed_gen_direction</code> <p>Converts extracted intent into embeddings. General direction determines intent and manages short term memory.</p> <code>_vector_fetch_direction_meta</code> <p>Queries the metadata collection in the vector DB using intent embeddings.</p> <code>_fetch_direction_meta</code> <p>Loads Meta objects from the metadata store based on query results.</p> <code>_analyze_prompt</code> <p>Uses LLM plugin to analyze the user prompt in the context of metadata.</p> <code>_generate_indices</code> <p>Generates semantic indices from the prompt and metadata for retrieval.</p> <code>_generate_indicies_embeddings</code> <p>Converts generated index phrases into embeddings.</p> <code>_query_index_db</code> <p>Searches the main vector DB with embeddings to identify related engrams.</p> <code>on_fetch_history_complete</code> <p>Callback when prompt history fetch is complete; begins direction generation.</p> <code>on_direction_ret_complete</code> <p>Callback when conversation direction is generated; begins embedding.</p> <code>on_embed_direction_complete</code> <p>Callback when direction embedding is ready; begins vector DB search for metadata.</p> <code>on_vector_fetch_direction_meta_complete</code> <p>Callback when metadata vector search is complete; begins metadata fetch.</p> <code>on_fetch_direction_meta_complete</code> <p>Callback when metadata objects are fetched; begins analysis and index generation.</p> <code>on_analyze_complete</code> <p>Callback when prompt analysis and index generation are complete; begins embedding generation.</p> <code>on_indices_embeddings_generated</code> <p>Callback when index embeddings are ready; triggers main index DB query.</p> <code>on_query_index_db</code> <p>Final callback when engram retrieval is complete; assembles and emits the result.</p> Source code in <code>src/engramic/application/retrieve/ask.py</code> <pre><code>class Ask(Retrieval):\n    \"\"\"\n    Ask is a specifc type of retrieval focused on traditional Q&amp;A. It is a single instanc of an ask.\n\n    This class handles the end-to-end workflow of transforming a raw prompt into\n    contextual embeddings, querying the vector database, and returning retrieved engram ids.\n    It also supports various conversation analysis and prompt index generation.\n\n    Attributes:\n        id (str): A unique identifier for this retrieval session.\n        prompt (Prompt): The original prompt provided by the user.\n        plugin_manager (PluginManager): Plugin manager used to access LLM, vector DB, and embedding components.\n        metrics_tracker (MetricsTracker): Tracks operational metrics for observability.\n        db_plugin (dict): Plugin used to interact with the document database.\n        service (RetrieveService): Reference to the parent service coordinating this request.\n        library (str | None): Optional name of the target library to search within.\n        conversation_direction (dict[str, str]): Stores current user intent and working memory.\n        prompt_analysis (PromptAnalysis | None): Stores structured analysis of the prompt after processing.\n\n    Methods:\n        get_sources():\n            Initiates the async pipeline for directional memory retrieval.\n\n        _fetch_history():\n            Asynchronously retrieves prior user history from the document DB.\n\n        _retrieve_gen_conversation_direction():\n            Uses LLM plugin to extract user intent and conversational memory.\n\n        _embed_gen_direction():\n            Converts extracted intent into embeddings. General direction determines intent and manages short term memory.\n\n        _vector_fetch_direction_meta():\n            Queries the metadata collection in the vector DB using intent embeddings.\n\n        _fetch_direction_meta():\n            Loads Meta objects from the metadata store based on query results.\n\n        _analyze_prompt():\n            Uses LLM plugin to analyze the user prompt in the context of metadata.\n\n        _generate_indices():\n            Generates semantic indices from the prompt and metadata for retrieval.\n\n        _generate_indicies_embeddings():\n            Converts generated index phrases into embeddings.\n\n        _query_index_db():\n            Searches the main vector DB with embeddings to identify related engrams.\n\n        on_fetch_history_complete(fut):\n            Callback when prompt history fetch is complete; begins direction generation.\n\n        on_direction_ret_complete(fut):\n            Callback when conversation direction is generated; begins embedding.\n\n        on_embed_direction_complete(fut):\n            Callback when direction embedding is ready; begins vector DB search for metadata.\n\n        on_vector_fetch_direction_meta_complete(fut):\n            Callback when metadata vector search is complete; begins metadata fetch.\n\n        on_fetch_direction_meta_complete(fut):\n            Callback when metadata objects are fetched; begins analysis and index generation.\n\n        on_analyze_complete(fut):\n            Callback when prompt analysis and index generation are complete; begins embedding generation.\n\n        on_indices_embeddings_generated(fut):\n            Callback when index embeddings are ready; triggers main index DB query.\n\n        on_query_index_db(fut):\n            Final callback when engram retrieval is complete; assembles and emits the result.\n    \"\"\"\n</code></pre>"},{"location":"reference/codify_service/","title":"Codify Service","text":"<p>               Bases: <code>Service</code></p> <p>CodifyService is a system-level service responsible for validating and extracting engrams (memories) from AI model responses using a TOML-based validation pipeline.</p> <p>This service listens for prompts that have completed processing, and if the system is in training mode, it fetches related engrams and metadata, applies an LLM-based validation process, and stores structured observations. It tracks metrics related to its activity and supports training workflows.</p> <p>Key Responsibilities: - Subscribes to relevant service events like <code>MAIN_PROMPT_COMPLETE</code> and <code>ACKNOWLEDGE</code>. - Fetches engrams and their associated metadata based on a completed model response. - Uses a validation plugin to process model responses and extract structured observation data. - Validates and loads TOML-encoded responses into structured Observation objects. - Merges observations when applicable and sends results asynchronously to downstream systems. - Tracks system-level metrics for observability and debugging.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manages access to system plugins such as the LLM and document DB.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Repository for accessing and managing engram data.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Repository for associated metadata retrieval.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles validation and normalization of observation data.</p> <code>prompt</code> <code>Prompt</code> <p>Default prompt object used during validation.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks custom CodifyMetric metrics.</p> <code>training_mode</code> <code>bool</code> <p>Flag indicating whether the system is in training mode.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes the service to key topics.</p> <code>stop</code> <p>Stops the service.</p> <code>init_async</code> <p>Initializes async components, including DB connections.</p> <code>on_set_training_mode</code> <p>Sets training mode flag based on incoming message.</p> <code>on_main_prompt_complete</code> <p>Main entry point triggered after a model completes a prompt.</p> <code>fetch_engrams</code> <p>Asynchronously fetches engrams associated with a response.</p> <code>on_fetch_engram_complete</code> <p>Callback that processes fetched engrams and triggers metadata retrieval.</p> <code>fetch_meta</code> <p>Asynchronously fetches metadata for given engrams.</p> <code>on_fetch_meta_complete</code> <p>Callback that begins the validation process after fetching metadata.</p> <code>validate</code> <p>Runs the validation plugin on the response and returns an observation.</p> <code>on_validate_complete</code> <p>Final step that emits the completed observation to other systems.</p> <code>on_acknowledge</code> <p>Responds to ACK messages by reporting and resetting metrics.</p> Source code in <code>src/engramic/application/codify/codify_service.py</code> <pre><code>class CodifyService(Service):\n    \"\"\"\n    CodifyService is a system-level service responsible for validating and extracting engrams (memories) from AI model responses using a TOML-based validation pipeline.\n\n    This service listens for prompts that have completed processing, and if the system is in training mode, it fetches related engrams and metadata, applies an LLM-based validation process, and stores structured observations. It tracks metrics related to its activity and supports training workflows.\n\n    Key Responsibilities:\n    - Subscribes to relevant service events like `MAIN_PROMPT_COMPLETE` and `ACKNOWLEDGE`.\n    - Fetches engrams and their associated metadata based on a completed model response.\n    - Uses a validation plugin to process model responses and extract structured observation data.\n    - Validates and loads TOML-encoded responses into structured Observation objects.\n    - Merges observations when applicable and sends results asynchronously to downstream systems.\n    - Tracks system-level metrics for observability and debugging.\n\n    Attributes:\n        plugin_manager (PluginManager): Manages access to system plugins such as the LLM and document DB.\n        engram_repository (EngramRepository): Repository for accessing and managing engram data.\n        meta_repository (MetaRepository): Repository for associated metadata retrieval.\n        observation_repository (ObservationRepository): Handles validation and normalization of observation data.\n        prompt (Prompt): Default prompt object used during validation.\n        metrics_tracker (MetricsTracker): Tracks custom CodifyMetric metrics.\n        training_mode (bool): Flag indicating whether the system is in training mode.\n\n    Methods:\n        start(): Subscribes the service to key topics.\n        stop(): Stops the service.\n        init_async(): Initializes async components, including DB connections.\n        on_set_training_mode(message_in): Sets training mode flag based on incoming message.\n        on_main_prompt_complete(response_dict): Main entry point triggered after a model completes a prompt.\n        fetch_engrams(response): Asynchronously fetches engrams associated with a response.\n        on_fetch_engram_complete(fut): Callback that processes fetched engrams and triggers metadata retrieval.\n        fetch_meta(engram_array, meta_id_array, response): Asynchronously fetches metadata for given engrams.\n        on_fetch_meta_complete(fut): Callback that begins the validation process after fetching metadata.\n        validate(engram_array, meta_array, response): Runs the validation plugin on the response and returns an observation.\n        on_validate_complete(fut): Final step that emits the completed observation to other systems.\n        on_acknowledge(message_in): Responds to ACK messages by reporting and resetting metrics.\n    \"\"\"\n\n    ACCURACY_CONSTANT = 3\n    RELEVANCY_CONSTANT = 3\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.llm_validate = self.plugin_manager.get_plugin('llm', 'validate')\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.meta_repository: MetaRepository = MetaRepository(self.db_document_plugin)\n        self.observation_repository: ObservationRepository = ObservationRepository(self.db_document_plugin)\n\n        self.prompt = Prompt('Validate the llm.')\n        self.metrics_tracker: MetricsTracker[CodifyMetric] = MetricsTracker[CodifyMetric]()\n        self.training_mode = False\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_main_prompt_complete)\n        self.subscribe(Service.Topic.SET_TRAINING_MODE, self.on_set_training_mode)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_set_training_mode(self, message_in: dict[str, Any]) -&gt; None:\n        self.training_mode = message_in['training_mode']\n\n    def on_main_prompt_complete(self, response_dict: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, response_dict)\n\n        if not self.training_mode:\n            return\n\n        prompt_str = response_dict['prompt_str']\n        model = response_dict['model']\n        analysis = PromptAnalysis(**response_dict['analysis'])\n        retrieve_result = RetrieveResult(**response_dict['retrieve_result'])\n        response = Response(\n            response_dict['id'], response_dict['response'], retrieve_result, prompt_str, analysis, model\n        )\n        self.metrics_tracker.increment(CodifyMetric.RESPONSE_RECIEVED)\n        fetch_engram_step = self.run_task(self._fetch_engrams(response))\n        fetch_engram_step.add_done_callback(self.on_fetch_engram_complete)\n\n    \"\"\"\n    ### Fetch Engrams &amp; Meta\n\n    Fetch engrams based on retrieved results.\n    \"\"\"\n\n    async def _fetch_engrams(self, response: Response) -&gt; dict[str, Any]:\n        engram_array: list[Engram] = await asyncio.to_thread(\n            self.engram_repository.load_batch_retrieve_result, response.retrieve_result\n        )\n\n        self.metrics_tracker.increment(CodifyMetric.ENGRAM_FETCHED, len(engram_array))\n\n        meta_array: set[str] = set()\n        for engram in engram_array:\n            if engram.meta_ids is not None:\n                meta_array.update(engram.meta_ids)\n\n        return {'engram_array': engram_array, 'meta_array': list(meta_array), 'response': response}\n\n    def on_fetch_engram_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        fetch_meta_step = self.run_task(self._fetch_meta(ret['engram_array'], ret['meta_array'], ret['response']))\n        fetch_meta_step.add_done_callback(self.on_fetch_meta_complete)\n\n    async def _fetch_meta(\n        self, engram_array: list[Engram], meta_id_array: list[str], response: Response\n    ) -&gt; dict[str, Any]:\n        meta_array: list[Meta] = await asyncio.to_thread(self.meta_repository.load_batch, meta_id_array)\n        # assembled main_prompt, render engrams.\n        return {'engram_array': engram_array, 'meta_array': meta_array, 'response': response}\n\n    def on_fetch_meta_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n        fetch_meta_step = self.run_task(self._validate(ret['engram_array'], ret['meta_array'], ret['response']))\n        fetch_meta_step.add_done_callback(self.on_validate_complete)\n\n    \"\"\"\n    ### Validate\n\n    Validates and extracts engrams (i.e. memories) from responses.\n    \"\"\"\n\n    async def _validate(self, engram_array: list[Engram], meta_array: list[Meta], response: Response) -&gt; dict[str, Any]:\n        # insert prompt engineering\n\n        del meta_array\n\n        input_data = {\n            'engram_list': engram_array,\n            'response': response.response,\n        }\n\n        prompt = PromptValidatePrompt(response.prompt_str, input_data=input_data)\n\n        plugin = self.llm_validate\n        validate_response = await asyncio.to_thread(\n            plugin['func'].submit, prompt=prompt, structured_schema=None, args=self.host.mock_update_args(plugin)\n        )\n\n        self.host.update_mock_data(self.llm_validate, validate_response)\n\n        toml_data = None\n\n        try:\n            if __debug__:\n                prompt_render = prompt.render_prompt()\n                self.send_message_async(\n                    Service.Topic.DEBUG_OBSERVATION_TOML_COMPLETE,\n                    {'prompt': prompt_render, 'toml': validate_response[0]['llm_response'], 'response_id': response.id},\n                )\n\n            toml_data = tomli.loads(validate_response[0]['llm_response'])\n\n        except tomli.TOMLDecodeError as e:\n            logging.exception('TOML decode error: %s', validate_response[0]['llm_response'])\n            error = 'Malformed TOML file in codify:validate.'\n            raise TypeError(error) from e\n\n        if 'not_memorable' in toml_data:\n            return {'return_observation': None}\n\n        if not self.observation_repository.validate_toml_dict(toml_data):\n            error = 'Codify TOML did not pass validation.'\n            raise TypeError(error)\n\n        return_observation = self.observation_repository.load_toml_dict(\n            self.observation_repository.normalize_toml_dict(toml_data, response)\n        )\n\n        # if this observation is from multiple sources, it must be merged the sources into it's meta.\n        if len(engram_array) &gt; 0:\n            return_observation_merged: Observation = return_observation.merge_observation(\n                return_observation,\n                CodifyService.ACCURACY_CONSTANT,\n                CodifyService.RELEVANCY_CONSTANT,\n                self.engram_repository,\n            )\n\n            return {'return_observation': return_observation_merged}\n\n        self.metrics_tracker.increment(CodifyMetric.ENGRAM_VALIDATED)\n\n        return {'return_observation': return_observation}\n\n    def on_validate_complete(self, fut: Future[Any]) -&gt; None:\n        ret = fut.result()\n\n        if ret['return_observation'] is not None:\n            self.send_message_async(Service.Topic.OBSERVATION_COMPLETE, asdict(ret['return_observation']))\n\n        if __debug__:\n            self.host.update_mock_data_output(self, asdict(ret['return_observation']))\n\n    \"\"\"\n    ### Ack\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/consolidate_service/","title":"Consolidate Service","text":"<p>               Bases: <code>Service</code></p> <p>The ConsolidateService orchestrates the post-processing pipeline for completed observations, coordinating summarization, engram generation, index generation, and embedding creation.</p> <p>This service is triggered when an observation is marked complete and is responsible for the following:</p> <ol> <li>Summarization - Generates a natural language summary from the observation using an LLM plugin.</li> <li>Embedding Summaries - Uses an embedding plugin to create vector embeddings of the summary text.</li> <li>Engram Generation - Extracts or constructs engrams from the observation's content.</li> <li>Index Generation - Applies an LLM to generate meaningful textual indices for each engram.</li> <li>Embedding Indices - Uses an embedding plugin to convert each index into a vector representation.</li> <li>Publishing Results - Emits messages like <code>ENGRAM_COMPLETE</code>, <code>META_COMPLETE</code>, and <code>INDEX_COMPLETE</code> at various stages to notify downstream systems.</li> </ol> <p>Metrics are tracked throughout the pipeline using a <code>MetricsTracker</code> and returned on demand via the <code>on_acknowledge</code> method.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Manages access to all system plugins.</p> <code>llm_summary</code> <code>dict</code> <p>Plugin used for generating summaries.</p> <code>llm_gen_indices</code> <code>dict</code> <p>Plugin used for generating indices from engrams.</p> <code>embedding_gen_embed</code> <code>dict</code> <p>Plugin used for generating embeddings for summaries and indices.</p> <code>db_document</code> <code>dict</code> <p>Plugin for document-level database access.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles deserialization of incoming observations.</p> <code>engram_builder</code> <code>dict[str, Engram]</code> <p>In-memory store of engrams awaiting completion.</p> <code>index_builder</code> <code>dict[str, Index]</code> <p>In-memory store of indices being constructed.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks metrics across each processing stage.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes the service to message topics.</p> <code>stop</code> <p>Stops the service and clears subscriptions.</p> <code>on_observation_complete</code> <p>Handles post-processing when an observation completes.</p> <code>_ generate_summary</code> <p>Creates a summary of the observation content. (not implemented yet)</p> <code>on_summary</code> <p>Callback after summary generation completes.</p> <code>generate_summary_embeddings</code> <p>Generates and attaches embeddings for a summary.</p> <code>generate_engrams</code> <p>Constructs engrams from observation data.</p> <code>on_engrams</code> <p>Callback after engram generation; handles index and embedding creation.</p> <code>gen_indices</code> <p>Uses an LLM to create indices from an engram.</p> <code>gen_embeddings</code> <p>Creates embeddings for generated indices.</p> <code>on_acknowledge</code> <p>Sends a metrics snapshot for observability/debugging.</p> Source code in <code>src/engramic/application/consolidate/consolidate_service.py</code> <pre><code>class ConsolidateService(Service):\n    \"\"\"\n    The ConsolidateService orchestrates the post-processing pipeline for completed observations,\n    coordinating summarization, engram generation, index generation, and embedding creation.\n\n    This service is triggered when an observation is marked complete and is responsible for the following:\n\n    1. **Summarization** - Generates a natural language summary from the observation using an LLM plugin.\n    2. **Embedding Summaries** - Uses an embedding plugin to create vector embeddings of the summary text.\n    3. **Engram Generation** - Extracts or constructs engrams from the observation's content.\n    4. **Index Generation** - Applies an LLM to generate meaningful textual indices for each engram.\n    5. **Embedding Indices** - Uses an embedding plugin to convert each index into a vector representation.\n    6. **Publishing Results** - Emits messages like `ENGRAM_COMPLETE`, `META_COMPLETE`, and `INDEX_COMPLETE` at various stages to notify downstream systems.\n\n    Metrics are tracked throughout the pipeline using a `MetricsTracker` and returned on demand via the\n    `on_acknowledge` method.\n\n    Attributes:\n        plugin_manager (PluginManager): Manages access to all system plugins.\n        llm_summary (dict): Plugin used for generating summaries.\n        llm_gen_indices (dict): Plugin used for generating indices from engrams.\n        embedding_gen_embed (dict): Plugin used for generating embeddings for summaries and indices.\n        db_document (dict): Plugin for document-level database access.\n        observation_repository (ObservationRepository): Handles deserialization of incoming observations.\n        engram_builder (dict[str, Engram]): In-memory store of engrams awaiting completion.\n        index_builder (dict[str, Index]): In-memory store of indices being constructed.\n        metrics_tracker (MetricsTracker): Tracks metrics across each processing stage.\n\n    Methods:\n        start(): Subscribes the service to message topics.\n        stop(): Stops the service and clears subscriptions.\n        on_observation_complete(observation_dict): Handles post-processing when an observation completes.\n        _ generate_summary(observation): Creates a summary of the observation content. (not implemented yet)\n        on_summary(summary_fut): Callback after summary generation completes.\n        generate_summary_embeddings(meta): Generates and attaches embeddings for a summary.\n        generate_engrams(observation): Constructs engrams from observation data.\n        on_engrams(engram_list_fut): Callback after engram generation; handles index and embedding creation.\n        gen_indices(index, id_in, engram): Uses an LLM to create indices from an engram.\n        gen_embeddings(id_and_index_dict, process_index): Creates embeddings for generated indices.\n        on_acknowledge(message_in): Sends a metrics snapshot for observability/debugging.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.llm_summary: dict[str, Any] = self.plugin_manager.get_plugin('llm', 'summary')\n        self.llm_gen_indices: dict[str, Any] = self.plugin_manager.get_plugin('llm', 'gen_indices')\n        self.embedding_gen_embed: dict[str, Any] = self.plugin_manager.get_plugin('embedding', 'gen_embed')\n        self.db_document: dict[str, Any] = self.plugin_manager.get_plugin('db', 'document')\n        self.observation_repository = ObservationRepository(self.db_document)\n        self.engram_builder: dict[str, Engram] = {}\n        self.metrics_tracker: MetricsTracker[ConsolidateMetric] = MetricsTracker[ConsolidateMetric]()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def on_observation_complete(self, observation_dict: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, observation_dict)\n\n        # should run a task for this.\n        observation = self.observation_repository.load_dict(observation_dict)\n        self.metrics_tracker.increment(ConsolidateMetric.OBSERVATIONS_RECIEVED)\n\n        summary_observation = self.run_task(self._generate_summary(observation))\n        summary_observation.add_done_callback(self.on_summary)\n\n        generate_engrams = self.run_task(self._generate_engrams(observation))\n        generate_engrams.add_done_callback(self.on_engrams)\n\n    \"\"\"\n    ### Summarize\n\n    Will be used in the future when we pull in data from other sources.\n    \"\"\"\n\n    async def _generate_summary(self, observation: Observation) -&gt; Meta:\n        if (\n            observation.meta.summary_full is not None and not observation.meta.summary_full.text\n        ):  # native LLM observations have a summary already.\n            not_test = 'not tested yet'\n            raise NotImplementedError(not_test)\n            self.metrics_tracker.increment(ConsolidateMetric.SUMMARIES_GENERATED)\n\n        return observation.meta\n\n    def on_summary(self, summary_fut: Future[Any]) -&gt; None:\n        result = summary_fut.result()\n        self.run_task(self._generate_summary_embeddings(result))\n\n    async def _generate_summary_embeddings(self, meta: Meta) -&gt; None:\n        if meta.summary_full is None:\n            error = 'Summary full is none.'\n            raise ValueError(error)\n\n        plugin = self.embedding_gen_embed\n        embedding_list_ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=[meta.summary_full.text], args=self.host.mock_update_args(plugin)\n        )\n\n        self.host.update_mock_data(plugin, embedding_list_ret)\n\n        embedding_list = embedding_list_ret[0]['embeddings_list']\n        meta.summary_full.embedding = embedding_list[0]\n\n        self.send_message_async(Service.Topic.META_COMPLETE, asdict(meta))\n\n    \"\"\"\n    ### Generate Engrams\n\n    Create engrams from the observation.\n    \"\"\"\n\n    async def _generate_engrams(self, observation: Observation) -&gt; list[Engram]:\n        self.metrics_tracker.increment(ConsolidateMetric.ENGRAMS_GENERATED, len(observation.engram_list))\n\n        return observation.engram_list\n\n    def on_engrams(self, engram_list_fut: Future[Any]) -&gt; None:\n        engram_list = engram_list_fut.result()\n\n        # Keep references so we can fill them in later\n        for engram in engram_list:\n            if self.engram_builder.get(engram.id) is None:\n                self.engram_builder[engram.id] = engram\n            else:\n                error = 'Engram ID Collision. During conslidation, two Engrams with the same IDs were detected.'\n                raise RuntimeError(error)\n\n        # 1) Generate indices for each engram\n        index_tasks = [self._gen_indices(i, engram.id, engram) for i, engram in enumerate(engram_list)]\n\n        indices_future = self.run_tasks(index_tasks)\n\n        indices_future.add_done_callback(self.on_indices_done)\n\n    async def _gen_indices(self, index: int, id_in: str, engram: Engram) -&gt; dict[str, Any]:\n        data_input = {'engram': engram}\n\n        prompt = PromptGenIndices(prompt_str='', input_data=data_input)\n        plugin = self.llm_gen_indices\n\n        response_schema = {'index_text_array': list[str]}\n\n        indices = await asyncio.to_thread(\n            plugin['func'].submit,\n            prompt=prompt,\n            structured_schema=response_schema,\n            args=self.host.mock_update_args(plugin, index),\n        )\n\n        self.host.update_mock_data(plugin, indices, index)\n\n        self.metrics_tracker.increment(ConsolidateMetric.INDICES_GENERATED, len(indices))\n\n        response_json = json.loads(indices[0]['llm_response'])\n\n        return {'id': id_in, 'indices': response_json['index_text_array']}\n\n    # Once all indices are generated, generate embeddings\n    def on_indices_done(self, indices_list_fut: Future[Any]) -&gt; None:\n        # This is the accumulated result of each gen_indices(...) call\n        indices_list: dict[str, Any] = indices_list_fut.result()\n        # indices_list should have a key like 'gen_indices' -&gt; list[dict[str, Any]]\n        index_sets: list[dict[str, Any]] = indices_list['_gen_indices']\n\n        # 2) Generate embeddings for each index set\n        embed_tasks = [self._gen_embeddings(index_set, i) for i, index_set in enumerate(index_sets)]\n\n        embed_future = self.run_tasks(embed_tasks)\n\n        embed_future.add_done_callback(self.on_embeddings_done)\n\n    async def _gen_embeddings(self, id_and_index_dict: dict[str, Any], process_index: int) -&gt; str:\n        indices = id_and_index_dict['indices']\n        engram_id: str = id_and_index_dict['id']\n\n        plugin = self.embedding_gen_embed\n        embedding_list_ret = await asyncio.to_thread(\n            plugin['func'].gen_embed, strings=indices, args=self.host.mock_update_args(plugin, process_index)\n        )\n\n        self.host.update_mock_data(plugin, embedding_list_ret, process_index)\n\n        embedding_list = embedding_list_ret[0]['embeddings_list']\n\n        self.metrics_tracker.increment(ConsolidateMetric.EMBEDDINGS_GENERATED, len(embedding_list))\n\n        # Convert raw embeddings to Index objects and attach them\n        try:\n            index_array: list[Index] = []\n            for i, vec in enumerate(embedding_list):\n                index = Index(indices[i], vec)\n                index_array.append(index)\n        except Exception:\n            logging.exception('Exception caught.')\n\n        self.engram_builder[engram_id].indices = index_array\n        serialized_index_array = [asdict(index) for index in index_array]\n\n        # We can optionally notify about newly attached indices\n        self.send_message_async(Service.Topic.INDEX_COMPLETE, {'index': serialized_index_array, 'engram_id': engram_id})\n\n        # Return the ID so we know which engram was updated\n        return engram_id\n\n    # Once embeddings are generated, then we're truly done\n    def on_embeddings_done(self, embed_fut: Future[Any]) -&gt; None:\n        ret = embed_fut.result()  # ret should have 'gen_embeddings' -&gt; list of engram IDs\n\n        ids = ret['_gen_embeddings']  # which IDs got their embeddings updated\n\n        # 3) Now that embeddings exist, we can send \"ENGRAM_COMPLETE\" for each\n        engram_dict: list[dict[str, Any]] = []\n        engram_dict = [asdict(self.engram_builder[eid]) for eid in ids]\n\n        self.send_message_async(Service.Topic.ENGRAM_COMPLETE, {'engram_array': engram_dict})\n\n        if __debug__:\n            self.host.update_mock_data_output(self, {'engram_array': engram_dict})\n\n        for eid in ids:\n            del self.engram_builder[eid]\n\n    \"\"\"\n    ### Acknowledge\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/emgram/","title":"Engram Class","text":"<p>Represents a unit of memory, consisting of a text string (e.g., a phrase, sentence, or paragraph) along with contextual information that helps in retrieval and responses.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>A unique identifier for the engram.</p> <code>locations</code> <code>list[str]</code> <p>One or more locations where the engram was generated such as file paths or URLs.</p> <code>source_ids</code> <code>list[str]</code> <p>One or more identifiers linking the engram to its originating sources.</p> <code>content</code> <code>str</code> <p>The textual content of the engram.</p> <code>is_native_source</code> <code>bool</code> <p>Whether the content is directly extracted from a source (True) or derived/generated (False).</p> <code>context</code> <code>dict[str, str] | None</code> <p>Optional key-value pairs providing additional context for the engram.</p> <code>indices</code> <code>list[Index] | None</code> <p>Optional list of semantic indices associated with the engram, typically used for embedding-based search.</p> <code>meta_ids</code> <code>list[str] | None</code> <p>Optional metadata identifiers associated with this Engram.</p> <code>library_ids</code> <code>list[str] | None</code> <p>Optional identifiers grouping this engram into document collections or libraries.</p> <code>accuracy</code> <code>int | None</code> <p>An optional accuracy score assigned to the engram by validation on the Codify Service).</p> <code>relevancy</code> <code>int | None</code> <p>An optional relevancy score assigned to the engram by validation on the Codify Service).</p> <code>created_date</code> <code>datetime | None</code> <p>The creation timestamp of the engram.</p> <p>Methods:</p> Name Description <code>generate_toml</code> <p>Serializes the engram to a TOML-formatted string, including all non-null attributes and flattening indices.</p> Source code in <code>src/engramic/core/engram.py</code> <pre><code>@dataclass()\nclass Engram:\n    \"\"\"\n    Represents a unit of memory, consisting of a text string (e.g., a phrase, sentence, or paragraph)\n    along with contextual information that helps in retrieval and responses.\n\n    Attributes:\n        id (str): A unique identifier for the engram.\n        locations (list[str]): One or more locations where the engram was generated such as file paths or URLs.\n        source_ids (list[str]): One or more identifiers linking the engram to its originating sources.\n        content (str): The textual content of the engram.\n        is_native_source (bool): Whether the content is directly extracted from a source (True) or derived/generated (False).\n        context (dict[str, str] | None): Optional key-value pairs providing additional context for the engram.\n        indices (list[Index] | None): Optional list of semantic indices associated with the engram, typically used for embedding-based search.\n        meta_ids (list[str] | None): Optional metadata identifiers associated with this Engram.\n        library_ids (list[str] | None): Optional identifiers grouping this engram into document collections or libraries.\n        accuracy (int | None): An optional accuracy score assigned to the engram by validation on the Codify Service).\n        relevancy (int | None): An optional relevancy score assigned to the engram by validation on the Codify Service).\n        created_date (datetime | None): The creation timestamp of the engram.\n\n    Methods:\n        generate_toml() -&gt; str:\n            Serializes the engram to a TOML-formatted string, including all non-null attributes and flattening indices.\n    \"\"\"\n\n    id: str\n    locations: list[str]\n    source_ids: list[str]\n    content: str\n    is_native_source: bool\n    context: dict[str, str] | None = None\n    indices: list[Index] | None = None\n    meta_ids: list[str] | None = None\n    library_ids: list[str] | None = None\n    accuracy: int | None = 0\n    relevancy: int | None = 0\n    created_date: datetime | None = None\n\n    def generate_toml(self) -&gt; str:\n        def toml_escape(value: str) -&gt; str:\n            return f'\"{value}\"'\n\n        def toml_list(values: list[str]) -&gt; str:\n            return '[' + ', '.join(toml_escape(v) for v in values) + ']'\n\n        lines = [\n            f'id = {toml_escape(self.id)}',\n            f'content = {toml_escape(self.content)}',\n            f'is_native_source = {str(self.is_native_source).lower()}',\n            f'locations = {toml_list(self.locations)}',\n            f'source_ids = {toml_list(self.source_ids)}',\n        ]\n\n        if self.meta_ids:\n            lines.append(f'meta_ids = {toml_list(self.meta_ids)}')\n\n        if self.library_ids:\n            lines.append(f'library_ids = {toml_list(self.library_ids)}')\n\n        if self.context:\n            # Assuming context has a render_toml() method or can be represented as a dict\n            inline = ', '.join(f'{k} = {toml_escape(v)}' for k, v in self.context.items())\n            lines.append(f'context = {{ {inline} }}')\n\n        if self.indices:\n            # Flatten the index section\n            for index in self.indices:\n                # Assuming index has `text` and `embedding` attributes\n                if index.text is None:\n                    error = 'Null text in generate_toml.'\n                    raise ValueError(error)\n\n                lines.extend([\n                    '[[indices]]',\n                    f'text = {toml_escape(index.text)}',\n                    f'embedding = {toml_escape(str(index.embedding))}',\n                ])\n\n        return '\\n'.join(lines)\n</code></pre>"},{"location":"reference/message_service/","title":"Message Service","text":"<p>               Bases: <code>BaseMessageService</code></p> <p>A system-level message handling service that provides runtime CPU profiling and metrics reporting.</p> <p>MessageService extends BaseMessageService to handle system-level control messages, enabling dynamic profiling using Python's built-in <code>cProfile</code> module and responding to acknowledgment messages for metrics tracking. It operates by subscribing to control topics and exposing runtime observability features.</p> <p>Responsibilities: - Handles <code>START_PROFILER</code> and <code>END_PROFILER</code> messages to control a CPU profiler. - Responds to <code>ACKNOWLEDGE</code> messages by emitting a status update with performance metrics. - Uses the <code>metrics_tracker</code> inherited from BaseMessageService to report accumulated metrics.</p> <p>Attributes:</p> Name Type Description <code>profiler</code> <code>Profile | None</code> <p>An optional CPU profiler used to capture runtime performance data. The profiler is started and stopped in response to specific control messages and saves output to a file named 'profile_output.prof'.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Resets the profiler during asynchronous initialization.</p> <code>start</code> <p>Subscribes to relevant system topics for profiler control and metric acknowledgment.</p> <code>stop</code> <p>Gracefully shuts down the message service.</p> <code>start_profiler</code> <p>Initializes and starts the CPU profiler.</p> <code>end_profiler</code> <p>Stops the profiler and dumps results to a profile file.</p> <code>on_acknowledge</code> <p>Sends a metric snapshot and service status in response to ACKNOWLEDGE messages.</p> Source code in <code>src/engramic/application/message/message_service.py</code> <pre><code>class MessageService(BaseMessageService):\n    \"\"\"\n    A system-level message handling service that provides runtime CPU profiling and metrics reporting.\n\n    MessageService extends BaseMessageService to handle system-level control messages, enabling\n    dynamic profiling using Python's built-in `cProfile` module and responding to acknowledgment\n    messages for metrics tracking. It operates by subscribing to control topics and exposing\n    runtime observability features.\n\n    Responsibilities:\n    - Handles `START_PROFILER` and `END_PROFILER` messages to control a CPU profiler.\n    - Responds to `ACKNOWLEDGE` messages by emitting a status update with performance metrics.\n    - Uses the `metrics_tracker` inherited from BaseMessageService to report accumulated metrics.\n\n    Attributes:\n        profiler (cProfile.Profile | None): An optional CPU profiler used to capture runtime performance data.\n            The profiler is started and stopped in response to specific control messages and saves output\n            to a file named 'profile_output.prof'.\n\n    Methods:\n        init_async(): Resets the profiler during asynchronous initialization.\n        start(): Subscribes to relevant system topics for profiler control and metric acknowledgment.\n        stop(): Gracefully shuts down the message service.\n        start_profiler(data): Initializes and starts the CPU profiler.\n        end_profiler(data): Stops the profiler and dumps results to a profile file.\n        on_acknowledge(message_in): Sends a metric snapshot and service status in response to ACKNOWLEDGE messages.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.profiler: cProfile.Profile | None = None\n\n    def init_async(self) -&gt; None:\n        super().init_async()\n        self.profiler = None\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.START_PROFILER, self.start_profiler)\n        self.subscribe(Service.Topic.END_PROFILER, self.end_profiler)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    def start_profiler(self, data: dict[Any, Any]) -&gt; None:\n        if data is not None:\n            del data\n        logging.info('Start Profiler')\n        self.profiler = cProfile.Profile()\n        if self.profiler:\n            self.profiler.enable()\n\n    def end_profiler(self, data: dict[Any, Any]) -&gt; None:\n        if data is not None:\n            del data\n        logging.info('Stop Profiler')\n        if self.profiler:\n            self.profiler.disable()\n            self.profiler.dump_stats('profile_output.prof')\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/response_service/","title":"Response Service","text":"<p>               Bases: <code>Service</code></p> <p>ResponseService orchestrates AI response generation by integrating retrieval results, historical context, and prompt engineering. It coordinates plugin-managed large language models (LLMs), websockets for streaming, and metrics tracking.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Provides access to LLM and DB plugins.</p> <code>web_socket_manager</code> <code>WebsocketManager</code> <p>Manages live streaming over websocket.</p> <code>db_document_plugin</code> <code>dict</code> <p>Document store plugin interface.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Access point for loading engrams.</p> <code>llm_main</code> <code>dict</code> <p>Plugin for executing the main LLM-based response generation.</p> <code>instructions</code> <code>Prompt</code> <p>Placeholder prompt object for main prompt design.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks internal response metrics.</p> <p>Methods:</p> Name Description <code>start</code> <p>Subscribes to service topics and initializes websocket manager.</p> <code>stop</code> <p>Shuts down the websocket manager and stops the service.</p> <code>init_async</code> <p>Initializes the DB plugin connection asynchronously.</p> <code>on_retrieve_complete</code> <p>Triggered when retrieval results are received. Initiates engram and history fetch processes.</p> <code>_fetch_history</code> <p>Asynchronously fetches historical conversation context.</p> <code>_fetch_retrieval</code> <p>Loads engrams using retrieve result.</p> <code>on_fetch_data_complete</code> <p>Callback fired after both history and engrams are loaded; launches the main prompt generation task.</p> <code>main_prompt</code> <p>Constructs and submits the main prompt to the LLM plugin; formats and returns the response.</p> <code>on_main_prompt_complete</code> <p>Callback fired after main prompt response is generated; sends result and updates metrics.</p> <code>on_acknowledge</code> <p>Sends current metrics snapshot to monitoring topics.</p> Notes <ul> <li>Asynchronous database and model operations are handled via <code>asyncio.to_thread</code>.</li> <li>Debug mode transmits intermediate prompt input and final output via websocket topics.</li> <li>Metrics are captured throughout the response generation pipeline to monitor performance.</li> </ul> Source code in <code>src/engramic/application/response/response_service.py</code> <pre><code>class ResponseService(Service):\n    \"\"\"\n    ResponseService orchestrates AI response generation by integrating retrieval\n    results, historical context, and prompt engineering. It coordinates plugin-managed\n    large language models (LLMs), websockets for streaming, and metrics tracking.\n\n    Attributes:\n        plugin_manager (PluginManager): Provides access to LLM and DB plugins.\n        web_socket_manager (WebsocketManager): Manages live streaming over websocket.\n        db_document_plugin (dict): Document store plugin interface.\n        engram_repository (EngramRepository): Access point for loading engrams.\n        llm_main (dict): Plugin for executing the main LLM-based response generation.\n        instructions (Prompt): Placeholder prompt object for main prompt design.\n        metrics_tracker (MetricsTracker): Tracks internal response metrics.\n\n    Methods:\n        start(): Subscribes to service topics and initializes websocket manager.\n        stop(): Shuts down the websocket manager and stops the service.\n        init_async(): Initializes the DB plugin connection asynchronously.\n        on_retrieve_complete(retrieve_result_in): Triggered when retrieval results are received.\n            Initiates engram and history fetch processes.\n        _fetch_history(): Asynchronously fetches historical conversation context.\n        _fetch_retrieval(prompt_str, analysis, retrieve_result): Loads engrams using retrieve result.\n        on_fetch_data_complete(fut): Callback fired after both history and engrams are loaded;\n            launches the main prompt generation task.\n        main_prompt(prompt_str, analysis, engram_array, retrieve_result, history_array):\n            Constructs and submits the main prompt to the LLM plugin; formats and returns the response.\n        on_main_prompt_complete(fut): Callback fired after main prompt response is generated;\n            sends result and updates metrics.\n        on_acknowledge(message_in): Sends current metrics snapshot to monitoring topics.\n\n    Notes:\n        - Asynchronous database and model operations are handled via `asyncio.to_thread`.\n        - Debug mode transmits intermediate prompt input and final output via websocket topics.\n        - Metrics are captured throughout the response generation pipeline to monitor performance.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.web_socket_manager: WebsocketManager = WebsocketManager(host)\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.llm_main = self.plugin_manager.get_plugin('llm', 'response_main')\n        self.instructions: Prompt = Prompt('Placeholder for prompt engineering for main prompt.')\n        self.metrics_tracker: MetricsTracker[ResponseMetric] = MetricsTracker[ResponseMetric]()\n        ##\n        # Many methods are not ready to be until their async component is running.\n        # Do not call async context methods in the constructor.\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.RETRIEVE_COMPLETE, self.on_retrieve_complete)\n        self.web_socket_manager.init_async()\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await self.web_socket_manager.shutdown()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_retrieve_complete(self, retrieve_result_in: dict[str, Any]) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, retrieve_result_in)\n\n        prompt_str = retrieve_result_in['prompt_str']\n        prompt_analysis = PromptAnalysis(**retrieve_result_in['analysis'])\n        retrieve_result = RetrieveResult(**retrieve_result_in['retrieve_response'])\n        self.metrics_tracker.increment(ResponseMetric.RETRIEVES_RECIEVED)\n        fetch_engrams_task = self.run_tasks([\n            self._fetch_retrieval(prompt_str=prompt_str, analysis=prompt_analysis, retrieve_result=retrieve_result),\n            self._fetch_history(),\n        ])\n        fetch_engrams_task.add_done_callback(self.on_fetch_data_complete)\n\n    \"\"\"\n    ### Fetch History &amp; Engram\n\n    Fetch engrams based on the IDs provided by the retrieve service.\n    \"\"\"\n\n    async def _fetch_history(self) -&gt; dict[str, Any]:\n        plugin = self.db_document_plugin\n        args = plugin['args']\n        args['history'] = 1\n\n        ret_val = await asyncio.to_thread(plugin['func'].fetch, table=DB.DBTables.HISTORY, ids=[], args=args)\n        history: dict[str, Any] = ret_val[0]\n        return history\n\n    async def _fetch_retrieval(\n        self, prompt_str: str, analysis: PromptAnalysis, retrieve_result: RetrieveResult\n    ) -&gt; dict[str, Any]:\n        engram_array: list[Engram] = await asyncio.to_thread(\n            self.engram_repository.load_batch_retrieve_result, retrieve_result\n        )\n\n        # assembled main_prompt, render engrams.\n        return {\n            'prompt_str': prompt_str,\n            'analysis': analysis,\n            'retrieve_result': retrieve_result,\n            'engram_array': engram_array,\n        }\n\n    def on_fetch_data_complete(self, fut: Future[Any]) -&gt; None:\n        exc = fut.exception()\n        if exc is not None:\n            raise exc\n        result = fut.result()\n        retrieval = result['_fetch_retrieval'][0]\n        history = result['_fetch_history'][0]\n\n        main_prompt_task = self.run_task(\n            self.main_prompt(\n                retrieval['prompt_str'],\n                retrieval['analysis'],\n                retrieval['engram_array'],\n                retrieval['retrieve_result'],\n                history,\n            )\n        )\n        main_prompt_task.add_done_callback(self.on_main_prompt_complete)\n\n    \"\"\"\n    ### Main Prompt\n\n    Combine the previous stages to generate the response.\n    \"\"\"\n\n    async def main_prompt(\n        self,\n        prompt_str: str,\n        analysis: PromptAnalysis,\n        engram_array: list[Engram],\n        retrieve_result: RetrieveResult,\n        history_array: dict[str, Any],\n    ) -&gt; Response:\n        self.metrics_tracker.increment(ResponseMetric.ENGRAMS_FETCHED, len(engram_array))\n\n        engram_dict_list = [asdict(engram) for engram in engram_array]\n\n        # build main prompt here\n        prompt = PromptMainPrompt(\n            prompt_str=prompt_str,\n            input_data={\n                'engram_list': engram_dict_list,\n                'history': history_array,\n                'working_memory': retrieve_result.conversation_direction,\n                'analysis': retrieve_result.analysis,\n            },\n        )\n\n        plugin = self.llm_main\n\n        response = await asyncio.to_thread(\n            plugin['func'].submit_streaming,\n            prompt=prompt,\n            websocket_manager=self.web_socket_manager,\n            args=self.host.mock_update_args(plugin),\n        )\n\n        if __debug__:\n            main_prompt = prompt.render_prompt()\n            self.send_message_async(\n                Service.Topic.DEBUG_MAIN_PROMPT_INPUT, {'main_prompt': main_prompt, 'ask_id': retrieve_result.ask_id}\n            )\n\n        self.host.update_mock_data(self.llm_main, response)\n\n        model = ''\n        if plugin['args'].get('model'):\n            model = plugin['args']['model']\n\n        response = response[0]['llm_response'].replace('$', 'USD ').replace('&lt;context&gt;', '').replace('&lt;/context&gt;', '')\n\n        response_inst = Response(str(uuid.uuid4()), response, retrieve_result, prompt.prompt_str, analysis, model)\n\n        return response_inst\n\n    def on_main_prompt_complete(self, fut: Future[Any]) -&gt; None:\n        result = fut.result()\n        self.metrics_tracker.increment(ResponseMetric.MAIN_PROMPTS_RUN)\n\n        self.send_message_async(Service.Topic.MAIN_PROMPT_COMPLETE, asdict(result))\n\n        if __debug__:\n            self.host.update_mock_data_output(self, asdict(result))\n\n    \"\"\"\n    ### Ack\n\n    Acknowledge and return metrics\n    \"\"\"\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/retrieve_service/","title":"Retrieve Service","text":"<p>               Bases: <code>Service</code></p> <p>Manages semantic prompt retrieval and indexing by coordinating between vector/document databases, tracking metrics, and responding to system events.</p> <p>This service is responsible for receiving prompt submissions, retrieving relevant information using vector similarity, and handling the indexing and metadata enrichment process. It interfaces with plugin-managed databases and provides observability through metrics tracking.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Access point for system plugins, including vector and document DBs.</p> <code>vector_db_plugin</code> <code>dict</code> <p>Plugin used for vector database operations (e.g., semantic search).</p> <code>db_plugin</code> <code>dict</code> <p>Plugin for interacting with the document database.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Collects and resets retrieval-related metrics for monitoring.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Handles Meta object persistence and transformation.</p> <p>Methods:</p> Name Description <code>init_async</code> <p>Initializes database connections and plugin setup asynchronously.</p> <code>start</code> <p>Subscribes to system topics for prompt processing and indexing lifecycle.</p> <code>stop</code> <p>Cleans up the service and halts processing.</p> <code>submit</code> <p>Prompt): Begins the retrieval process and logs submission metrics.</p> <code>on_submit_prompt</code> <p>str): Converts raw prompt string to a Prompt object and submits for processing.</p> <code>on_index_complete</code> <p>dict): Converts index payload into Index objects and queues for insertion.</p> <code>_insert_engram_vector</code> <p>list[Index], engram_id: str): Asynchronously inserts semantic indices into vector DB.</p> <code>on_meta_complete</code> <p>dict): Loads and inserts metadata summary into the vector DB.</p> <code>insert_meta_vector</code> <p>Meta): Runs metadata vector insertion in a background thread.</p> <code>on_acknowledge</code> <p>str): Emits service metrics to the status channel and resets the tracker.</p> Source code in <code>src/engramic/application/retrieve/retrieve_service.py</code> <pre><code>class RetrieveService(Service):\n    \"\"\"\n    Manages semantic prompt retrieval and indexing by coordinating between vector/document databases,\n    tracking metrics, and responding to system events.\n\n    This service is responsible for receiving prompt submissions, retrieving relevant information using\n    vector similarity, and handling the indexing and metadata enrichment process. It interfaces with\n    plugin-managed databases and provides observability through metrics tracking.\n\n    Attributes:\n        plugin_manager (PluginManager): Access point for system plugins, including vector and document DBs.\n        vector_db_plugin (dict): Plugin used for vector database operations (e.g., semantic search).\n        db_plugin (dict): Plugin for interacting with the document database.\n        metrics_tracker (MetricsTracker): Collects and resets retrieval-related metrics for monitoring.\n        meta_repository (MetaRepository): Handles Meta object persistence and transformation.\n\n    Methods:\n        init_async(): Initializes database connections and plugin setup asynchronously.\n        start(): Subscribes to system topics for prompt processing and indexing lifecycle.\n        stop(): Cleans up the service and halts processing.\n\n        submit(prompt: Prompt): Begins the retrieval process and logs submission metrics.\n        on_submit_prompt(data: str): Converts raw prompt string to a Prompt object and submits for processing.\n\n        on_index_complete(index_message: dict): Converts index payload into Index objects and queues for insertion.\n        _insert_engram_vector(index_list: list[Index], engram_id: str): Asynchronously inserts semantic indices into vector DB.\n\n        on_meta_complete(meta_dict: dict): Loads and inserts metadata summary into the vector DB.\n        insert_meta_vector(meta: Meta): Runs metadata vector insertion in a background thread.\n\n        on_acknowledge(message_in: str): Emits service metrics to the status channel and resets the tracker.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.vector_db_plugin = host.plugin_manager.get_plugin('vector_db', 'db')\n        self.db_plugin = host.plugin_manager.get_plugin('db', 'document')\n        self.metrics_tracker: MetricsTracker[RetrieveMetric] = MetricsTracker[RetrieveMetric]()\n        self.meta_repository: MetaRepository = MetaRepository(self.db_plugin)\n\n    def init_async(self) -&gt; None:\n        self.db_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.SUBMIT_PROMPT, self.on_submit_prompt)\n        self.subscribe(Service.Topic.INDEX_COMPLETE, self.on_index_complete)\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        super().start()\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n\n    # when called from monitor service\n    def on_submit_prompt(self, data: str) -&gt; None:\n        self.submit(Prompt(data))\n\n    # when used from main\n    def submit(self, prompt: Prompt) -&gt; None:\n        if __debug__:\n            self.host.update_mock_data_input(self, asdict(prompt))\n\n        self.metrics_tracker.increment(RetrieveMetric.PROMPTS_SUBMITTED)\n        retrieval = Ask(str(uuid.uuid4()), prompt, self.plugin_manager, self.metrics_tracker, self.db_plugin, self)\n        retrieval.get_sources()\n\n    def on_index_complete(self, index_message: dict[str, Any]) -&gt; None:\n        raw_index: list[dict[str, Any]] = index_message['index']\n        engram_id: str = index_message['engram_id']\n        index_list: list[Index] = [Index(**item) for item in raw_index]\n        self.run_task(self._insert_engram_vector(index_list, engram_id))\n\n    async def _insert_engram_vector(self, index_list: list[Index], engram_id: str) -&gt; None:\n        plugin = self.vector_db_plugin\n        self.vector_db_plugin['func'].insert(\n            collection_name='main', index_list=index_list, obj_id=engram_id, args=plugin['args']\n        )\n\n        self.metrics_tracker.increment(RetrieveMetric.EMBEDDINGS_ADDED_TO_VECTOR)\n\n    def on_meta_complete(self, meta_dict: dict[str, Any]) -&gt; None:\n        meta = self.meta_repository.load(meta_dict)\n        self.run_task(self.insert_meta_vector(meta))\n        self.metrics_tracker.increment(RetrieveMetric.META_ADDED_TO_VECTOR)\n\n    async def insert_meta_vector(self, meta: Meta) -&gt; None:\n        plugin = self.vector_db_plugin\n        await asyncio.to_thread(\n            self.vector_db_plugin['func'].insert,\n            collection_name='meta',\n            index_list=[meta.summary_full],\n            obj_id=meta.id,\n            args=plugin['args'],\n        )\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"},{"location":"reference/storage_service/","title":"Storage Service","text":"<p>               Bases: <code>Service</code></p> <p>A service responsible for persisting runtime data artifacts within the Engramic system.</p> <p>StorageService listens for various system events and saves corresponding data\u2014including observations, engrams, metadata, and prompt histories\u2014via plugin-based repositories. It also tracks metrics for each type of saved entity to facilitate performance monitoring and operational insights.</p> <p>Attributes:</p> Name Type Description <code>plugin_manager</code> <code>PluginManager</code> <p>Provides access to system plugins, including database integrations.</p> <code>db_document_plugin</code> <p>Plugin used by repositories for data persistence.</p> <code>history_repository</code> <code>HistoryRepository</code> <p>Manages saving of prompt/response history data.</p> <code>observation_repository</code> <code>ObservationRepository</code> <p>Handles saving of Observation entities.</p> <code>engram_repository</code> <code>EngramRepository</code> <p>Handles saving of Engram entities.</p> <code>meta_repository</code> <code>MetaRepository</code> <p>Handles saving of Meta configuration entities.</p> <code>metrics_tracker</code> <code>MetricsTracker</code> <p>Tracks counts of saved items for metric reporting.</p> <p>Methods:</p> Name Description <code>start</code> <p>Registers the service to relevant message topics and begins operation.</p> <code>init_async</code> <p>Connects to the database plugin asynchronously before full service startup.</p> <code>on_engram_complete</code> <p>Callback for storing completed engram batches.</p> <code>on_observation_complete</code> <p>Callback for storing completed observations.</p> <code>on_prompt_complete</code> <p>Callback for storing completed prompt/response history.</p> <code>on_meta_complete</code> <p>Callback for storing finalized meta configuration.</p> <code>save_observation</code> <p>Coroutine to persist observations and update metrics.</p> <code>save_history</code> <p>Coroutine to persist prompt/response history and update metrics.</p> <code>save_engram</code> <p>Coroutine to persist engram data and update metrics.</p> <code>save_meta</code> <p>Coroutine to persist metadata and update metrics.</p> <code>on_acknowledge</code> <p>Collects current metrics and publishes service status.</p> Source code in <code>src/engramic/application/storage/storage_service.py</code> <pre><code>class StorageService(Service):\n    \"\"\"\n    A service responsible for persisting runtime data artifacts within the Engramic system.\n\n    StorageService listens for various system events and saves corresponding data\u2014including observations,\n    engrams, metadata, and prompt histories\u2014via plugin-based repositories. It also tracks metrics for each\n    type of saved entity to facilitate performance monitoring and operational insights.\n\n    Attributes:\n        plugin_manager (PluginManager): Provides access to system plugins, including database integrations.\n        db_document_plugin: Plugin used by repositories for data persistence.\n        history_repository (HistoryRepository): Manages saving of prompt/response history data.\n        observation_repository (ObservationRepository): Handles saving of Observation entities.\n        engram_repository (EngramRepository): Handles saving of Engram entities.\n        meta_repository (MetaRepository): Handles saving of Meta configuration entities.\n        metrics_tracker (MetricsTracker): Tracks counts of saved items for metric reporting.\n\n    Methods:\n        start(): Registers the service to relevant message topics and begins operation.\n        init_async(): Connects to the database plugin asynchronously before full service startup.\n        on_engram_complete(engram_dict): Callback for storing completed engram batches.\n        on_observation_complete(response): Callback for storing completed observations.\n        on_prompt_complete(response_dict): Callback for storing completed prompt/response history.\n        on_meta_complete(meta_dict): Callback for storing finalized meta configuration.\n        save_observation(response): Coroutine to persist observations and update metrics.\n        save_history(response): Coroutine to persist prompt/response history and update metrics.\n        save_engram(engram): Coroutine to persist engram data and update metrics.\n        save_meta(meta): Coroutine to persist metadata and update metrics.\n        on_acknowledge(message_in): Collects current metrics and publishes service status.\n    \"\"\"\n\n    def __init__(self, host: Host) -&gt; None:\n        super().__init__(host)\n        self.plugin_manager: PluginManager = host.plugin_manager\n        self.db_document_plugin = self.plugin_manager.get_plugin('db', 'document')\n        self.history_repository: HistoryRepository = HistoryRepository(self.db_document_plugin)\n        self.observation_repository: ObservationRepository = ObservationRepository(self.db_document_plugin)\n        self.engram_repository: EngramRepository = EngramRepository(self.db_document_plugin)\n        self.meta_repository: MetaRepository = MetaRepository(self.db_document_plugin)\n        self.metrics_tracker: MetricsTracker[StorageMetric] = MetricsTracker[StorageMetric]()\n\n    def start(self) -&gt; None:\n        self.subscribe(Service.Topic.ACKNOWLEDGE, self.on_acknowledge)\n        self.subscribe(Service.Topic.MAIN_PROMPT_COMPLETE, self.on_prompt_complete)\n        self.subscribe(Service.Topic.OBSERVATION_COMPLETE, self.on_observation_complete)\n        self.subscribe(Service.Topic.ENGRAM_COMPLETE, self.on_engram_complete)\n        self.subscribe(Service.Topic.META_COMPLETE, self.on_meta_complete)\n        super().start()\n\n    def init_async(self) -&gt; None:\n        self.db_document_plugin['func'].connect(args=None)\n        return super().init_async()\n\n    def on_engram_complete(self, engram_dict: dict[str, Any]) -&gt; None:\n        engram_batch = self.engram_repository.load_batch_dict(engram_dict['engram_array'])\n        for engram in engram_batch:\n            self.run_task(self.save_engram(engram))\n\n    def on_observation_complete(self, response: Observation) -&gt; None:\n        self.run_task(self.save_observation(response))\n\n    def on_prompt_complete(self, response_dict: dict[Any, Any]) -&gt; None:\n        response = Response(**response_dict)\n        self.run_task(self.save_history(response))\n\n    def on_meta_complete(self, meta_dict: dict[str, str]) -&gt; None:\n        meta: Meta = self.meta_repository.load(meta_dict)\n        self.run_task(self.save_meta(meta))\n\n    async def save_observation(self, response: Observation) -&gt; None:\n        self.observation_repository.save(response)\n        self.metrics_tracker.increment(StorageMetric.OBSERVATION_SAVED)\n        logging.debug('Storage service saving observation.')\n\n    async def save_history(self, response: Response) -&gt; None:\n        await asyncio.to_thread(self.history_repository.save_history, response)\n        self.metrics_tracker.increment(StorageMetric.HISTORY_SAVED)\n        logging.debug('Storage service saving history.')\n\n    async def save_engram(self, engram: Engram) -&gt; None:\n        await asyncio.to_thread(self.engram_repository.save_engram, engram)\n        self.metrics_tracker.increment(StorageMetric.ENGRAM_SAVED)\n        logging.debug('Storage service saving engram.')\n\n    async def save_meta(self, meta: Meta) -&gt; None:\n        logging.debug('Storage service saving meta.')\n        await asyncio.to_thread(self.meta_repository.save, meta)\n        self.metrics_tracker.increment(StorageMetric.META_SAVED)\n\n    def on_acknowledge(self, message_in: str) -&gt; None:\n        del message_in\n\n        metrics_packet: MetricPacket = self.metrics_tracker.get_and_reset_packet()\n\n        self.send_message_async(\n            Service.Topic.STATUS,\n            {'id': self.id, 'name': self.__class__.__name__, 'timestamp': time.time(), 'metrics': metrics_packet},\n        )\n</code></pre>"}]}